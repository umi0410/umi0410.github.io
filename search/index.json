[{"content":"시작하며 안녕하세요. 저번 글에서는 fiber 라는 웹 프레임워크로 간단히 웹 애플리케이션을 만드는 방법에 대해 알아봤으니 이번엔 웹 애플리케이션을 만들다보면 꼭 필요해지는 middleware 작성법에 대해 알아보겠습니다. 여태까지 백엔드에서 서버를 개발하면서는 JS의 Express 프레임워크를 제외하고는 미들웨어 개발이 그닥 쉽게 이해되는 부분은 아니었던 것 같습니다. Express에서 미들웨어를 작성하는 방법은 굉장히 직관적이고 문서도 많은 편이었거든요. 하지만 Spring은 Filter나 Interceptor를 이해하기 위해 많은 내용을 알아야하고, Django는 Class based나 function based, 그리고 middleware를 추가하는 법 등이 좀 복잡한 편이라고 생각합니다.\n그리고 Golang은 아무래도 코드로 말하는 사람들이 많아서인지 그닥 middleware 작성과 같은 부분들이 문서로 잘 나와있진 않고, 코드를 까보면서 만드는 경우가 많았던 것 같습니다. 그래서 이번 글에서는 fiber 로 웹서버를 띄우면서 간단히 미들웨어 하나 만들어보겠습니다.\n 제공되는 Basic Authentication middleware을 이용해 요청 유저를 식별 식별된 유저 정보가 삽입된 logger를 context에 주입  이렇게 하면 이후 service layer등에서는 요청 유저가 삽입된 logger을 통해 로그를 작성하니 로그 추적하기가 용이해지겠죠? ㅎㅎ (BasicAuth를 이용하는 이유는 남이 작성한 미들웨어를 가져다쓰는 방법을 편하게 알아보기 위함일 뿐 인증 방법으로 권장해서는 아닙니다.)\n미들웨어?? 그게 뭐야? 왜 필요해?! 웹 프레임워크에서 말하는 미들웨어는 보통 요청에 따른 핸들러를 수행하기 전 혹은 수행한 후에 수행하는 작업을 말합니다. 주로 체인의 형태라고 보시면 되고 한 미들웨어에서 응답을 완료하면 걔가 요청 핸들러의 역할을 하게 된다고 보면 될 것 같습니다. 응답을 완료하지 않으면 다음 미들웨어를 호출하죠.\n서버 개발을 처음할 때는 미들웨어의 필요성을 잘 느끼지 못할 수 있습니다. 하지만 어느정도 개발을 하거나 배포를 해나가다보면 \u0026lsquo;아\u0026hellip; 모든 요청마다 ~~~한 작업을 수행하고 싶은데\u0026rsquo; 이런 생각이 들 때가 있을 수 있습니다. 예를 들어 API 별로 레이턴시를 체크하고 싶을 수도 있고, body나 query string을 로그로 남기거나 어떤 유저의 요청인지 매 요청마다 정보를 주입하고 싶을 수 있죠. 이런 경우 middleware를 이용하면 모든 요청은 미들웨어를 거치면서 처리되기 때문에 편리하게 작업할 수 있습니다!\n대표적으로는 인증이나 캐싱, 로깅, 응답 타임아웃 등에 사용하고 있는 것 같아요!\n남이 만든 미들웨어 사용해보기 - BasicAuth Go를 통해 개발하는 이상 저희는 문서에만 의존할 수는 없습니다. 코드를 까봐야죠 !_! Guidelines for creating Middleware - fiber Github Issue 이슈에는 미들웨어 작성에 대한 몇 가지 가이드라인이 제시되어있더라구요. 참고해보시면 좋을 것 같기도 합니다.\n저는 gofiber/fiber/middleware/basicauth 의 코드를 바탕으로 해석해보겠습니다. 링크를 따라가면 README가 있긴하지만 정보가 많지는 않죠.\n// fiber의 basicauth 미들웨어 코드 중 일부입니다.  // Config defines the config for middleware. type Config struct { // Next defines a function to skip this middleware when returned true. \t// \t// Optional. Default: nil \tNext func(c *fiber.Ctx) bool // Users defines the allowed credentials \t// \t// Required. Default: map[string]string{} \tUsers map[string]string // Realm is a string to define realm attribute of BasicAuth. \t// the realm identifies the system to authenticate against \t// and can be used by clients to save credentials \t// \t// Optional. Default: \u0026#34;Restricted\u0026#34;. \tRealm string // Authorizer defines a function you can pass \t// to check the credentials however you want. \t// It will be called with a username and password \t// and is expected to return true or false to indicate \t// that the credentials were approved or not. \t// \t// Optional. Default: nil. \tAuthorizer func(string, string) bool // Unauthorized defines the response body for unauthorized responses. \t// By default it will return with a 401 Unauthorized and the correct WWW-Auth header \t// \t// Optional. Default: nil \tUnauthorized fiber.Handler // ContextUser is the key to store the username in Locals \t// \t// Optional. Default: \u0026#34;username\u0026#34; \tContextUsername string // ContextPass is the key to store the password in Locals \t// \t// Optional. Default: \u0026#34;password\u0026#34; \tContextPassword string } 위의 코드가 BasicAuth 미들웨어의 설정을 담당하는 Config 타입에 대한 코드입니다. 간단히 몇 개만 알아두면 이해하는 데에 도움이 되는 것들만 짚어보겠습니다.\n Next - nil이 아닌 경우 Next(*fiber.Ctx)가 true를 반환하면 Basic Auth 미들웨어 자체를 건너뜀. Users - 이 map에 정의된 유저만이 유효한 유저로 이용될 수도 있음. (Authorizer가 nil인 경우 이 정보를 바탕으로 인증 진행) Authorizer - 인증 로직을 담당함. nil로 둘 경우 Users에서 일치하는 유저를 찾음. 여길 커스터마이징해서 DB에서 찾거나 할 수도 있음. ContextUsername, ContextPassword - 인증 후 Context에 어떤 키로 각 정보를 담을지. (기본값은 \u0026ldquo;username\u0026rdquo;, \u0026ldquo;password\u0026rdquo;)  오호.. 막상 코드를 까보니 그닥 어렵지 않죠?! 그럼 과연 이 코드들이 설명한 게 맞는지 한 번 서버를 띄워봅시다.\nBasic Auth 미들웨어를 다양하게 설정해보기 1. 항상 인증에 성공하는 Authorizer를 이용하는 Config // newBasicAuthConfigAlwaysAllow 는 언제나 인증에 성공하는 Authorizer를 // 이용하는 Config를 만듭니다. func newBasicAuthConfigAlwaysAllow() *basicauth.Config{ return \u0026amp;basicauth.Config{ Authorizer: func(username string, password string) bool { return true }, } } func main(){ ... app.Use(basicauth.New(*newBasicAuthConfigAlwaysAllow())) ... } # curl -u username:password는 해당 username과 password로 Basic auth를 하는 요청을 보냅니다. $ curl -u wrong:wrong localhost:8000 Welcome! $ curl -u foo:bar localhost:8000 Welcome! Authorizer가 username과 password가 어떻든 true를 반환해 인증에 성공합니다.\n2. 미리 정의된 유저에 대해서만 인증에 성공하는 Authorizer를 이용하는 Config // newBasicAuthConfigAlwaysAllow 는 Users에 존재하는 유저 정보에 대해서만 // 인증에 성공하는 Authorizer를 이용하는 Config를 만듭니다. func newBasicAuthConfigAllowOnlyAdmin() *basicauth.Config{ return \u0026amp;basicauth.Config{ Users: map[string]string{ \u0026#34;foo\u0026#34;: \u0026#34;bar\u0026#34;, }, } } func main(){ ... app.Use(basicauth.New(*newBasicAuthConfigAllowOnlyAdmin())) ... } # curl -u username:password는 해당 username과 password로 Basic auth를 하는 요청을 보냅니다. $ curl -u wrong:wrong localhost:8001 Unauthorized $ curl -u foo:bar localhost:8001 Welcome! 앞서 말했듯 Authorizer를 설정하지 않고 nil로 둘 경우 Users 맵의 데이터를 이용하는 Authorizer가 기본값으로 설정됩니다. 저는 username=foo, password=bar인 유저 정보를 맵에 기록했기 때문에 foo:bar 요청만이 승인되는 것을 볼 수 있습니다.\n자, 이렇게 남이 작성한 미들웨어를 설정만 조금 바꿔서 이용해봤는데, 코드를 읽는 것도 그걸 바탕으로 가져다가 쓰는 것도 어렵지 않다는 걸 알게 됐군요. 이제는 직접 미들웨어를 작성해보겠습니다.\nCustom middleware 작성하기 커스텀으로 미들웨어를 작성할 때에는 그닥 정해진 기준이 많지는 않습니다. 대부분의 미들웨어들은 Skipper 등의 개념으로(basic auth 미들웨어에선 Next라고 정의됨.) 해당 미들웨어의 기능을 실행하지 않고 넘어가는 설정이나 뭐 각종 설정들이 존재는 하지만 우리는 근본적으로 커스텀 미들웨어를 어떻게 작성하는지에 대해 궁금하니까 그 부분만 알아보겠습니다.\npackage fiber ... // Handler defines a function to serve HTTP requests. type Handler = func(*Ctx) error fiber 프레임워크에서는 middleware을 정의할 때 fiber.Handler 라는 일종의 함수 타입을 이용합니다. 그리고 fiber.Handler는 보통은 클로져를 이용하는 형태로 제공됩니다. 제가 볼 땐 좀 특이한 것 같은데 아마 echo도 이런 식으로 클로져를 이용해 미들웨어를 생성했던 것 같습니다. 다음과 같이 말이죠.\nfunc NewMiddleware(config Config) fiber.Handler { // config에 기본값을 넣은 config를 클로져로 접근하려 함.  config = injectDefaultValues(config) // 실제 미들웨어 역할을 할 함수.  // config에 클로져로 접근한다.  return func(c *fiber.Ctx) error{ ... // 다음 미들웨어(혹은 요청 핸들러)를 수행한다.  return c.Next() } } 근데 이런 경우는 아무래도 config를 설정해야하는 경우인 것 같고 저희는 그냥 최대한 심플하게 만들어볼게요. 목표는 Basic Auth를 거친 뒤 유저 정보가 기입된 logger를 context에 삽입해주는 것입니다. 그럼 어떤 계층에서든 ctx.Get(\u0026quot;logger\u0026quot;) 등을 이용해 logger에 편리하게 접근할 수 있죠! log 프레임워크는 sirupsen/logrus 를 이용하겠습니다. 컬러도 잘 나오고 확장성도 좋은 걸로 알고 있어요!\nvar injectLoggerMiddleware = func (ctx *fiber.Ctx) error{ // basic auth middleware가 주입한 username을 이용합니다. \tusername := ctx.Locals(\u0026#34;username\u0026#34;) logger := log.WithField(\u0026#34;user\u0026#34;, username) // context에 logger라는 키로 유저 정보를 주입한 logger를 전달합니다! \tctx.Locals(\u0026#34;logger\u0026#34;, logger) // 다음 미들웨어(혹은 요청 핸들러)를 수행합니다. \treturn ctx.Next() } func main(){ ... // injectLoggerMiddleware는 basicauth가 context에 주입한 username을 이용하기 때문에  // 꼭 basicauth middleware 다음에 수행되어야합니다!  app.Use(basicauth.New(*newBasicAuthConfigAllowOnlyAdmin())) app.Use(injectLoggerMiddleware) // 요청 핸들러는 이런 식으로 middleware에게 주입받은 logger를 context에서 꺼내쓸 수 있습니다.  app.Get(\u0026#34;\u0026#34;, func(ctx *fiber.Ctx) error { logger := ctx.Locals(\u0026#34;logger\u0026#34;).(*log.Entry) logger.Info(\u0026#34;유저가 접속했습니다\u0026#34;) return ctx.SendString(\u0026#34;Welcome!\\n\u0026#34;) }) ... }  log.png \n그럼 다음과 같이 Basic Auth를 통해 각각 다른 유저로 요청을 보내면 해당 요청을 처리하는 동안은 해당 context 속의 logger을 이용해 로그를 남길 수 있는 걸 볼 수 있습니다. 이렇게 정리해보니 golang에서도 middleware를 작성하는 게 그리 어렵지 않네요~!\n마치며 echo 프레임워크를 쓰면서는 미들웨어를 작성할 때 좀 애를 먹었던 기억이 있는데, 막상 fiber로 미들웨어를 작성해보니 생각보다 많이 직관적이고 쉬워서 좀 놀랐습니다. (사실 글을 쓸 필요가 없었을지도..?) 그래도 Go로 작성된 한글 자료도 많아졌으면 좋겠고, 누군가는 fiber 미들웨어를 작성하는 방법이 좀 이해되지 않고 어려우실 수 있으니 도움이 될 수 있으면 좋겠습니다 ㅎㅎ 다음엔 서버 개발에 있어 또 하나의 중요한 점인! 에러 핸들링에 대해 알아보겠습니다! 감사합니다.\n참고  Guidelines for creating Middleware - fiber Github Issue Fiber middleware Fiber error handling Redhat - 미들웨어란  ","date":"2021-09-01T12:46:54+09:00","image":"https://umi0410.github.io/blog/golang/how-to-backend-in-go-middleware/log_hu6da6c55463c6a62988b186d685cd2c7c_23812_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/golang/how-to-backend-in-go-middleware/","title":"Golang으로 백엔드 개발하기 - 4. Custom Middleware(미들웨어) 작성해보기 (feat. fiber)"},{"content":"시작하며 이번에는 Go언어에서 유명한 웹 프레임워크를 비교해보고 간단하게 Pingpong 서버를 개발해보겠습니다. 다만 \u0026ldquo;Golang으로 백엔드 개발하기\u0026ldquo;를 주제로 글을 쓰면서 주 목적으로 했던 것은 개발하면서 계속 궁금했지만 어딘가에서 뚜렷한 설명을 찾아보기 힘들었던 내용들을 다뤄보고자하는 것이었기 때문에 이번 주제는 그닥 주 목적에 해당하는 주제는 아니므로 이번엔 아주 가~볍게 훑어보고 넘어가겠습니다. 왜냐하면 웹 프레임워크를 사용하는 것은 해당 웹프레임워크의 문서나 이런 저런 블로그들에 이미 너무도 많이 자료가 존재하기 때문입니다.\nGolang으로 웹 애플리케이션을 개발하는 방법들 보통 다른 언어 같은 경우 Java에서는 Spring MVC 혹은 Spring WebFlux, Python은 FastApi, Flask, Django, Node.js는 Express, nest, koa처럼 각 언어마다 손에 꼽을 만한 대표적인 웹 프레임워크들이 있는 것 같은데 Go는 그렇지가 않습니다. Go는 잘 나가는 웹 프레임워크들이 너무도 다양하게 존재합니다. 아주 아주 다양한 프레임워크가 존재하지만 그닥 우위가 뚜렷하진 않습니다. 어떤 한 기술이 생태계를 독점해버리거나 그 기술만이 강요되는 일 없이 서로(프레임워크간)가 경쟁하며 좋은 기술로 나아가는 게 장점이 될 수도 있고, 너무 프레임워크가 다양해서 선택하기 곤란하다는 것이 단점이 될 수도 있을 것 같습니다. 커뮤니티가 한 데 집중된다면 더 버그 픽스하기 쉬울텐데 그런 면에서는 조금 아쉽기도 합니다.\n다만 한 가지 흐름이 있다면 Python을 사용하는 개발자분들이 \u0026quot;Pythonic\u0026quot;이라는 철학(?)을 굉장히 좋아하시듯 Go를 사용하는 개발자분들은 \u0026quot;Go스러움\u0026quot;을 굉장히 좋아하는 경향이 있어 어떤 프레임워크가 계속해서 인기를 끄는 데에는 \u0026quot;Go스러운가\u0026quot;가 큰 영향을 끼치는 것 같습니다. 아마 Go스러움이란 다음과 같은 특징들을 얘기하는 것일 겁니다.\n 타입을 통해 안정적으로 개발한다. (interface{} 와 같은 빈 인터페이스를 이용하면서 type assertion이나 reflect를 이용하는 것을 지양) 가볍고 간결하다. (Spring MVC와 같은 방식은 지양. 프레임워크에서 쓸 데 없이 많은 걸 지원하면서 서비스가 무거워지는 것을 지양.) 빠르다. 동시성을 goroutine과 channel을 이용해 잘 사용한다. (근데 이 부분은 web framework을 고를 때는 영향을 끼치지 않는 것 같고 개발자의 코드에 대한 go스러움일 것입니다.)  프레임워크 추천  net/http - Go 표준(빌트인) 패키지 github.com/labstack/echo - 스타 20k github.com/gofiber/fiber - ❤️ 스타 14.6k github.com/gin-gonic/gin - 스타 50k github.com/beego/beego - 스타 26k \u0026hellip;  이런 특징들을 가진 웹프레임워크로 한국에서 많이 쓰이는 듯한 프레임워크들은 fiber, echo, gin 이 세 친구들인 것 같습니다. 저도 다 깊이 있게 써본 건 아니지만 그 동안 리서치 해온 것들이나 한국 Go 커뮤니티에서 오가는 이런 저런 내용들을 참고했을 때의 의견은 다음과 같습니다. gin은 오랜 시간 개발되어 온 프로젝트이고 꾸준히 스타도 많지만 아마 좀 \u0026lsquo;무겁다. 느리다.\u0026rsquo; 이런 얘기가 있었던 것 같습니다. echo는 간결하게 사용할 수 있으면서 필요한 기능들은 다 있고 개발이 활발하게 이루어지고 있는 듯한 느낌이지만 문서가 그닥 친절하지 않고 썩 빠른 건 아니다는 생각이 듭니다. 문서가 거의 그냥 예시 코드 작성한 정도입니다. 그래도 Go 언어 특성상 남의 코드를 까서 봐도 해석하기 쉽고 주석도 잘 달려있는 편이라 큰 어려움은 없었습니다. fiber는 요즘 가장 핫한 프레임워크 같습니다. 빠르게 깃헙 스타 수도 늘어나고 있고 fasthttp를 사용해서인지 아마 월등히 빠른 것으로 알려져있습니다. 게다가 문서도 너무 잘 작성되어있고 개발도 활발히 이루어지고 있습니다.\n이러한 이유로 저는 현재 fiber와 echo를 이용 중이고 추후 새로운 서비스를 개발해야한다면 fiber를 이용할 예정입니다! (사실 gin이 스타는 꾸준히 많이 늘어나고 있고 스타 수 자체도 1타인데 왜 한국에서 많이들 사용하지 않는지는 잘 모르겠습니다. 아시는 분이나 gin을 사용해보신 분 있으시면 의견 부탁드립니다..!)\n예시 코드 간단하게 net/http 나 fiber 중 하나를 이용해 GET /ping에 응답하는 웹 애플리케이션을 만들어보았으니 참고하실 분들은 참고하시길 바랍니다. 웬만하면 공식 문서의 Getting started나 Example들을 보시는 걸 추천드립니다.\n다른 글들과 마찬가지로 예시 코드는 https://github.com/umi0410/how-to-backend-in-go의 webapp 디렉토리에 업로드 해두겠습니다.\npackage main import ( \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/gofiber/fiber/v2\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) var ( framework string port = flag.Int(\u0026#34;p\u0026#34;, 8888, \u0026#34;서버가 Listen할 port 번호를 입력해주세요.\u0026#34;) ) func init() { flag.Parse() if len(flag.Args()) != 1 { log.Fatal(\u0026#34;하나의 인자를 전달해 framework를 정의해주세요. (e.g. http, echo, fiber)\u0026#34;) } framework = flag.Arg(0) } func main() { switch framework { case \u0026#34;http\u0026#34;: RunNewHttpServer() case \u0026#34;fiber\u0026#34;: RunNewFiberServer() default: log.Fatal(\u0026#34;지원하지 않는 framework 입니다.\u0026#34;) } } func RunNewHttpServer() { addr := fmt.Sprintf(\u0026#34;:%d\u0026#34;, *port) log.Printf(\u0026#34;Server is listening %d\u0026#34;, *port) http.HandleFunc(\u0026#34;/ping\u0026#34;, func(w http.ResponseWriter, req *http.Request) { if _, err := w.Write([]byte(\u0026#34;PingPong by net/http\\n\u0026#34;)); err != nil { log.Print(err) } }) if err := http.ListenAndServe(addr, nil); err != nil { log.Print(err) } } func RunNewFiberServer() { addr := fmt.Sprintf(\u0026#34;:%d\u0026#34;, *port) app := fiber.New() app.Get(\u0026#34;/ping\u0026#34;, func(c *fiber.Ctx) error { return c.SendString(\u0026#34;Pingpong by fiber\\n\u0026#34;) }) log.Printf(\u0026#34;Server is listening %d\u0026#34;, *port) if err := app.Listen(addr); err != nil { log.Print(err) } }  output.png \n마치며 간단하게 Go언어에서 유명한 프레임워크들을 비교해보고 Pingpong 서버 예제 코드를 작성해봤습니다. 앞서 언급드렸듯이 아무래도 웹 프레임워크로 hello world를 찍는 수준의 서버를 작성하는 내용은 이미 어느 곳에서든 찾아볼 수 있기 때문에 이번 글은 가볍게 마무리하고 이런 저런 프레임워크들의 공식 문서를 보시는 걸 추천드립니다 ㅎㅎ 다음 글은 미들웨어 작성이나 에러 처리에 대해 다뤄볼 예정인데 이번 글 보다 재밌을 것으로 예상됩니다. \u0026lsquo;에러를 어떻게 처리할까\u0026lsquo;나 \u0026lsquo;미들웨어는 어떻게 작성할 수 있을까\u0026lsquo;와 같은 내용은 그닥 흔히 찾아볼 수 있는 내용은 아닌 것 같았기 때문입니다. 그럼 다음 글에서 봐요 우리~ ㅎㅎ 👋\n참고  간단한 웹 서버 (HTTP 서버) - http://golang.site/go/article/111-%EA%B0%84%EB%8B%A8%ED%95%9C-%EC%9B%B9-%EC%84%9C%EB%B2%84-HTTP-%EC%84%9C%EB%B2%84 Understanding Listen Ports and Addresses - https://www.ateam-oracle.com/listen-addresses https://www.esparkinfo.com/top-golang-web-framework-development.html https://blog.devgenius.io/best-web-framework-of-golang-in-2021-aae4b2ad9bf 내가 Go 언어를 선택한 이유 - https://pronist.tistory.com/67  ","date":"2021-08-13T12:46:54+09:00","image":"https://umi0410.github.io/blog/golang/how-to-backend-in-go-webapp/output_hu537a11d34916ac1c2def7909b69ddbc5_22456_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/golang/how-to-backend-in-go-webapp/","title":"Golang으로 백엔드 개발하기 - 3. 웹 애플리케이션 개발해보기 (feat. fiber)"},{"content":"시작하며 저는 Go 언어를 공부하면서 거의 처음으로 본격적인 테스트 코드를 작성하게 되었습니다. 과거에는 그저 기능을 구현하는 것에만 관심이 있었지 애플리케이션을 어떻게 계층을 나눠 설계할지, 어떻게 해야 유지 보수하기 쉬우면서 안정적인 개발을 할 수 있을지에 대한 고민을 하지 않았습니다. 하지만 Go를 공부하면서 클린 아키텍쳐나 MSA, 동시성 패턴 등을 비롯해 테스트 코드에 대해서도 공부해볼 수 있었습니다. 그 중에서도 테스트 코드에 대해 공부해보며 익혔던 점들을 바탕으로 이번 글에서는 Golang으로 백엔드 개발을 하면서 테스트 코드를 어떻게 작성할 수 있을지에 대해 알아보려합니다.\n본 글에서 사용된 예시 코드는 저의 Github how-to-backend-in-go 레포지토리에 업로드 해둘테니 참고하실 분들은 참고해주세요~!\nTDD (Test Driven Development)와 유닛 테스트 사용자의 반응에 맞춰 재빠르게 대응할 수 있도록 개발 주기가 빨라지면서 DevOps 문화 뿐만 아니라 TDD도 많은 인기를 얻게 된 것 같습니다. TDD란 개발을 먼저 다~ 진행한 뒤 한 번에 배포하거나 시간 날 때 테스트 코드를 짜는 것이 아니라 우리가 개발해야할 최소한의 내용들을 검증할 수 있는 테스트 코드를 먼저 작성한 뒤 해당 테스트를 통과하는 코드를 개발해나가는 개발 방법론입니다.\nTDD에 대해 설명하자면 TDD만으로도 책 몇 권의 분량이 될 수 있고, 저 또한 TDD를 잘 아는 것은 아니기에 자세한 설명은 생략하겠습니다. 이번 글의 주된 내용은 TDD에 대해 소개하는 것보다는 Golang으로 유닛 테스트를 작성하는 요령입니다.\n유닛 테스트는 다른 계층과 무관하게 독립적으로 각각의 메소드의 기능을 테스트하는 것이라고 볼 수 있겠습니다. 계층마다 해당 테스트를 통해 검증하고자 하는 주요한 내용은 다를 수 있습니다. 유닛 테스트는 주로 자신만을 테스트 하기 때문에 자기 계층의 동작을 테스트하지 다른 계층의 동작까지 테스트하진 않습니다. 또한 경우에 따라 아예 다른 계층을 실제로 이용하지 않고 모킹(Mocking)한 타입을 이용하는 경우도 있습니다.\n  Data access layer\n userRepository.Create(user *entity.User) - 유저 테이블에 해당 데이터가 잘 저장되는지 테스트    Business layer\n  userService.Create(ctx *fiber.Ctx, user UserCreateInput) - 유저 생성 관련 비즈니스 로직을 주로 테스트.\nDB에 잘 영속화 되었는지 까지는 테스트 할 필요 없음. 이는 Data access layer에서 테스트 할 내용\n    Presentation layer\n  userHandler.Craete(ctx *fiber.Ctx) - 응답 코드가 바라던 대로 201 번이 맞는지. JSON으로 잘 응답하는지\n마찬가지로 DB에 잘 영속화 되었는지까지는 테스트할 필요 없음. 이는 Data access layer에서 테스트 할 내용.\n또한 어떤 business layer의 비즈니스 로직에 대해서도 자세히 테스트 할 필요 없음. 이 또한 Business layer에서 테스트할 내용\n    참고로 저는 Business layer를 위주로 테스트하고 있습니다. Data access layer의 경우 많은 부분이 Database framework(ORM)을 통해 검증이 되었고, Presentation layer도 Web framework들에 의해 많은 부분이 검증되었을 뿐 아니라 포스트맨 같은 도구들을 통해 확인해보기도 쉽기 때문입니다. 반면 Business layer의 동작은 매번 눈으로 혹은 머리로 확인하기 쉽지 않기 때문입니다.\nUnit test를 도입하기 전에 알아두면 좋은 것들  테스트를 위한 의존성 주입 및 변수 초기화 방법 mocking에 대하여 assert 구문 이용  Spring을 바탕으로 개발하는 경우 JUnit이라는 테스트 프레임워크를 주로 이용하고 이를 통해 테스트하는 방식이 거의 정형화 되어있는 것 같습니다. 하지만 Golang은 그닥 테스트 코드를 작성하는 형태가 정형화되어있지는 않습니다. 하지만 어떤 언어나 프레임워크를 통해 유닛 테스트 코드를 사용하든 위의 세 가지 항목들을 어떻게 이용할 지 알아야할 것입니다.\n1. 테스트를 위한 의존성 주입 및 변수 초기화 방법 어떠한 모듈이 자신의 멤버 변수들을 내부적으로 생성하는 것이 아니라 외부에서 주입 받는 형태인 의존성 주입은 유연한 의존성 관리를 위해 권장됩니다. 예를 들어 User에 대한 도메인/비즈니스 로직을 실행하며 UserRepository를 이용해 User를 영속적으로 저장하기도 하는 UserService가 UserRepository에 의존하는 경우를 살펴봅시다.\n// 1. 의존성 주입 형태 - Good func NewUserService(userRepository UserRepository) *UserService{ return \u0026amp;UserService{ userRepository: userRepository, } } // 2. 의존성을 주입하지 않는 형태 - Bad func NewUserServiceWithoutInjection(client *ent.UserClient) *UserService{ return \u0026amp;UserService{ userRepository: NewUserRepository(client), } } 첫 번째 - 의존성 주입 형태는 테스트 하는 경우나 개발 환경에 따라 유동적으로 적절한 UserService가 이용할 UserRepository를 주입받을 수 있습니다. 즉 실제로 API 서버를 구동시킬 때에는 NewUserService(userRepository UserRepository) 메소드에 실제로 DB를 이용하는 Repository인 UserRepositoryImpl 을 주입하고, 유닛 테스트를 수행할 때에는 모킹한 타입인 MockUserRepository를 주입하는 것입니다.\n두 번째 - 의존성을 주입하지 않는 형태는 자신이 의존하는 UserRepository가 이미 NewUserServiceByItSelf라는 메소드에서 NewUserRepository 라는 메소드를 통해 특정 UserRepository를 이용하도록 정의되어있기 때문에 Mocking을 적용하기도 힘들고, 만약 다른 모듈들 또한 UserRepository를 이용하는 경우 Singleton으로 이용하지 못하고 여러 UserRepository가 생성될 수도 있습니다. 사실 ent.UserClient라는 DB Client 또한 주입받지 않고 내부적으로 새로 생성해야 의존성을 완전히 주입받지 않는 형태이겠지만 그런 방식은 너무도 구현하기 번거로워 편의상 DB client는 주입받았습니다. 이것만 봐도 개발하는 형태가 일관되지 못하고 불편한 사항이 많다는 것을 알 수 있죠.\n2. Mocking에 대하여 모킹이란 실제로 서버가 실행될 때 사용하는 모듈이 아닌 가짜 모듈을 이용하는 기법입니다. 앞에서 잠깐 언급했듯이 종종 테스트 코드 작성 시에 종종 Mocking이 이용됩니다. 의존성 주입 패턴을 통해 테스트 코드에서는 실제 type이 아닌 Mocking type을 유동적으로 주입할 수 있습니다. 잘 와닿지 않을 수 있는데 예를 들어 UserService의 비즈니스 룰과 로직이 잘 동작하는지를 확인할 때 Data access layer의 동작까지 테스트하고 싶지 않을 수 있습니다. 이 경우 Data access layer의 모듈을 가짜 타입으로 모킹합니다.\ntype MockUserRepository struct{} func (m MockUserRepository) Create(input *UserCreateInput) (*ent.User, error) { return \u0026amp;ent.User{ ID: input.ID, Password: input.Password, Name: input.Name, }, nil } func TestUserService_Create(t *testing.T){ // 해당 이름의 유저가 있는지 확인  s := NewUserService(new(MockUserRepository)) _, err := s.Create(\u0026amp;UserCreateInput{ID: \u0026#34;jinsu_umi\u0026#34;, Name: \u0026#34;Jinsu Park\u0026#34;, Password: \u0026#34;123123123\u0026#34;}) assert.NoError(t, err) } 이런 식으로 MockUserRepository 타입에서는 Create 메소드가 직접 DB에 유저를 영속화하는 작업을 생략한 뒤 그럴싸한 리턴값을 리턴하도록만 구현함으로써 UserRepsotiroy를 모킹하는 것입니다.\n하지만 이렇게 수작업으로 Mock type을 매번 정의하는 것은 귀찮을 수 있죠. 따라서 이번 글의 뒤에서는 mocking framework들에 대해서도 정리해보겠습니다.\n 종종 Stub과 Mock에 대해 혼동이 올 수 있습니다. 기본적으로 실제 모듈이 아닌 가짜 모듈을 이용하는 기법 자체를 mocking이라고 합니다. 이를 좀 더 세분화해서 봤을 땐 stub은 단순히 테스트를 간단히 통과시키기 위해 실제 모듈을 대체하는 가짜 객체를 말하고 mock은 예상값까지 비교를 하고, 때때로 테스트를 실패시키기도 하는 더 스마트한 stub이라고 볼 수 있다고 합니다.\n참고: https://stackoverflow.com/questions/3459287/whats-the-difference-between-a-mock-stub\n 3. assert 구문 이용하기 func TestUserService_Create(t *testing.T){ // 해당 이름의 유저가 있는지 확인  s := NewUserService(new(MockUserRepository)) t. _, err := s.Create(\u0026amp;UserCreateInput{ID: \u0026#34;jinsu_umi\u0026#34;, Name: \u0026#34;Jinsu Park\u0026#34;, Password: \u0026#34;123123123\u0026#34;}) // 이런 식으로 로그를 통해 하나 하나 테스트 결과를 확인하는 것이 아니라  //t.Log(\u0026#34;에러가 존재하지 않아야합니다.\u0026#34;)  //t.Log(\u0026#34;err == nil 인지 확인하십시오. err == nil: \u0026#34; , err == nil)  // assert를 이용해 자동으로 성공/실패를 판단합니다.  assert.NoError(t, err) } assert 라는 방식을 통한 테스트 케이스 검증은 다양한 테스트 프레임워크에서 사용됩니다. 눈으로 하나 하나 테스트 케이스들의 결과를 확인하는 것이 아니라 assert라는 방식을 통해 자동으로 테스트의 성공/실패를 판단하는 것입니다. 저는 주로 github.com/stretchr/testify 패키지의 assert 메소드를 이용하고 있습니다.\n test-output.png \nassert를 통해 테스트의 성공/실패 여부를 판단하면 성공시에는 크게 stdout을 확인할 필요 없고, 실패한 경우에는 그것에 대한 에러 메시지들만 확인하면 되기 때문에 편리합니다. 만약 로그나 프린트 문을 통해 수작업으로 테스트 결과를 판단해야한다면\u0026hellip; 매번 테스트마다 너무 불편하겠죠?!\nGolang에서 Mocking 이용하기 이번엔 Golang에서 Mocking을 이용하는 방법을 좀 더 자세히 알아보겠습니다. golang에서 Mocking을 이용하는 방법은 크게 3가지로 나눌 수 있을 것 같습니다.\n 직접 Mock type 구현하기 golang/mock 즉 gomock 프레임워크 이용하기 stretchr/testify/mock 프레임워크 이용하기  자바의 경우 따로 미리 mock 타입(즉 가짜 타입)을 작성해놓지 않더라도 그리고 기존 타입이 interface가 아닌 concrete한 일반 class이더라도 상속 혹은 프록시를 통해 mock 타입을 이용할 수 있습니다. 반면 Golang은 투명한 동작과 엄격한 타입이 특징인 만큼 테스트 진행 시에 자동으로 적절한 mock 타입을 생성하거나 프록시를 이용하거나 interface가 아닌 struct에 다른 type의 struct를 할당할 수도 없습니다. 따라서 Go에서 mocking을 이용하기 위해서는 아래와 같은 제약 조건들이 존재합니다. 이러한 점을 생각해본다면 Go에서는 Mocking을 이용하는 것은 다른 언어에 비해 쉬운 편은 아닌 것처럼 느껴지기도 합니다.\n interface로 선언된 변수에만 mock type을 할당할 수 있다. mock type을 직접 정의하거나 mocking framework을 이용해 코드를 생성한다.(테스트 진행 시 자동이 아님. 직접 우리가 실행시켜야함.)  주로 1번(직접 Mock type 구현하기) 같은 경우는 이 mock 타입이 우리 서비스에서 자주 사용될 것이고 그냥 예상치못한 call이나 missing call(호출되길 예상했지만 호출되지 않는 경우)에 대한 대응이 필요 없는 경우 사용하면 좋을 것 같습니다. 반면 2, 3번은 이번 테스트를 위한 1회성 mocking인 경우 많이 사용되기도 하고 mock 타입의 메소드에 대한 unexpected call이나 missing call 에 대한 대응을 수행하고자하는 경우 이용할 수 있습니다.\n추가적으로 1번 방식이 아닌 2, 3 번 방식처럼 모킹 프레임워크를 이용하는 게 더 편했던 적이 있는데요. 바로 메소드가 여러 개 존재하는 타입을 모킹할 때 입니다. Golang이 지향하는 방향 자체가 interface는 많은 메소드를 갖지 않는다는 것이긴 하지만 뭐.. 경우에 따라 메소드가 많아질 수도 있죠 ㅎㅎ. interface를 모킹하기 위해 별로 테스트 시에 필요도 없는 메소드들을 구현하는 것은 꽤나 귀찮습니다. 하지만 mocking 프레임워크들을 이용하면 기본적으로 모킹하려는 interface들의 메소드들은 모두 구현되고, 우리는 필요한 경우에만 추가적으로 구현을 해나가는 형태이므로 모킹 프레임워크를 이용하는 것이 편리할 수 있습니다.\n반면 mocking 프레임워크를 이용할 때의 단점은 프레임워크 사용법을 익혀야한다는 것이나 내가 직접 작성하지 않은 코드가 많이 생성된다는 점이 되겠네요. 그럼 이제 gomock과 testify/mock을 비교해보겠습니다. \u0026ldquo;GoMock vs. Testify: Mocking frameworks for Go\u0026rdquo; - codecentric blog에도 내용이 잘 정리되어있으니 참고해보세요.\nGolang mocking 프레임워크 gomock vs testify/mock 비교  star-comparison.png \ngolang에서 직접 제작하는 gomock과 stretchr/testify라는 유명한 테스트 관련 레포지토리의 하위 패키지인 testify/mock 은 Golang에서 사용할 수 있는 가장 유명한 모킹 프레임워크들입니다. 깃헙 스타만 놓고 봤을 때에는 testify가 훨씬 많지만 testify는 다른 테스트를 위한 패키지들도 함께 있어서 정확히 비교는 안되겠네요.\n저도 두 프레임워크의 특징에 대해 정확히는 모르지만 간단히 아는 선에서 주관적인 추천을 하자면 gomock을 추천드리겠습니다. testify/mock에 비해 원본 interface 타입을 바탕으로 모킹 메소드를 좀 더 safe하게 정의할 수 있었던 것 같습니다. 다만 정말 뭘 쓰든 상관 없을 것 같습니다. 각자 조금의 장단점은 있지만 뭐 하나가 특출나거나 모자란 느낌은 아닙니다. 두 프레임워크의 특징을 아래와 같이 정리해봤습니다. (참고로 알아보니 제가 좋아하는 서비스인 ArgoCD (K8s의 GitOps 방식의 배포 도구)에서는 testify/mock을 이용하더군요\u0026hellip; T.T)\n  코드 생성 도구를 이용해 원하는 interface의 mock type을 정의, 구현하는 코드를 생성하는 것이 testify/mock이 좀 더 편함.\n  testify/mock 이 좀 더 메소드를 mocking하고 expectation을 설정하는 게 직관적이고 간결하다. 이 부분이 testify/mock의 큰 장점인듯 함.\n  testify/mock 의 코드 생성 도구인 mockery가 gomock의 코드 생성 도구인 mockgen보다 쓰기 편하다는 의견이 있음. (본인은 잘 체감 못하겠음)\n  gomock이 좀 더 type safe하게 모킹 코드를 작성할 수 있다. 일단 메소드가 gomock은 타입을 기반으로 자동완성 되지만 testify/mock은 메소드 명을 문자열로 받음. 이 부분이 gomock의 큰 장점인듯 함.\n  두 프레임워크 모두 unexpected call이나 missing call, 특정 횟수 만큼 call 등등의 기능들은 제공하는 것 같음. 단 조사한 바로는 testify/mock은 모킹 메소드 구현 시에 인자 값을 이용할 수 없음.\n  그럼 말로만 설명하지 말고 코드로 한 번 간단하게 비교드리겠습니다!\n1. 직접 mock 타입 작성 type ManualMockUserRepository struct{} func (m *ManualMockUserRepository) Create(input *UserCreateInput) (*ent.User, error) { log.Info(\u0026#34;직접 Mocking. Args: \u0026#34;, input) return \u0026amp;ent.User{}, nil } func TestUserService_CreateWithManualMock(t *testing.T){ s := NewUserService(new(ManualMockUserRepository)) _, err := s.Create(\u0026amp;UserCreateInput{ID: \u0026#34;jinsu_umi\u0026#34;, Name: \u0026#34;Jinsu Park\u0026#34;, Password: \u0026#34;123123123\u0026#34;}) // 이런 식으로 로그를 통해 하나 하나 테스트 결과를 확인하는 것이 아니라  //t.Log(\u0026#34;에러가 존재하지 않아야합니다.\u0026#34;)  //t.Log(\u0026#34;err == nil 인지 확인하십시오. err == nil: \u0026#34; , err == nil)  // assert를 이용해 자동으로 성공/실패를 판단하십시오.  assert.NoError(t, err) } 앞서 말씀드린 대로 직접 mock type을 작성하는 경우입니다. 정말 정말 safe하지만 테스트 케이스마다 메소드가 return하는 값이나 동작을 다르게 하고 싶다면 코드가 복잡해지거나 한 interface에 대한 다양한 mock type을 여러 개 작성해야할 수 있습니다.\n2. testify/mock 이용  testify/mock을 이용하는 경우 mockery라고 하는 코드 생성 CLI 도구를 이용하면 편리합니다. 본 코드는 mockery를 통해 생성된 mock 타입을 이용했습니다.\n func TestUserService_CreateWithMockery(t *testing.T){ mockUserRepository := \u0026amp;MockUserRepository{} // method를 문자열 자체로 설정해야해서 safe하지 않음.  mockUserRepository.On(\u0026#34;Create\u0026#34;, mock.Anything).Run(func(args mock.Arguments) { t.Log(\u0026#34;testify/mock과 mockery를 이용한 Mocking. Args: \u0026#34;, args.Get(0)) }).Return(\u0026amp;ent.User{}, nil) // 해당 이름의 유저가 있는지 확인  s := NewUserService(mockUserRepository) _, err := s.Create(\u0026amp;UserCreateInput{ID: \u0026#34;jinsu_umi\u0026#34;, Name: \u0026#34;Jinsu Park\u0026#34;, Password: \u0026#34;123123123\u0026#34;}) assert.NoError(t, err) } 앞서 말씀드린 대로 메소드 명이 unsafe하게 .On(methodName string, ...)의 형태로 문자열을 이용하고 있습니다. argument 또한 단순히 mock.Arguments를 통해 얻을 수 있고 이에 대해 .Get을 통해 얻는 값은 interface{}이기 때문에 type assertion을 거쳐야만 내부 값을 복사해서 이용할 수 있습니다.\n3. gomock 이용  gomock은 mockgen(참고: https://github.com/golang/mock)이라는 코드 생성 CLI 도구를 이용합니다. 본 코드는 mockgen을 통해 생성된 mock 타입을 이용했습니다.\n func TestUserService_CreateWithMockgen(t *testing.T){ // gomock controller을 만들고 Finish 시켜주는 등의 불편함 존재.  ctrl := gomock.NewController(t) defer ctrl.Finish() mockUserRepository := NewGomockRepository(ctrl) // 원래의 type을 기반으로 method가 safe하게 제공됨. \tmockUserRepository.EXPECT().Create(gomock.Any()).DoAndReturn( func(input *UserCreateInput) (*ent.User, error) { t.Log(\u0026#34;Gomock을 이용한 Mocking. Args: \u0026#34;, input) return \u0026amp;ent.User{}, nil }) // 해당 이름의 유저가 있는지 확인  s := NewUserService(mockUserRepository) _, err := s.Create(\u0026amp;UserCreateInput{ID: \u0026#34;jinsu_umi\u0026#34;, Name: \u0026#34;Jinsu Park\u0026#34;, Password: \u0026#34;123123123\u0026#34;}) assert.NoError(t, err) } gomock controller라는 녀석을 생성해주고 finish 시켜줘야하는 불편함이 생겨났습니다만 모킹하고자 하는 메소드를 safe하게 제공받을 수 있습니다. .DoAndReturun() 메소드의 인자로서 어떤 함수로 모킹할 지를 전달하는데 사실 이 함수 자체를 interface{}가 받기 때문에 인자는 제가 정의하기 나름이라 원본 interface와 싱크되지는 않는 약간의 아쉬움이 존재합니다. 하지만 testify/mock처럼 args의 순서에 따라 .Get()을 한 뒤 interface{} 타입의 각각의 인자들을 type assertion할 필요도 없고 가독성이 더 좋다고 생각합니다.\n테스트 코드 작성해보기  *_test.go 형태의 파일 생성하기 func Test* (t *testing.T){...} 형태의 테스트 케이스 작성하기 로그나 Print문이 아닌 assert 문을 이용해 테스트 성공/실패 판단을 자동화하기  Golang에서 테스트 코드를 하기 위해선 우선 위의 3가지 사항만 알아두면 됩니다.\n그리고 추가적으로 golang에선 테스트 케이스 속에 nested된 테스트 케이스를 얼마든지 집어넣을 수 있습니다. 너무너무 편리하죠~! 그럼 이번에도 코드로 예시를 보여드리겠습니다. User에 대한 Domain/Business layer에서의 도메인 룰을 테스트하는 코드입니다. Data access 계층을 통해 User가 영속적으로 잘 저장되었는지보다는 도메인 룰을 테스트하려는 목적이기 때문에 Data access 계층(Repository)에 대해서는 모킹을 적용했습니다.\n// nested test case를 작성하고 싶은 경우. 이렇게 작성하십시오. func TestA(t *testing.T){ t.Run(\u0026#34;A-a\u0026#34;, func(t *testing.T){ t.Run(\u0026#34;A-a-1\u0026#34;, func(t *testing.T){ ... }) }) } // service_test.go 예시 func TestUserService_유저_생성(t *testing.T){ // repository mocking  ctrl := gomock.NewController(t) defer ctrl.Finish() mockUserRepository := NewGomockRepository(ctrl) // 원래의 type을 기반으로 method가 safe하게 제공됨. \tmockUserRepository.EXPECT().Create(gomock.Any()).DoAndReturn( func(input *UserCreateInput) (*ent.User, error) { t.Log(\u0026#34;Gomock을 이용한 Mocking. Args: \u0026#34;, input) return \u0026amp;ent.User{}, nil }) // 해당 이름의 유저가 있는지 확인  s := NewUserService(mockUserRepository) t.Run(\u0026#34;성공\u0026#34;, func(t *testing.T) { _, err := s.Create(\u0026amp;UserCreateInput{ID: \u0026#34;jinsu_umi\u0026#34;, Name: \u0026#34;Jinsu Park\u0026#34;, Password: \u0026#34;123123123\u0026#34;}) assert.NoError(t, err) }) t.Run(\u0026#34;에러) 너무 짧은 아이디\u0026#34;, func(t *testing.T) { _, err := s.Create(\u0026amp;UserCreateInput{ID: \u0026#34;short\u0026#34;, Name: \u0026#34;Jinsu Park\u0026#34;, Password: \u0026#34;123123123\u0026#34;}) assert.ErrorIs(t, err, ErrInvalidUserID) assert.ErrorIs(t, err, ErrInvalidValue) }) t.Run(\u0026#34;에러) 너무 긴 이름\u0026#34;, func(t *testing.T) { _, err := s.Create(\u0026amp;UserCreateInput{ID: \u0026#34;jinsu_umi\u0026#34;, Name: \u0026#34;니노막시무스 카이저 쏘제 쏘냐도르앤 스파르타 죽지 않아 나는 죽지않아 오오오오 나는 카이저 쏘제\u0026#34;, Password: \u0026#34;123123123\u0026#34;}) assert.ErrorIs(t, err, ErrInvalidUserName) assert.ErrorIs(t, err, ErrInvalidValue) }) t.Run(\u0026#34;에러) 너무 짧은 비밀번호들\u0026#34;, func(t *testing.T) { errorPasswords := []string{ \u0026#34;123\u0026#34;, \u0026#34;abc\u0026#34;, \u0026#34;a1b2\u0026#34;, \u0026#34;asd\u0026#34;, } for _, errorPassword := range errorPasswords { t.Run(\u0026#34;테스트 케이스) \u0026#34; + errorPassword, func(t *testing.T) { _, err := s.Create(\u0026amp;UserCreateInput{ID: \u0026#34;jinsu_umi\u0026#34;, Name: \u0026#34;Jinsu Park\u0026#34;, Password: errorPassword}) assert.ErrorIs(t, err, ErrInvalidPassword) assert.ErrorIs(t, err, ErrInvalidValue) }) } }) } TestUserService_유저_생성 이라는 테스트 케이스 안에 성공하는 경우와 올바르지 않은 ID나 이름, 비밀번호로 인해 에러가 발생해야하는 경우를 nested된 테스트 케이스로 작성했습니다. 특히나 비밀번호는 한 번 더 nested된 케이스를 작성했습니다. 같은 로직을 다양한 인풋을 이용해 테스트 하고 싶은 경우에는 errorPasswords 부분처럼 인풋 혹은 결과값을 배열로 선언한 뒤 테스트 하는 경우를 종종 보게되더군요. (참고: \u0026ldquo;Using Subtests and Sub-benchmarks\u0026rdquo; - Go Blog, \u0026ldquo;Uber Go style guide\u0026ldquo;의 test-tables 파트 - uber)\n test-with-ide.png \n run-each-test.png \n게다가 GoLand를 비롯해 Go언어를 지원하는 IDE를 이용 중이시라면 높은 확률로 이렇게 편리하게 테스트를 실행하고 결과를 확인하실 수도 있습니다. 그리고 두 번째 사진에 보이듯 내부 테스트 케이스에 대한 단일 실행도 가능하다는 점~! 이제 좀 더 테스트 코드를 쉽게 이용할 수 있겠군요. ㅎㅎ\n마치며 이번 편을 준비하면서 처음 유닛 테스트를 작성할 때 찾아봤던 자료들도 다시 한 번 읽어보고 etcd나 argocd 같은 Go로 작성된 유명 오픈 소스들은 어떻게 테스트 코드를 작성하는지도 찾아보는 등 꽤 조사를 많이 하게된 것 같습니다. 아무래도 테스트 코드라는 것이 어떠한 방향으로 테스트 코드와 개발을 설계해나갈지, 어디까지 테스트할 지와 같은 방법론적인 부분과 테스트 프레임워크 자체의 사용법, 모킹 프레임워크 사용법과 같은 약간은 Golang 특화적인 내용 등등 다양한 영역에 걸친 내용이라 쉽진 않은 것 같습니다. 그래도 이번을 기회 삼아 저도 평소에 궁금했던 mockery vs testify/mock 의 비교도 해볼 수 있었던 것 같고, mocking을 적용하는 방법에 대해 좀 더 자세히 알아볼 수 있었습니다! 감사합니다~!\n예시 코드: https://github.com/umi0410/how-to-backend-in-go/tree/master/testcode\n참고 자료  What\u0026rsquo;s the difference between a mock \u0026amp; stub? - https://stackoverflow.com/questions/3459287/whats-the-difference-between-a-mock-stub gomock vs testify - https://blog.codecentric.de/2019/07/gomock-vs-testify/ mockery Github Repository - https://github.com/vektra/mockery stretchr/testify Github Repository - https://github.com/stretchr/testify gomock Github Repository - https://github.com/golang/mock Using Subtests and Sub-benchmarks - https://blog.golang.org/subtests uber-go Guide - https://github.com/uber-go/guide/blob/master/style.md  ","date":"2021-07-17T12:46:54+09:00","image":"https://umi0410.github.io/blog/golang/how-to-backend-in-go-testcode/test-with-ide_hu7b61495424919d2e1c8d663236a41b57_159980_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/golang/how-to-backend-in-go-testcode/","title":"Golang으로 백엔드 개발하기 - 2. 테스트 코드 작성 및 Go에서 Mocking 이용하기 (gomock, testify 이용)"},{"content":"시작하며 Go 언어를 처음 시작한 지 벌써 1년이 지났다니 시간이 참 빠른 것 같습니다! Java는 Spring이라고하는 거대한 프레임워크가 자리 잡고 있어 딱히 어떤 프레임워크나 라이브러리를 사용할지에 대한 고민이 별로 필요 없었던 것 같습니다. 반면 Go 언어는 정형화된 아키텍쳐에 대한 내용이 별로 없고 프레임워크나 라이브러리의 대세도 참 빠르게 변하다보니 그게 장점이라면 장점이겠지만 이래저래 고생도 했네요.\n게다가 Go 언어는 개발 입문자들이 많이 사용하지 않는 언어라 그런지 아직 웹 백엔드 개발에 익숙하지 않은 (저처럼) 사람들을 위한 자료들은 많이 없었던 것 같습니다. 예를 들면 Java의 스프링이나 Node.js의 express, Python의 django 같은 프레임워크들은 인프런만 가봐도 A to Z로 알려주는 강의가 많죠. 하지만 Go 언어는 보통 언어 입문 내용들이 많고, \u0026ldquo;자~! 여러분들은 어차피 개발 초보자들이 아니시잖아요? 이쯤하면 Golang을 맛보셨으니 알아서들 입맛대로 쓰십시오~!\u0026quot; 식으로 입문 이후의 내용은 보통 동시성 패턴에 대한 내용들일 뿐 백엔드 개발을 위한 아키텍쳐나 Unit test를 어떻게 실제로 적용하는지 같은 예시는 많이 없었습니다. 그래서 이번엔 Golang으로 웹 백엔드 개발을 하는 과정에 대해 좀 적어볼까합니다!\n이번 글은 그런 내용을 다룰 시리즈 중 첫 번째로 Golang으로 데이터베이스 작업하는 것과 관련한 내용을 담아봤습니다. 데이터베이스를 어떻게 사용하는지에 대한 세세한 내용보단 어떤 프레임워크를 추천드리고, 그 프레임워크를 사용하는 모습이 어떠한지 가볍게 쓰윽 보시면 좋을 것 같습니다.\n3 layered architecture에 대해 간단히 짚고 넘어가겠습니다  Presentation layer - 어떻게 데이터가 클라이언트에게 보여줄 지에 대한 작업을 담당. 클라이언트는 이 계층을 통해 상호작용한다. Business/Domain layer - Presentation layer와 Data access layer의 사이에 위치해 비즈니스 룰과 그 룰을 따르는 비즈니스 로직을 구현. 보통 이 계층에 Service라고 하는 클래스(혹은 타입 등등)가 위치한다. Data Access Layer - DB에 접근하는 작업을 담당. DAO(Data Access Object, repository)가 위치한다.  대부분의 웹 애플리케이션은 3 layered architecture라고하는 구조로 기초로 하여 개발됩니다. 그 속에서 여러 단순 작업이나 개별적으로 개발하긴 번거로운 기능들이 존재할 수 있는데, 우리는 프레임워크나 라이브러리가 제공하는 기능들을 영리하게 가져다 씀으로서 좀 더 유지 보수 하기 쉬운 개발을 할 수 있을 것입니다.\nPresentation layer app.Get(\u0026#34;/users/:id\u0026#34;, func(ctx *fiber.Ctx) error { userId := ctx.Params(\u0026#34;id\u0026#34;) // ... 기타 작업 생략 \treturn ctx.JSON(map[string]string{ \u0026#34;userId\u0026#34;: userId, }) })  presentation-layer-output.png \nurl path variable은 어떻게 인식할 것인지, Golang의 map을 어떻게 JSON으로 직렬화할 지와 같은 로직들이 모두 프레임워크에서 제공됨.\n저는 Presentation layer는 대부분 웹 프레임워크 자체나 해당 웹 프레임워크가 제공하는 형태를 따르는 핸들러 단에서 제공이 된다고 생각하고 있습니다. 예를 들어 어떻게 응답을 JSON 형태로 제공할지 HTML, XML, 이진 데이터(gRPC)와 같은 내용들 말이죠. 즉 Presentation layer에서는 우리가 그닥 개발할 것이 많지는 않습니다.\nBusiness layer  Domain logic과 Business logic의 차이는 어느 정도 존재하는 걸로 알고 있습니다만 3 tier로 나눴을 때는 동일한 계층으로 보겠습니다.\n // Domain/Business layer 코드 예시 - 우리 도메인 특정 코드들이 많아 남(우리 팀 혹은 기업이 아닌 사람들)이 만든 오픈소스로 대체하기 힘듦. func (svc *UserService) updateProfileImage(requestUser *User, imageFile Image) (*UserResponse, error){ requestUser가 인증된 유저인가 인증된 유저가 아니라면 어떤 에러를 리턴할 것인가 imageFile이 제대로 된 Image인가 제대로 된 Image가 아니라면 어떤 에러를 리턴할 것인가 imageFile을 업로드 업로드 과정 중 에러 발생 시 어떤 에러를 리턴할 것인가 유저의 친구들에게 새로운 프로필 사진 업로드에 대한 알림 발송 ... 등등 생략 return 생략 } 반면 Domain/Business layer에서는 비즈니스 룰과 로직이라고 하는 우리 애플리케이션의 핵심적인 기능에 대한 구현이나 조합이 이루어지게 되는데 이 부분 타 오픈소스들로 대체되기는 힘든 부분이 많아 당연히 직접 개발하는 경우가 많습니다. 위의 코드를 보면 다양한 우리 서비스의 로직들이 구현되거나 조합되는 것을 볼 수 있습니다.\nData access layer  jpa-method.png \nJava의 JPA 프레임워크의 경우 기본적으로 CRUD 메소드를 제공해준다.\n django-method.png \nPython의 django의 경우 Join 기능도 기본적으로 제공해준다.\n이제 Data Access Layer만이 남았네요. 이 계층 또한 대부분의 객체지향 언어들에선 ORM이라는 개념을 통해 대부분의 프레임워크들이 많은 기능을 제공합니다. 그래서 필요한 경우에만 추가적으로 정의하는 형태로 편리하게 이용할 수 있습니다.\n그렇다면 Go에서는 어떻게 Data access layer를 편하게 이용할 수 있을까요? 이번 글에서는 Golang은 이 Data Access Layer(Repository)에서 어떤 DB framework을 사용하면 좋을지 어떤 식으로 사용해나갈 수 있을지에 대해 다뤄보려 합니다.\nGolang의 좋은 DB framework 좀 추천해주세요.  ent-gorm-stars.png \n차트 출처: https://star-history.t9t.io/\nGolang에는 Django나 JPA와 같은 강력한 데이터베이스 관리 프레임워크가 몇 년 전까지 존재하지 않았던 것 같습니다. 생 쿼리문을 직접 짤 게 아닌 경우라면 얼마 전까지는 gorm 이라고하는 프레임워크가 거의 유일한 선택지였던 것 같습니다. 하지만 요즘 추세를 보면 무섭게 ent라는 패키지가 쫓아오고 있는 모습을 보실 수 있습니다.\n👎 gorm 프레임워크 사용 후기 제가 golang 개발을 막 시작했던 때가 2020년 여름쯤이었기에 저도 gorm을 첫 데이터베이스 프레임워크로 사용했었지만 몇 가지 단점들이 존재했습니다.\n  문서의 내용이 빈약했고, 에러에 대한 설명이나 디버깅이 쉽지 않았다.\n문서의 내용들이 실질적인 다양한 케이스에 대한 예시나 설명이 부족했다고 느껴졌습니다. 또한 에러가 발생한 경우 왜 에러가 발생한 것인지 정확한 위치나 이유가 제공되지 않아 디버깅하기도 힘들었던 것 같습니다.\n  불편한 struct tag 기반의 테이블, 컬럼 설정\nGo가 강력히 type을 강제하면서 높은 안정성과 편의성을 제공하는 반면 gorm은 struct tag를 기반으로 여러 설정들을 관리하도록 개발되었습니다. Go 언어를 좋아하지만 struct tag에 대해서는 가뜩이나 조금의 불만을 갖고 있었는데 이런 저런 컬럼 설정들마저 struct tag로 이용하니 정확히 이 tag가 동작하는 태그인지, 왜 에러가 나거나 적용이 안되는지도 안전하게 확인할 수 없었고 무엇보다 가독성이 너무 너무~! 좋지 않았습니다.\n  제공되는 메소드가 별로 없다.\ngorm은 아주 가벼운 느낌이었습니다. 하지만 데이터베이스 프레임워크를 사용함에도 불구하고 너무나 제공되는 메소드가 없다고 느껴졌습니다. JPA나 Django는 조금의 설정만 해주면 CRUD와 Join까지 제공해주는데 gorm은 거의 정말 정말 기본적인 기능을 하는 메소드에 적절한 인자를 전달함으로써 동작시켜야합니다. 예를 들어 메소드가 어떤 매개변수를 왜 필요로 하는지에 내용이 그닥 없고 그냥 통으로 Create메소드에 알아서 적절한 inteface{}인자를 전달해야하는 형태라 Go의 장점인 강력한 type을 바탕으로한 안정성, 편의성을 누릴 수도 없었고 이럴꺼면 파이썬 쓰지\u0026hellip; 싶은 생각이 자주 들었습니다.\n  👍 추천하는 프레임워크 - ent 물론 위의 Github star history만 봐도 아실 수 있겠지만 ent는 매우 무섭게 성장 중인 Golang의 데이터베이스 관리 프레임워크입니다. 게다가 아마 Facebook에서 내부적으로 사용하다가 오픈소스화한 프로젝트인 걸로 알고 있는데 따라서 어느 정도의 완성도나 편의성이 보장되겠죠. ㅎㅎ 저는 사이드 프로젝트에서 gorm을 사용하다가 많은 불편을 느꼈고, Go 개발 커뮤니티에서 ent가 그렇게나 핫하다길래 ent로 data access layer를 마이그레이션했고 만족 중입니다. ㅎㅎ\nent의 사용 후기 및 특징은 아래와 같습니다.\n  타입을 바탕으로 안전하고 편리하게 테이블을 설계할 수 있다.\n테이블, 컬럼 정의들이 모두 ent 패키지의 컬럼 type 혹은 테이블 type, 관계 type 등등을 이용해서 정의할 수 있기 때문에 너무 너무 편리하죠. 게다가 복잡한 struct tag도 안녕~! ent는 미리 정의된 타입과 메소드들로 다양한 설정을 할 수 있습니다.\n  다양한 메소드 제공\ngo generate라고 하는 기능을 통해 우리가 정의한 스키마(테이블 및 필드 설정)을 바탕으로 다양한 타입과 메소드들을 만들어줍니다. 이 부분으로 인해 꽤 빌드 시간이 늘어나는 느낌이긴한데 그래봤자 2초 정도인데 ent가 제공해주는 메소드들로 인해 개발적 편의와 안정성이 훨씬 크다고 느끼기 때문에 만족하고 있습니다.\n  별 다른 문서가 필요 없다..?\n그냥 메소드를 기반으로 이용할 수 있다보니 그 사용 방법이 직관적인 편이고, 경우에 따라서는 코드 자체를 까보면 이해되는 별 다른 문서 없이도 이용할 수 있었던 것 같습니다.\n  참조 관계 설정이 좀 헷갈린다 (몇 안 되는 단점 중 하나)\n뭔가 From이나 To등을 통해 참조 관계를 정의하는데 이 From이 어떤 테이블을 From으로 생각하는건지 뭔가 많이 애매합니다. 커뮤니티를 보면 이로 인해 많은 분들도 혼란을 겪고 있는 현황입니다.\n하지만 너무 걱정은 마세요~! 이번 글에서 정리해드리려하니까요~! ㅎㅎ\n  Ent 패키지를 통해 Database 작업해보기 \u0026ldquo;시작하며\u0026ldquo;에서도 말씀드렸다시피 사실 Go 언어로 개발하시는 분들 중에 정말 개발 자체가 익숙하지 않은 분들은 정말 적으리라 생각합니다\u0026hellip;ㅎㅎ 따라서 너무 디테일한 내용을 직접 알려드리기 보다는 커다란 흐름이나 헷갈리는 요소들을 짚어드리는 방향으로 글을 작성해 보겠습니다.\n예시 Application - 여행 상품 관리 CRUD 서비스  1:N의 여행 상품 담당자 관계 - 담당자(일종의 유저)는 여행 상품과 1:N 관계  위의 관계를 갖는 여행 상품 관리 CRUD 서비스를 만들어보겠습니다. 본 글에서는 코드를 짧게 짧게 잘라서 올릴테니 원본 코드를 보고싶으신 분들은 제 깃헙을 참고해주세요.\n원본 코드 - https://github.com/umi0410/how-to-backend-in-go\n1. ent package 설치 # 참고: https://entgo.io/docs/tutorial-setup/#installation $ go get entgo.io/ent/cmd/ent 우선 ent는 다양한 명령 기능도 제공하고 저희는 그걸 필요로 하기 때문에 ent를 설치해주어야합니다. 스키마 정의 코드 또한 ent 를 이용해 자동으로 기본 형태를 제공 받을 수 있습니다. ent 패키지를 설치한 이후 저희는 다음과 같은 명령을 수행하게 될 것입니다.\n $ go run entgo.io/ent/cmd/ent init {{엔티티 이름}} - 엔티티 이름을 바탕으로한 초기 스키마 코드 생성 $ go generate ./ent - 정의했던 스키마를 바탕으로 한 많은 boilerplate 코드를 자동으로 생성  2. 스키마 설정 $ go run entgo.io/ent/cmd/ent init User # or shortly $ ent init User $ go run entgo.io/ent/cmd/ent init TourProduct # or shortly $ ent init TourProduct 위의 명령어들을 통해 User와 User가 관리하는 TourProduct라는 엔티티들의 스키마를 작성할 것입니다. 정확한 스키마 작성에 대한 설명은 생략하겠습니다. ent 공식 문서에 매우 잘 설명되어있고, examples in ent github에서 대부분의 예시 코드도 확인해볼 수 있기 때문입니다.\n⭐ 참조 관계 설정 시의 From과 To 다만 한 가지. Edge를 이용해 참조 관계를 설정할 때 From과 To의 사용에 대해 한 가지만 짚고 넘어가려 합니다. 저를 포함해 많은 ent 사용자분들께서 Edge 설정 시에 많은 혼란을 겪습니다. 그 이유는 일반적으로 우리는 1:N 관계에서 N쪽이 From, 1쪽이 To가 되며 N쪽 테이블에서 1을 참조하는 내용을 정의하는 반면 ent는 그 반대로 동작하기 때문입니다.\n(ent에서의 참조 관계 정의 방식에 대해 궁금하지 않으신 분들은 넘어가셔도 됩니다.)\n o2m-relation.png \n출처: https://entgo.io/docs/schema-edges\n위와 같은 경우 데이터베이스상으로는 Pet 테이블이 자신의 owner_id 컬럼을 통해 User을 참조하므로 Pet이 From, User가 To가 되며 이를 통해 owner라는 관계를 나타낼 수 있습니다. 하지만 ent는 그 반대로 User가 Pet을 관리(참조)한다는 의미로 User가 From, Pet이 To가 되어 pets라는 관계를 나타내는 식으로 정의합니다. 이러한 대조적인 방식으로 인해 많은 사람들이 혼란을 겪는 것 같습니다.\n굳이 이러한 방식이 합당한 이유를 찾아보자면\u0026hellip; 저는 collection의 아이템 관리 방식과 유사하게 관계를 정의하고 싶어서가 아니었을까 싶습니다.\n예를 들어 일반적으로 우리는 collection에 아이템을 추가할 때 다음과 같이 코드를 작성하지 데이터베이스의 방식처럼 아이템(N)이 자신이 속하는 컬렉션(1)을 설정하지 않습니다.\n// Go에서의 예시 // 1. 컬렉션 방식 user.Pets = append(user.Pets, newPet) // 2. 보통의 우리가 생각하는 데이터베이스의 방식. N쪽이 1을 참조 newPet.Owner = user // Java에서의 예시 // 1. 컬렉션 방식 user.pets.append(newPet); // 2. 보통의 우리가 생각하는 데이터베이스의 방식. N쪽이 1을 참조 newPet.setOwner(user); 아무튼 그래서 ent에서는 1:N 방식에서 1이 연관 관계의 주인처럼 동작/정의한다고 생각하고 작업하고 있습니다.(실제 문서에서도 From쪽 즉 1 혹은 User 쪽이 연관 관계의 주인이라고 기술되어있는데, 이 부분은 JPA와는 거의 반대라고 볼 수 있죠.) 사실 굳이 저렇게 정의하는 이유가 뭘까 나름의 이유를 붙여본 것이고, 그냥 예시를 참고하시면서 테이블의 참조 관계랑 반대라고 외우시길 추천(ㅎㅎ\u0026hellip;.;;)합니다!\n스키마 정의 및 적용 // ent/schema/user.go package schema import ( \u0026#34;entgo.io/ent\u0026#34; \u0026#34;entgo.io/ent/schema/edge\u0026#34; \u0026#34;entgo.io/ent/schema/field\u0026#34; ) // User holds the schema definition for the User entity. type User struct { ent.Schema } // Fields of the User. func (User) Fields() []ent.Field { return []ent.Field{ // 타입을 기반으로 안전하고 편리하게 컬럼을 정의할 수 있습니다. \tfield.String(\u0026#34;id\u0026#34;), field.String(\u0026#34;name\u0026#34;), field.Bool(\u0026#34;isActivated\u0026#34;).Default(true), } } // Edges of the User. func (User) Edges() []ent.Edge { return []ent.Edge{ // ent에서는 To를 정의하는 스키마, 즉 여기선 User \t// 가 참조 관계의 주인이라고 정의합니다. \t// 일반적인 JPA의 방식과는 반대입니다. \tedge.To(\u0026#34;products\u0026#34;, TourProduct.Type), } } $ go generate ./ent 아무튼 저는 User 스키마를 이렇게 설정했고 이 스키마들을 적용해 ent가 generate해준 코드들을 이용해보겠습니다.\n(TourProduct 스키마는 생략했습니다. 원본 코드를 참고해주세요.)\n3. repository 정의 ent가 데이터베이스 관련 작업을 위한 다양한 메소드를 지원해줍니다. 하지만 ent는 간단 간단한 메소드들을 조합해서 사용해야하기 때문에 비즈니스 로직을 구현하는 계층에서 매번 ent의 메소드를 조합하며 사용하는 것은 계층의 관심사와 책임을 흐리게 할 수 있습니다. 따라서 몇 가지 메소드들을 data access layer인 repository에 정의하겠습니다.\n예를 들면 특정 user가 관리하는 여행 상품을 모두 조회하는 FindAllByManager(managerID string)와 같은 메소드 말이죠. 이 작업을 위해서는 User 테이블과 TourProduct 테이블을 조인해야하는데 ent가 역시 이 기능을 제공합니다.\n// repository.go ... type TourProductRepository struct{ Client *ent.TourProductClient } func (repo *TourProductRepository) FindAllByManager(managerID string) []*ent.TourProduct{ result := repo.Client.Query(). // 특정 manager_id의 TourProduct를 조회하도록 조인  Where(tourproduct.HasManagerWith(user.ID(managerID))). WithManager(). AllX(context.TODO()) return result } 4. 실행 // main.go func main(){ ... fmt.Println(\u0026#34;전체 유저 조회\u0026#34;) for _, user := range userRepository.FindAll() { fmt.Printf(\u0026#34;User(id=%s, name=%s)\\n\u0026#34;, user.ID, user.Name) } fmt.Println(\u0026#34;--------------------------------------------------------------------------\u0026#34;) fmt.Println(\u0026#34;전체 여행 상품 조회\u0026#34;) for _, tour := range tourProductRepository.FindAll() { fmt.Printf(\u0026#34;TourProduct(id=%d, name=%s, manager=%s)\\n\u0026#34;, tour.ID, tour.Name, tour.Edges.Manager.ID) } fmt.Println(\u0026#34;--------------------------------------------------------------------------\u0026#34;) fmt.Println(user1.ID + \u0026#34;가 관리하는 전체 여행 상품 조회\u0026#34;) for _, tour := range tourProductRepository.FindAllByManager(user1.ID) { fmt.Printf(\u0026#34;TourProduct(id=%d, name=%s, manager=%s)\\n\u0026#34;, tour.ID, tour.Name, tour.Edges.Manager.ID) } } 전체 유저 조회 User(id=umi0410, name=박진수) User(id=devumi, name=개발자) -------------------------------------------------------------------------- 전체 여행 상품 조회 TourProduct(id=1, name=미국 뉴욕 여행, manager=umi0410) TourProduct(id=2, name=유럽 여행, manager=devumi) -------------------------------------------------------------------------- umi0410가 관리하는 전체 여행 상품 조회 TourProduct(id=1, name=미국 뉴욕 여행, manager=umi0410) 간단한 테스트용으로 2명의 유저를 만들었고, 각각의 유저가 관리하는 2개의 여행 상품을 만들어서 조회하는 프로그램을 만들어보았습니다. repository 계층에서 만든 FindAll()이나 FindAllByManager()와 같은 메소드들을 통해 편리하게 작업할 수 있네요!\n마치며 글을 쓰다보니 백엔드 개발에서 데이터베이스가 왜 필요하고 어떤 식으로 사용되는지를 다루는 글도 아니고, Golang에서 ent 프레임워크를 사용하는 방법을 자세히 알려주는 글도 아닌 이도 저도 아닌 글이 된 건 아닌가 싶기도 합니다.\n그래도 앞으로 Golang으로 백엔드 개발하는 것에 대해 천천히 한 5~6편 정도의 글을 써나가볼까하는데 이때 DB 관련된 내용을 우선 짚고는 넘어가야할 것 같기도 했고, 무엇보다 한글로 된 ent 자료를 하나도 찾을 수 없다는 점에서 데이터베이스 관련 프레임워크인 ent를 소개해보고자 했습니다! ㅎㅎ\n지면 사정 상 코드들을 잘라서 올렸는데 원본 코드는 https://github.com/umi0410/how-to-backend-in-go 에 올려놓을테니 참고해주시면 감사하겠습니다.\n참고  https://entgo.io/ https://github.com/ent/ent  ","date":"2021-07-03T12:46:54+09:00","image":"https://umi0410.github.io/blog/golang/how-to-backend-in-go-db/o2m-relation_huda98d5d877ff5063fa9c786910881686_21160_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/golang/how-to-backend-in-go-db/","title":"Golang으로 백엔드 개발하기 - 1. 데이터베이스 작업하기 (Ent 프레임워크 이용)"},{"content":"문제 설명 문제 출처: https://programmers.co.kr/learn/courses/30/lessons/67258\n 각각의 보석상은 하나의 보석을 판매 연속된 보석상들을 쇼핑하면서 방문할 때마다 보석을 구매 모든 종류의 보석을 구매할 때까지 진행 모든 종류의 보석을 구매하는 경우 중 가장 조금의 보석을 가장 앞선 보석상에서 구매하는 경우를 구하기  문제 접근 우선 단순하게 문제의 조건을 그림으로 나타내면 위와 같다. 결국 1번 상점부터 방문하는 경우, 2번 상점부터 방문하는 경우, \u0026hellip; 이런 식으로 진행해나간 뒤 가장 보석을 적게, 앞의 상점에서 사는 경우를 구하면 된다.\n👎 첫 번째 접근 \u0026lsquo;\u0026lsquo;각 경우마다 직접 구해볼까?'\n허걱.. 보석상은 최대 10만개를 입력으로 준다. 각 경우를 매번 직접 N번 상점부터 시작해서 모든 종류의 보석을 사는 경우를 구하는 것은 너무 비효율적일 것 같다.\n👍 두 번째 접근 \u0026ldquo;그러고보니 앞의 경우와 비교하면서 방금 제외된 보석만 추가되는 경우를 참고하며 구하면 되지 않을까?\u0026quot;\n즉 그림을 보면 두 번째 경우는 앞의 경우(첫 번째 경우)와 비교했을 때 A가 줄었고, 내가 방문한 보석상 중 A가 없다면 A를 구매할 때까지 보석상을 방문하면 된다. 세 번째 경우는 그와 반대로 앞의 경우(두 번째 경우)와 비교했을 때 B가 줄었지만 내가 방문한 보석상 중 B를 판매하는 곳이 있기에 새로운 보석상을 방문할 필요가 없다.\n=\u0026gt; 오호라\u0026hellip; 추상적으로 접근 방식을 바라봤을 때는 효율적인 것 같다! 이제 구현에서만 잘 구현하면 되겠다.\n구현 👍 올바른 구현 # 풀이법이 떠오르지 않는다. 처음 보는 유형이다... # 그냥 1번에서 시작해서 다 포함하는 경우 # 2번에서 시작해서 다 포함하는 경우 이렇게 다 세보면 효율성이 오바일까..? 최대 10만칸임... # 약간 dp스럽게 전꺼를 바탕으로 생각하는 게 좋겠다. # 전꺼를 없앤 대신 걔가 포함되어있으면 ㄱㅊ은 거니까. def solution(gems): end = -1 gems_set = set(gems) shopping_bag={} for gem in gems_set: shopping_bag[gem] = 0 for i in range(len(gems)): gem = gems[i] gems_set.discard(gem) shopping_bag[gem] += 1 if len(gems_set) == 0: end = i break answers = [(end+1, 0, end)] for i in range(1, len(gems)): found, end = shop(gems, i, end, shopping_bag) if not found: break length = end - i + 1 if length == answers[0][0]: answers.append((length, i, end)) elif length \u0026lt; answers[0][0]: answers = [(length, i, end)] # print(answers) answer = sorted(answers)[0] return [answer[1] + 1, answer[2] + 1] # end 포함 def shop(gems, start, end, shopping_bag): left_gem = gems[start-1] is_complete = False new_end = -1 shopping_bag[left_gem] -= 1 if shopping_bag[left_gem] == 0: for i in range(end+1, len(gems)): shopping_bag[gems[i]] += 1 # 이전에 뺸 녀석을 찾은 경우 if gems[i] == left_gem: is_complete = True new_end = i break else: is_complete = True new_end = end return is_complete, new_end 내가 방문하면서 구매한 보석들의 각각의 개수를 딕셔너리를 통해 관리했다.\n이전 시도 때 구매한 보석들의 각각의 개수를 참고해 만약 이전에 1번 보석상에서 A 보석을 구매하는 것을 시작으로 했다면 이번 시도에는 2번 보석상에서 어떤 보석을 구매하게 될 것이다. 이 경우 이젠 1번 보석상에서 A 보석을 구매할 일이 없으니 딕셔너리에서 A 보석의 개수를 1개 줄여주고 시작을 하는 것이다. 이후 구매 개수가 0개가 되는 보석이 없는지 확인하고 존재한다면 해당 보석을 구매할 때까지 계속해서 보석상을 방문해나가면 된다.\n 보석의 모든 종류를 구하기 위해서는 Set 자료 구조를 이용했다. 구매한 보석을 순서대로 리스트로 관리하며 리스트에 내가 구매한 보석이 있는지 찾는 방식은 __contains__ 를 이용하게 되고 이 작업만 놓고 보면 시간 복잡도가 O(n)이다. 반면 딕셔너리에 각 보석의 구매 개수를 이용해 구매하지 않은 보석이 있는지 확인하는 작업은 O(1)이다. 따라서 딕셔너리를 이용해 보석을 구매했는지를 확인하는 방식을 이용했다.  👎 실패했던 구현  이 부분은 개인적으로 기억하려는 의도로 작성한 것이기 때문에 설명이 이상할 수 있습니다.\n  list를 이용해 내가 구매한 보석을 순서대로 담았던 방식  위에서도 언급은 했지만 list를 이용해 내가 구매한 보석을 순서대로 담은 뒤 어떤 보석을 적어도 한 번 구매한 적이 있는지를 in 으로 찾는 방식을 이용한 경우이다. 게다가 이전 시도에서 left_gem을 제거한 뒤 새로운 shopping_bag을 대입하는 과정에서 아래와 같이 O(n)의 복잡도를 갖는 list slicing을 이용하다보니 더욱 비효율적이었다. 프로그래머스의 효율성 테스트를 5개 실패했다.\nshopping_bag = shopping_bag[1:] 1번 방식을 개선해 list slicing이 아닌 dequeue 자료구조 이용하기  list slicing 시 O(n)의 시간 복잡도를 갖는 list가 아닌 앞의 요소를 제거하면서도 O(1)의 시간 복잡도를 갖는 dequeue 자료구조를 이용했다. 하지만 프로그래머스 효율성 테스트를 2개 실패했다.\n즉 요점은 list든 dequeue든 둘 중 무엇을 이용하더라도 O(n)의 시간 복잡도를 갖는 in을 통해 특정 요소가 collection 내에 존재하는 것이 비효율적이므로 딕셔너리를 이용해 O(1)의 시간 복잡도를 갖는 방식을 이용하는 것이다.\n","date":"2020-09-06T19:11:07+09:00","image":"https://umi0410.github.io/blog/algorithm/kakao-gem-shopping/illust_hucf6765420ea389d80eec71a13daa7cfd_88664_120x120_fill_q75_box_smart1.jpg","permalink":"https://umi0410.github.io/blog/algorithm/kakao-gem-shopping/","title":"2020 카카오 인턴쉽 코딩 테스트 - 보석 쇼핑 풀이"},{"content":"Go 언어로 적용해보는 Computer Science의 첫 번째 내용으로 OS 관련 내용 중 이론적으로는 흔하게 접할 수 있지만 실제 적용에 대한 내용은 찾아보기 힘들었던 Mutex, Semaphore에 대해 알아보려한다.\nMutex와 Semaphore은 각각의 추상적인 개념을 바탕으로 OS나 Go 등에서 사용될 수 있기에 세부적인 내용은 문맥에 따라 달라질 수 있다고 생각한다. 예를 들어 Go에서의 Mutex는 주로 sync.Mutex를 이용한 서로 다른 Goroutine의 동시 접근에 대한 제어를 의미하는 반면, 다른 프로그래밍 언어나 OS에서는 주로 서로 다른 Kernel thread나 Process에 대한 동시 접근 제어를 의미할 수 있다.\nMutex Mutex는 Mutual Exclusion의 줄임말로 상호 배제를 의미한다. 즉 서로 다른 워커가 공유 자원에 접근하는 것을 제한한다는 말이다. Go에서는 이 \u0026ldquo;워커\u0026ldquo;가 Goroutine이 되고, 컨텍스트에 따라 프로세스가 될 수도, 스레드가 될 수도 그 외의 다른 존재가 될 수도 있다. 공유 자원이란 여러 워커가 동시에 접근하는 자원을 말하고 이 공유 자원에 대한 동시 접근을 제한해 thread-safe하게 작업하고자 하는 영역을 Critical section이라고 한다. Cricical section은 Mutex가 Lock을 수행한 뒤 Unlock되기 전까지의 영역이며, Lock과 Unlock은 Atomic한 작업이기때문에 어떠한 경우에도 동시적으로 수행될 수 없다.\n mutex-1.png \n이해하기 쉽게 예시를 통해 접근해보겠다. 원래 화장실 예제를 보고 굉장히 와닿았는데, 화장실보다는 피팅룸이 좀 더 청결한 것 같아서 피팅룸으로 예를 들어보겠다. 우리 귀여운 고퍼가 피팅룸을 한 개만 운영하는 옷가게에 방문했다고 가정해보자.\n mutex-2.png \n 피팅룸 - Critical section 고퍼가 피팅룸에 들어가서 옷을 입는 작업은 하나의 Goroutine  동시에 여러 고퍼가 하나의 피팅룸에서 옷을 갈아입으려한다면 😟 난처한 상황이 발생할 것이다. 그렇기때문에 피팅룸에는 Lock, Unlock 기능이 존재해야하는데, 이 기능을 구현한다해도 Atomic하게 동작할 수 있도록 제대로 구현하지 않는다면 한 워커가 Lock을 하는 사이에 다른 워커가 동시에 Lock을 걸려할 수 있고, 그 경우 두 워커가 동시에 한 피팅룸(Critical Section)에서 작업을 하려할 것이다. 그렇기때문에 Lock, Unlock은 Atomic해야하며 이렇게 각 워커를 상호 배제시켜 동시에 작업할 수 없도록 하는 것이 바로 Mutex이다.\nMutex를 이용한 Go program - Counter Count = 0에서 시작해 동시적으로 +1을 10만 번, -1을 10만 번 수행하는 카운터 프로그램\n그럼 실생활에 비유한 Mutex는 이 정도로 마치고, Mutex가 어떻게 사용되는지 Go로 짠 간단한 Counter 프로그램을 통해 알아보자.\n Mutex를 잘 적용한 경우 - 몇 번을 수행하든 결과는 0. Mutex가 적용되지 않은 safe하지 않은 경우 - 결과가 0이 나오지 않을 수 있다.  특히 Go에서는 sync 패키지의 Mutex라는 type을 통해 간단하게 Mutex 기능을 이용할 수 있다. 동일한 Mutex struct를 참조한다면 같은 Key를 이용한다는 개념이고, mutex.Lock()을 호출한 뒤 mutex.Unlock()이 호출되기 이전까지가 Critical section이 되며 다른 goroutine들은 같은 Mutex(즉 키)에 의한 Critical section에 진입할 수 없다.\n내가 처음 Mutex를 처음 접했을 때 한 가지 헷갈렸던 것은 당시 Critical section이라는 개념이 없었기 때문에 Lock을 호출하기만 하면 런타임에 알아서 스마트하게 Lock과 Unlock사이의 변수들에 대한 접근 중 동시적인 접근만을 잠시 블락해주는 줄 알았는데, 사실 그 사이 변수들중 동시적으로 접근하려는 변수에 대해서만 잠시 블락해주는 게 아니라 Lock과 Unlock 사이를 Critical section으로 만드는 것이었다.\n자 그럼 Count라는 하나의 int형 변수에 동시적으로 접근을 하는 상황을 극대화하기 위해 여러 고루틴으로 작업해보겠다. +1을 1만 번하는 고루틴을 10개, -1을 1만 번하는 고루틴을 마찬가지로 10개 이용하겠다.\n전체 코드 참고: https://play.golang.org/p/xaLE1YkAdvd\n output-1.png \n 우리가 일반적으로 익숙한 Sync 즉 동기적인 순차적 진행 시에는 당연히 10만번 +1, 10만번 -1 후에 결과가 0이었다. 동시적 접근을 수행하자 10만번 +1, 10만번 -1을 했지만 Count += 1 을 수행하던 중 또 다른 Count += 1이 수행되는 등의 원치않던 상황이 야기될 수 있기에 결과값이 0인 경우를 찾기 힘들다. 여러 Goroutine이 concurrent하게 진행하되 Count += 1, Count -= 1 과 같은 동시적 접근이 수행되어서는 안되는 영역은 Critical Area로 설정해 상호 배제적으로 작업되게 하기 위해 Critical section의 전/후에 sync.Mutex를 이용해 Lock, Unlock 기능을 넣어주자 기대했던 대로 0의 결과를 얻을 수 있었다.  💡 Mutex.. 그래서 언제 써요? 사실 Mutex라는 개념이 그렇게 어려운 것도 아니고, 사용법 자체가 어려운 것도 아니다. 나는 하지만 항상 \u0026ldquo;언제\u0026rdquo;, \u0026ldquo;왜\u0026rdquo; 써야하는지를 궁금해하는 편이다.\n Critical section에서 많은 시간이 소요되는 경우? ⇒ ❌ 위의 counter와 같은 작업은 대부분의 작업이 critical section 속에 있다고 볼 수 있다. 이 경우 동시적인 작업과 함께 Lock, Unlock을 하며 critical section을 관리하는 것보다 애초에 작업 자체를 동기적으로 수행하는 게 나을 수 있다. 왜냐하면 동시적 작업 속에서 Mutex를 통해 동기적으로 작업할 수 있도록 하는 것에서 오는 오버헤드는 분명히 존재하고, Critical section 밖에서 효율적으로 동시적으로 작업을 진행했다하더라도 critical section에서 병목이 발생해버려 전체적인 Throughput이 안 좋아질 것이다. Critical section에서는 적은 시간이 소요되고, 대부분은 동시적으로 작업이 가능한 경우? ⇒ ⭕ 예를 들어 어떤 API를 여러 번 호출한 뒤 그 응답 중 일부를 계속해서 더해 결과를 내는 Reduce 작업을 수행한다고 치자. 순차적으로 수행할 시 오랜 시간이 소요될 API 호출은 동시적으로 진행하고, 결과에 대한 Reduce 작업만 잠시 Critical section내에서 작업한다면 아주 좋은 성능과 함께 안전하게 작업할 수 있을 것이다!  Semaphore  semaphore.png \nMutex에서 대부분의 Lock이나 Critical section에 대한 내용을 설명했기 때문에 Semaphore에서 더 설명할 내용이 많지는 않다. Semaphore의 동작에 대한 간단한 예시는 위의 그림과 같은데, 5마리의 고퍼가 존재한다해도 동시에 접근할 수 있게하는 고퍼를 3개로 제한한다면 2마리의 고퍼는 피팅룸에 들어가지 못하고 블락된다. Locked와 Unlocked 상태 뿐인 Mutex와 달리 Semaphore는 임의의 개수를 세는 Counter처럼 동작해 임의의 개수의 워커만이 Critical section에 동시적으로 접근할 수 있도록 한다.\nCounter를 예로 들자면 10개의 워커가 존재한다해도 \u0026ldquo;5\u0026quot;를 세는 Semaphore는 한 워커가 Critical section에 진입할 때마다 Counter 값을 1씩 낮추고, Critical section을 탈출할 때마다 Counter 값을 다시 1씩 높인다. 만약 어떤 워커가 Critical section에 진입하려는데 Count 값이 0이라면 초기에 계획했던 대로 5개의 워커가 이미 동시적으로 작업중이라는 의미이므로 이 워커는 한 워커가 Critical section을 나오면서 Counter 값을 다시 1 증가시킬 때까지 Block된다.\nMutex의 Lock과 Unlock이 Atomic하기에 어떠한 경우에도 동시적으로 수행될 수 없었듯이 Semaphore의 Counting 작업 또한 Atomic해야하고 그래야만 Thread-safe한 counter로 동작할 수 있다. Semaphore가 수행하는 Counting 작업은 주로 try를 뜻하는 네덜란드어 Proberen의 앞 글자를 딴 P와 increment를 뜻하는 Verhogen의 앞 글자를 딴 V로 두 가지가 표현하는 듯하다. P는 Count 값이 0이 아니면 작업을 수행하겠다는 의미하며 만약 Count 값이 0이라면 0이 아닌 값이 될 때까지 wait했다가 0 아닌 값이 되면 count 값을 1 감소시키며 작업을 시작한다. V는 작업을 마치며 count 값을 다시 1 증가시키겠다는 의미이다.\nMutex와 Binary semaphore의 유사한 점 동시 접근 워커를 1개로 제한하는 Semaphore의 경우 Count 값이 0과 1 두 개로만 존재할 수 있는데 이를 Binary semaphore라고한다. 특히 이는 Locked와 Unlocked라는 두 가지의 상태만을 갖는 Mutex와 유사하다. 하지만 Binary semaphore는 mutex가 유사한 기능을 할 뿐 동일하지는 않다는 의견이 많다. 그 이유는 Mutex는 Lock 방식을 이용하고 Semaphore는 Signal(신호) 방식을 이용하기 때문이다. Lock 방식의 경우 Lock을 수행한 워커만이 Unlock을 할 수 있는 반면 Signal 방식은 try(작업 시도)를 수행한 워커가 아니더라도 increment(작업 완료) 신호를 보낼 수 있기에 서로 명백히 동작 방식이 다르다는 것이다. (하지만 이 부분에 대해서는 직접 실습해보지는 못했다.)\n*Semaphore를 이해하는 데에 있어 내가 착각해서 헤맸던 부분은 바로 Semaphore의 요점이 thread-safe한 count라고 착각*했던 것이다. 하지만 semaphore의 요점은 여러 워커에 대해서 thread safe한 count 기능을 제공하는 것이 아니라 임의의 숫자만큼의 동시적 접근을 허용하고, 그 이상은 Block 상태로 대기시킨다는 것이다.\nSemaphore를 이용한 Go Program 1 - Counter go에서 Semaphore를 이용하는 방법은 크게 2가지가 있는 것 같다. 아래 두 가지 방법 중 좀 더 Go스러운 channel을 이용해보겠다.\n Go 특유의 자료형인 channel을 이용하기 - channel은 여러 goroutine의 concurrent한 작업간 데이터 전송은 물론이고 동시적인 작업 중 데이터를 편리하게 동기화해주는 녀석이다. 따라서 동기적인 count와 유사한 기능을 내재하고있다. golang.org/x/sync/semaphore의 Weighted를 이용하기 - Semaphore의 P를 Acquire, V를 Release로 구현해 이용할 수 있게 했다.  전체 코드는 Mutex와 동일하며 마찬가지로 다음 링크로 참고해볼 수 있다: https://play.golang.org/p/xaLE1YkAdvd\nBuffered channel을 이용한 Semaphore in Go func DoSemaphore(maxConcurrent int64){ for i := 0; i \u0026lt; 5; i++{ Count = 0 wg := \u0026amp;sync.WaitGroup{} sem := make(chan struct{}, maxConcurrent) wg.Add(20) for i := 0; i \u0026lt; 10; i++{ go Add1Sem(wg, sem) go Sub1Sem(wg, sem) } wg.Wait() fmt.Println(\u0026#34;Concurrent goroutines + Semaphore\u0026#34;, maxConcurrent, \u0026#34;Result(Desired 0):\u0026#34;, Count, \u0026#34;\u0026#34;) } fmt.Println(\u0026#34;=====================================\u0026#34;) } func Sub1Sem(wg *sync.WaitGroup, sem chan struct{}){ defer wg.Done() for i := 0; i \u0026lt; 10000; i++{ sem \u0026lt;- struct{}{} Count -= 1 \u0026lt;-sem } } channel을 이용한 Semaphore 코드를 설명하기 위해 전체 코드 중 일부를 가져와보았다. Semaphore를 이용하기 위해 Unbuffered channel이 아닌 Buffered channel을 이용하는 이유는 다음과 같은 Unbuffered channel의 동작 방식이 Semaphore의 동작 방식과 동일하기 때문이다.\n Buffer의 크기만큼은 동시적으로 channel에 값을 넣으려는 시도가 허용됨. buffered channel이 꽉 찬 경우 채널에서 값을 꺼내지 않는 이상은 추가적으로 Channel에 값을 넣으려시도하는 goroutine은 Block됨.   output-2.png \n(앞서 코드를 링크한 go playground에서 코드를 바로 실행해볼 수 있다.)\n동시 접근 워커를 1개로 제한하는 Semaphore는 Binary Semaphore로서 Mutex를 이용했을 때와 유사하다고 말했듯이 thread-safe하게 count 작업이 이루어져 예상되는 값이었던 0이 출력됨을 확인할 수 있다. 하지만 동시접근이 가능한 워커가 1개 이상이 되면 thread-safe하지 않게 되고, 동시접근 워커 개수가 많아질수록 결과가 더 부정확한 경향이 있는 것으로 나타났다.\n💡 Semaphore.. 그래서 언제 쓰나요?  Mutex는 thread-safe한 작업을 할 때 사용하면 되는 것 같은데 Semaphore는 개념은 알겠는데 언제 써야할 지를 잘 모르겠네요. 일정 개수만큼만 동시 접근을 허용하려는 경우가 있을까요?\n Semaphore을 사용하기 좋은 케이스는 동시에 접근할 수 있는 워커 수를 제한하는 경우이고, 이는 주로 전체 작업이 늘어지는 것을 방지하고자 하는 경우에 이용된다. 많은 작업을 동시에 수행하려하면 먼저 수행될 수 있는 작업은 먼저 수행되도록 하기보다는 전체적으로 모든 작업이 늘어지게 되고 CPU나 Memory 리소스를 많이 소모하게 되고 이는 서버의 안정성에도 좋지 않다. Semaphore를 이용해 동시 접근 워커 수를 제한하고자하는 케이스는 Go의 Worker pool 패턴을 이용하는 경우나 Pipeline pattern을 이용하는 경우와 유사하다.(처음엔 Go의 모든 Concurrent pattern들을 개별적으로 구별지으려했었는데, 공부하다보니 일정한 개수의 Worker를 이용하는 Worker Pool pattern, Channel을 기반으로 작업 내역을 쪼개어 실시간 처리하는 Pipeline, 한 채널, 여러 Goroutine을 이용하는 Fan-in Fan-out pattern 등등 다들 유사하고 연관이 되어있더라.)\nSemaphore로 동시 접근 Worker 수를 제한하지 않고 모든 Goroutine을 동시적으로 수행하는 경우엔 어떻게 될까?\nLogical Processor 개수를 훨씬 넘는 모든 Goroutine 동시적으로 작업을 진행할 경우 아무리 Goroutine이 concurrent한 작업 수행에 뛰어난 성능을 보인다할지라도 과하게 많은 수의 Goroutine은 성능 저하를 야기하지 않을까 예상했다. 하지만 user-level thread 혹은 green level thread의 일종인 Goroutine은 OS(혹은 Kernel) thread와 달리 Context switch로 인한 penalty가 거의 없어서인지 거의 Throughput 면에서의 성능 차이가 없었다.\n Kernel level thread는 OS가 스케쥴링을 담당하기 때문에 Go 프로그램이 뭐 어떻게 할 수 있는 게 아니지만 Goroutine은 프로그램이 실행되는 동안 Go 런타임이 스케쥴링을 담당한다. 같은 Kernel level thread에 속한 User level thread인 goroutine간의 switch는 cost가 거의 없다. 즉 goroutine이 많든 적든 Kernel level thread간의 context switch cost는 동일하다고 볼 수 있고, 해당 Kernel level thread에 속한 goroutine간의 context switch cost는 거의 없다.\n  참고: Kernel level thread에 대한 스케쥴링은 주로 Preemptive 방식을, User level thread에 대한 스케쥴링은 주로 Cooperative한 방식을 이용하지만 Goroutine은 User level thread임에도 Go 1.14 버전부터는 10ns를 기준으로 goroutine을 switch 할 수 있는 asynchronously preemptive한 스케쥴 방식을 지원한다고 한다. 하지만 Go 1.14가 릴리즈된 지 얼마되지 않아서인지, asynchorously preemptive scheduling에 대해서는 명확히 설명된 문서를 찾기 힘들었다. User level thread의 예로는 RxJava in Java, Coroutine in Kotlin, Goroutine in Golang이 있다.\n 하지만 Throughput 적인 측면보다는 Machine의 Resource 소모 측면에서는 모든 Goroutine이 동시적으로 수행되는 구조보다는 Semaphore을 바탕으로한 동시에 일정 개수의 워커만이 작업하는 Worker Pool 구조가 훨씬 Memory나 CPU 리소스를 적게 소모하는 듯 했다. 또한 100개의 동시 요청을 수행하는데 동일한 Throughput으로 약 10초가 걸린다고 치면, 요청당 goroutine을 생성하는 경우는 첫 번째 요청도 거의 10초가 걸린 반면 Semaphore을 이용한 경우는 먼저 온 요청은 대체로 빠르게 먼저 처리되는 경향을 보였다. 이 차이는 처음 요청을 보낸 사용자 마저 10초를 기다리게 할 것이냐, 0.1초만에 응답을 받도록 할 것이냐의 차이이다. 또한 전체 작업이 늘어지면 그 작업에 대한 메모리 점유가 지속되기에 메모리 측면에서도 비효율적이다.\nSemaphore을 이용한 Go Program 2 - 이미지 크기 변환기 마침 이번에 프로젝트에서 thumbnail 생성, image 크기 변환, hashed uri 생성 작업 등을 담당하는 image processing 서버를 개발하려했는데, image를 불러오는 I/O 작업 이후의 image processing은 CPU Bound 한 작업이기 때문에 동시적으로 동작하는 Goroutine이 일정 숫자(대게 Logical Processor 개수) 이상으로는 많아져봤자 크게 효율이 없을 것이라 예상했고, 그와 관련해 간단하게 이미지 크기 변환 프로그램을 하나 만들어 테스트 해보았다.\n약 2MB의 이미지에 대한 크기 변환 작업 요청이 동시에 30개 들어왔다는 가정을 했고, semaphore을 이용한 경우 동시적으로 최대 4개의 worker(goroutine)이 작업을 수행할 수 있게, concurrent를 이용한 경우는 30개의 요청 모두 동시적으로 작업을 하는 경우이다.\n(코드 참고(인터넷 액세스를 하는 경우 Playground에서 동작하지는 않는듯하다): https://play.golang.org/p/Vfyw6uCOIuL)\n 사실 Logical Processor 8개, RAM 16GB의 개인 노트북으로는 그 차이가 많이 나지는 않아서 AWS EC2 t2.micro instance에서 성능을 테스트해봤다.\n ubuntu@ip-172-31-12-2:~$ ./sem semaphore 변환하고자하는 Image들을 메모리에 Load했습니다. 2021/01/18 20:28:13 Elapsed: 1.974380959s ... 생략 2021/01/18 20:28:26 Elapsed: 1.159552779s 2021/01/18 20:28:26 Elapsed: 1.016627316s 2021/01/18 20:28:26 Total elapsed: 14.959516003s ubuntu@ip-172-31-12-2:~$ ./sem concurrent 변환하고자하는 Image들을 메모리에 Load했습니다. Killed 놀랍게도 동시적으로 30개의 요청을 보내는 경우에 30개의 모든 goroutine이 동시에 작업을 시도할 때에는 과도한 리소스 사용으로 인해 아예 OS가 프로세스를 Kill해버렸다. 즉 Semaphore로 동시 접근 Worker 수를 제한하지 않는 경우 t2.micro 인스턴스로 돌리는 image 서버에 동시에 30개의 image resizing 요청이 들어오면 process가 죽어버린다는 말이다\u0026hellip;!\n또한 앞서 말했듯이 동시에 요청하는 사용자가 많아지더라도 Semaphore을 이용한 경우는 나중에 들어온 요청일 수록 처리가 늘어지지만, Semaphore를 통해 동시 작업을 제한하지 않는 경우는 전체 작업이 늘어진다.\n이렇게 Semaphore는 동시 접근 워커 수를 제한하여 전체적인 Throughput 측면보다는 리소스 소모적인 측면과 먼저 처리될 수 있는 작업은 먼저 처리되도록 할 수 있다는 면에서 장점이 있음을 확인할 수 있었다.\n마치며 Mutex나 Semaphore의 개념이나 설명과 같은 이론적인 내용은 구글링을 통해 어렵지 않게 얻을 수 있는 흔한 지식인 반면, 정확히 언제 쓰면 좋을지, 언제 쓰일 수 있을지와 같은 실용적인 내용은 찾아보기 어려웠기때문에 Go를 통해 Mutex와 Semaphore을 이용해보는 간단한 프로그램을 만들어 실습해보았다.\n예제 프로그램으로서 코드를 간결하고 읽기 쉽게 깔끔하게 제공해보고자했는데, 그러기 쉽지 않았던 것 같아 아쉽다. 다음엔 기회가 되면 go의 benchmark test를 이용해보면 어떨까싶다.\n참고 Goroutine과 Goroutine scheduling에 대해 - https://thegopher.tistory.com/3\n화장실에 비유한 뮤텍스와 세마포어 - https://worthpreading.tistory.com/90\nsemaphore in Go https://medium.com/@deckarep/gos-extended-concurrency-semaphores-part-1-5eeabfa351ce\ncooperative vs preemptive - https://medium.com/traveloka-engineering/cooperative-vs-preemptive-a-quest-to-maximize-concurrency-power-3b10c5a920fe\npreemptive scheduling in go - https://blog.puppyloper.com/menus/Golang/articles/Goroutine과 Go scheduler\n","date":"2021-01-20T15:25:54+09:00","image":"https://umi0410.github.io/blog/golang/go-mutex-semaphore/mutex-2_hu253887d6f424b5a6c4c14b3309becda5_68035_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/golang/go-mutex-semaphore/","title":"Go 언어로 적용해보는 Computer Science - Mutex와 Semaphore"},{"content":"시작하며 요즘 Go와 Java 모두를 이용해 개발을 하다보니 각각의 장단점에 대해 느껴볼 수 있었다. Go는 리소스를 적게 먹으며 코드가 간결하고 라이브러리나 프레임워크 또한 심플해서 적용하기 편하다. Java는 이런 저런 기능이 많은 반면 그런 기능을 이용하기 위해 이해해야하는 내용들이 많고, 코드가 투명하지는 않다(다양한 Annotation을 이용하게 되면서 코드가 투명하게 그 동작을 나타내지 않음). Java의 장점 중에서는 특히나 객체지향의 대표적인 언어답게 상속과 다형성을 능력에 따라 자유자재로 이용할 수 있다는 점이 매력적이었다.\nGo 언어를 좋아하는 입장에서 개인적으로 이런 객체지향적인 특징이나 예외 처리를 제외하고는 딱히 Java가 Go에 비해 갖는 장점이 크게 느껴지지 않았다. 예외 처리는 Go가 바라보는 방향이 일반적인 예외 처리와 다르기에 어쩔 수 없지만, 객체지향적의 특징들은 어떻게 적용해볼 수 있을까하는 생각에 공부를 좀 해봤고 그 내용을 정리해본다. (기회가 된다면 Go에서 error를 다루는 철학에 대해 추가적으로 공부해보고싶다.)\n❗Go에 대한 기본적인 내용을 정리해보는 것이 아니라 객체 지향 관점에서 바라본 Go에 대한 내용을 정리해보는 것이므로 Go의 기초 내용에 대한 설명은 생략할 것이므로 Go에 대한 기초 이해가 없다면, 그 부분을 먼저 알아보는 것을 추천한다!\n예시 코드 Go에서 객체 지향을 적용한 간단한 계산기 프로그램을 예시로 작성해보았다. 전체 소스코드를 다 볼 필요는 없겠지만 필요에 따라 참고할 수 있도록 아래와 같이 첨부한다.\nmain 패키지 - /main.go\npackage main import ( \u0026#34;bufio\u0026#34; \u0026#34;calculator/calc\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;strings\u0026#34; ) func main(){ var ( calculator *calc.Calculator = calc.NewCalculator() // 계산기 struct  operationUnit calc.OperationUnit // OperationUnit이라는 interface type을 통해 Polymorphism 이용  operationResult float64 // 계산 결과를 담음  operationErr error // 계산 수행에 대한 error을 담음  ) // main package에서는 calc package에 정의된 unexported name인 id에 접근할 수 없다.  //calculator.id = \u0026#34;Jinsu Park\u0026#34;  scanner := bufio.NewScanner(os.Stdin) // 연산 option을 위해 값을 입력받기 (e.g. 1)  fmt.Printf(`A simple calculator program. ======================================================= Operation options 1. Mulitply a, b float 64 2. Sqaure val, square float64 Please input an int for your desired operation. \u0026gt;\u0026gt;\u0026gt; `) scanner.Scan() option, _ := strconv.Atoi(scanner.Text()) // args를 위해 입력받기 (e.g. 10 20)  fmt.Printf(\u0026#34;Please input floats as args.\\n\u0026gt;\u0026gt;\u0026gt; \u0026#34;) scanner.Scan() inputs := make([]float64, 0) for _, input := range strings.Split(scanner.Text(), \u0026#34; \u0026#34;){ f, _ := strconv.ParseFloat(input, 64) inputs = append(inputs, f) } // 연산에 대한 multiplexing. 즉 option에 따른 연산을 수행한다는 의미  // operationUnit이라는 OperationUnit interface type을 통해 Polymorphism 이용  switch option{ case 1: operationUnit = calculator.Multiplier case 2: operationUnit = calculator.SquareMultiplier } // Operate라는 기능을 다양한 동작으로 수행할 수 있다.  operationResult, operationErr = operationUnit.Operate(inputs...) // result  if operationErr != nil{ fmt.Println(\u0026#34;[Error]\u0026#34;, operationErr) } else{ fmt.Println(\u0026#34;Result:\u0026#34;, operationResult) } } calc 패키지 - /calc/calc.go\npackage calc import ( \u0026#34;errors\u0026#34; ) var autoIncrementID int = 1 type Calculator struct{ id int // 제조된 Calculator을 식별하기 위한 ID. 변수명이 소문자로 시작하므로 export 되지 않는다.  Multiplier OperationUnit SquareMultiplier OperationUnit } type OperationUnit interface{ IsValidInput(args ...float64) bool Operate(args ...float64) (float64, error) } type MultiplyOperationUnit struct{} type SquareOperationUnit struct{ MultiplyOperationUnit // Embed의 예시 } func NewCalculator() *Calculator{ c := \u0026amp;Calculator{ id: autoIncrementID, Multiplier: \u0026amp;MultiplyOperationUnit{}, SquareMultiplier: \u0026amp;SquareOperationUnit{MultiplyOperationUnit: MultiplyOperationUnit{}}, } autoIncrementID += 1 return c } func (unit *MultiplyOperationUnit) IsValidInput(args ...float64) bool{ if len(args) != 2{ return false } return true } func (unit *MultiplyOperationUnit) Operate(args ...float64) (float64, error){ if !unit.IsValidInput(args...){ return 0, errors.New(\u0026#34;MultiplyOperationUnit의 args가 유효하지 않습니다.\u0026#34;) } return args[0] * args[1], nil } //func (unit *SquareOperationUnit) IsValidInput(args ...float64) bool{ // ... 필요에 따라 정의하면 Override처럼 이용 가능 //}  func (unit *SquareOperationUnit) Operate(args ...float64) (float64, error){ // *SquareOperationUnit에 대한 IsValidInput 메소드는 정의한 적 없지만  // Embedding을 통해 일반적인 OOP에서 부모의 메소드를 이용하듯이 이용 가능.  if !unit.IsValidInput(args...){ return 0, errors.New(\u0026#34;SquareOperationUnit의 args가 유효하지 않습니다.\u0026#34;) } var ( // val^square 즉 \u0026#34;val의 square 제곱\u0026#34;에 대한 계산  val = args[0] square = args[1] reduced float64 = val // 제곱 연산 중 값을 담아 놓는 변수  cnt = float64(1) // 제곱 연산 루프를 위한 counter  err error ) for ;cnt \u0026lt; square; cnt ++{ reduced, err = unit.MultiplyOperationUnit.Operate(reduced, val) if err != nil{ return 0, err } } return reduced, err }  preview.png \n declaration.png \n Calculator라는 계산기 struct가 존재  ✨ id field는 소문자로 시작하기때문에 외부에서 함부로 접근할 수 없도록 data를 캡슐화   계산기에서 각각의 연산을 담당하는 OperationUnit interface존재 OperationUnit interface는 Input의 유효성을 검사하는 IsValidInput 메소드와 연산을 수행하는 Operate 메소드 존재 각각의 연산을 담당하는 Unit은 OperationUnit interface가 정의한 메소드들을 구현함으로써 duck-typing을 통해 OperationUnit interface로 사용된다. ⇒ ✨ interface를 통한 추상화와 다형성 이용 가능  OperationUnit interface로 사용이 가능한 struct의 예시  MultiplyOperationUnit struct - 곱셈 연산을 담당 SquareOperationUnit struct - 제곱셈 연산을 담당  ✨ Embed를 통해 MultiplyOperationUnit을 상속한 것처럼 field와 method를 사용 가능        객체지향적 관점에서의 Go에 대해 Go는 아무래도 객체 지향 언어라고 하지는 않는 듯하다. 이에 대해선 다양한 의견이 있는 것 같은데 Post-OOP 언어라는 사람도 있고 OOP 언어는 아니지만 Object-Oriented하게 할 수 있으므로 OOP 언어이면서 OOP언어가 아니라는 사람도 있다.\nOOP에는 크게 4가지 원칙이 있다.\n 연관된 변수와 함수를 클래스로 묶으며 외부에서 특정 데이터나 기능에 접근하지 못하도록 하는 정보를 은닉해주는 캡슐화 부모 객체의 field, method를 자식 객체가 이용할 수 있도록해주고, Override할 수 있게 해주는 상속 세부 사항은 제외하고, 어떤 기능이 존재하는지 등의 추상적인 정보만으로도 이용할 수 있게 해주는 추상화 즉 하나의 타입이 여러 타입으로 이용될 수 있으며, 각각이 다양하게 동작할 수 있는 다형성  그리고 대표적인 OOP 언어인 Java는 이러한 내용들을 아주 잘 이용할 수 있게끔 되어있다. OOP 언어가 아닌 Go에서 이러한 OOP의 특징이자 장점인 요소들을 어떻게 적용할 수 있을지 알아보도록하겠다.\n💊 Encapsulation(캡슐화) Go에서는 Export를 통해 캡슐화를 이용할 수 있다. Export에 대해 간단히 설명하자면 private, public 을 이용해 변수나 함수에 접근 제한을 두는 것이 아니라 이름이 대문자냐 소문자냐에 따라 패키지 외부에서 접근을 제어하는 것을 말한다.\nGo의 Export 이용 방법은 많이 찾아볼 수 있으니 캡슐화에 초점을 맞춘 그 쓰임에 대해 알아본다.\n// Calculator에 대한 정의와 구현을 담당하는 calc 패키지 package calc type Calculator struct{ ID int // 제조된 Calculator을 식별하기 위한 ID. 변수명이 소문자로 시작하므로 export 되지 않는다. ... } 위와 같이 Calculator를 정의하는 calc라는 패키지가 있다고 가정하다. Calculator를 식별하는 ID에 대한 작업은 calc 패키지에서 담당하고 함부로 외부에서 값을 바꾸지 못하도록하고싶은 경우 field가 소문자로 시작하도록 함으로써 외부 패키지에서 직접 접근하지 못하도록할 수 있다.\n필요에 따라 Java에서 그러하듯 getter와 setter를 정의해줄 수도 있다.\npackage main ... func main(){ var ( calculator *calc.Calculator = calc.NewCalculator() ) // main package에서는 calc package에 정의된 unexported name인 id에 접근할 수 없다. //calculator.id = \u0026#34;Jinsu Park\u0026#34; } 앞서 말했듯이 이렇게 calc 패키지가 아닌 외부 패키지(예를 들어 main 패키지)에서는 unexported name인 id에 접근할 수 없다.\n주로 캡슐화와 Go에 대해선 은닉을 어떻게 하는가가 요점이라고 생각해 이 부분에 대해 다뤄보았다.\n 자세한 내용은? - 연관된 변수와 함수를 묶어주는 내용은 Go의 struct, receiver와 method 등에 대해 검색해보면 더 깊이 알아볼 수 있다.\n 👩‍👧‍👦 Inheriatance(상속) Go는 Composition을 이용한 Embedding이라는 방식을 통해 Inheritance와 같은 기능을 이용할 수 있게 해준다는 식으로 많이들 설명을 하는 것 같았으나 주관적인 해석을 해보자면 Go의 Embedding은 Composition이면서 자동으로 embed된 field의 method와 본인의 method인 것처럼 사용할 수 있게 해주기에 Inheritance처럼 이용할 수 있다고 볼 수 있겠다.\nGo의 Embedding은 struct의 field에 별도의 name이 아닌 type만을 적어줌으로써 이용할 수 있다. B라는 type이 는 메소드 Say()를 가지고 있는 경우 struct A가 type B를 Embed한다면 A는 B의 Say 메소드를 두 가지 방법으로 이용 가능하다.\n A.Say() - 이경우 암묵적으로 2.로 변환되어 수행되는 셈 A.B.Say() - 이렇게 명시적으로 Selector(여기선 B)를 적어줄 수도 있다. 일반적인 Composition과 동일하다.  Go에서 Embedding을 사용하는 방법 자체 또한 많은 내용을 인터넷에서 찾아볼 수 있으니 상속과 Embedding에 초점을 맞춰 설명해보도록하겠다.\n일반적인 객체 지향적인 방식에서는 type MultiplyOperationUnit struct 라는 type이 존재하고\ntype SquareOperationUnit struct 가 MultiplyOperationUnit type을 상속받는다면 SquareOperationUnit은 IsValidCheck 메소드를 비롯한 MultiplyOperationUnit의 메소드와 멤버 변수를 이용할 수 있을 것이다.\ntype MultiplyOperationUnit struct{} type SquareOperationUnit struct{ MultiplyOperationUnit // type만을 전달함으로써 Embed } // 곱셈 연산에 대한 Input validation func (unit *MultiplyOperationUnit) IsValidInput(args ...float64) bool{ fmt.Println(args) if len(args) != 2{ return false } return true } func (unit *MultiplyOperationUnit) Operate(args ...float64) (float64, error){ if !unit.IsValidInput(args...){ return 0, errors.New(\u0026#34;MultiplyOperationUnit의 args가 유효하지 않습니다.\u0026#34;) } ... } func (unit *SquareOperationUnit) Operate(args ...float64) (float64, error){ // *SquareOperationUnit에 대한 IsValidInput 메소드는 정의한 적 없지만  // Embedding을 통해 일반적인 OOP에서 부모의 메소드를 이용하듯이 이용 가능.  if !unit.IsValidInput(args...){ return 0, errors.New(\u0026#34;SquareOperationUnit의 args가 유효하지 않습니다.\u0026#34;) } ... } 이 경우 Go에서는 SquareOperationUnit이 MultiplyOperationUnit을 Embed하도록 한다. 평범한 Compostion 방식으로 이용할 수 있겠지만 Go의 특이한 Embed 방식을 이용함으로써 *SquareOperationUnit 에 대한 IsValidInput 메소드를 정의한 적 없지만 일반 OOP에서 부모 클래스에 정의된 메소드를 이용하듯unit *SquareOperationUnit과 같이 이용 가능하다.\nfunc (unit *SquareOperationUnit) IsValidInput(args ...float64) (float64, error){ ... } OOP에서의 Method Override와 같은 작업을 Go에서 하고싶다면 위와 같이 추가적으로 자신의 타입에 대한 method를 정의하면된다. 메소드를 추가적으로 정의해준 뒤 Selector를 지정하지 않으면 당연히 우리가 바란대로 Embed된 type의 method가 아닌 자기 자신의 method를 호출하게 된다.\n 자세한 내용은? - 아무래도 Go의 Embedding과 Composition에 대한 이해가 없다면 무슨 말인지 이해하기 힘들 수 있다. 따라서 해당 내용들에 대해 알아볼 것을 추천!\n Abstraction(추상화) 추상화는 그 객체의 세부 내용이 아닌 공통된 기능을 바탕으로 추려내는 것을 의미한다.\n추상화에 있어서는 Java와 Go가 interface를 이용한다는 점에서 크게 다르진 않다.\ninterface에 추상적으로 해당 interface를 구현하는 type들이 구현하기를 바라는 method를 정의만한다.\ntype OperationUnit interface{ IsValidInput(args ...float64) bool Operate(args ...float64) (float64, error) } 이 경우 OperationUnit는 \u0026ldquo;IsValidInput과 Operate 기능을 수행할 수 있는 무언가\u0026rdquo; 이라고 추상화된 것이다.\n 자세한 내용은? - 추상화의 쓰임은 다형성의 쓰임과도 밀접한 연관이 있다. Go에서 interface를 사용하는 패턴과 사용법에 대해 알아보면 좋을 것 같다.\n 🌒🌓🌕 Polymorphism(다형성) 다형성이란 한 가지 타입이 경우에 따라 같은 기능에 대해 다양한 동작을 수행할 수 있는 것을 말한다. 추상화는 interface에 대한 정의에 해당하고 다형성은 interface 활용에 해당하는 듯하다.\n일반적인 OOP 언어에서는 interface가 아닌 상속 관계에서도 부모⇒자식으로 타입 변환을 통해 다형성 활용이 가능하다. 하지만 Go는 이를 지원하지 않는다. 이유는 런타임에 동적으로 method dispatch(해당 type의 객체 혹은 value가 어떤 함수를 메소드로 할 지 결정하는 것)을 수행함으로 인한 오버헤드를 줄이기 위해서 컴파일 타임에 정적으로 method dispatch할 수 있게 하기 위해서라고한다.\ntype OperationUnit interface{ IsValidInput(args ...float64) bool Operate(args ...float64) (float64, error) } type MultiplyOperationUnit struct{} type SquareOperationUnit struct{ MultiplyOperationUnit } ... 각종 메소드 정의 생략 func main(){ var ( calculator *calc.Calculator = calc.NewCalculator() operationUnit calc.OperationUnit operationResult float64 operationErr error ) // 연산에 대한 multiplexing. 즉 option에 따른 연산을 수행한다는 의미  switch option{ case 1: operationUnit = calculator.Multiplier case 2: operationUnit = calculator.SquareMultiplier } operationResult, operationErr = operationUnit.Operate(inputs...) // result  if operationErr != nil{ fmt.Println(\u0026#34;[Error]\u0026#34;, operationErr) } else{ fmt.Println(\u0026#34;Result:\u0026#34;, operationResult) } } operationUnit이라는 Interface에 다른 type의 struct인 calculator.Multiplier와 calculator.SquareMultiplier가 담길 수 있다.\n이를 통해 operationUnit.Operate() 는 경우에 따라 \u0026ldquo;연산\u0026ldquo;이라는 기능으로 Multipliy 작업을 수행할 수도 있고, SqaureMultipliy 작업을 수행할 수도 있는데, 이를 다형성이라고 한다.\nGo에서의 객체 지향의 한계점과 장점 public class Example { public static void main(String[] args){ // Parent class와 Parent를 extends한 Child class에 대한 구현은 생략한다.  Parent parent = new Parent(); Parent child = new Child(); parent.ShowMetaData(); // parent.GetName() 이용  /* Output ======================================= Name: Parent ======================================= */ child.ShowMetaData(); // child.GetName()이용.  // 이 때에는 .ShowMetaData()가 Parent class가 아닌 Child class가 Override한 GetName() 이용  /* Output ======================================= Name: Child of Parent ======================================= */ } } 객체 지향 프로그래밍에선 부모 클래스에 정의된 메소드가 내부에서 자식이 Override한 메소드를 이용할 수도 있는데, Go는 그런 기능은 이용할 수 없다는 점이 가장 큰 한계점인 것 같다.\n예를 들어 Parent class의 .ShowMetaData()라는 method가 .GetName()이라는 메소드를 호출하는 경우, java에서는 Child가 GetName을 Override하면 child.ShowMetaData() 호출 시에 Child가 Override한 child.GetName()을 이용하지만, Go는 그럴 수 없다. 필요한 경우 함수를 인자나 field로 전달함으로써 사용할 수 있겠지만, 사용성이 제한적이다. 이 내용에 대해 여기서 설명하면 글이 길어질 것 같아 자세한 묘사는 생략하겠다.\n반면 Go에서의 객체 지향은 장점은 이 글(https://www.toptal.com/go/golang-oop-tutorial)의 후반부에 잘 나와있는데, 굳이 Java를 이용하지 않아도 이렇게 OOP가 충분히 가능하다는 것이 핵심이다. Java의 VM/JIT으로 인한 리소스 부족, 자유도가 떨어지는 프레임워크, 많은 annotation, \u0026hellip; 등등의 단점 없이도 충분히 Go를 통해 가볍게 OOP 할 수 있다는 것이 장점이다.\n마무리 아무래도 객체 지향적 개발을 하는 데에 있어서는 Java가 좀 더 직관적으로 그대로 설계, 구현해서 이용이 가능한 것 같다. 처음 Java를 공부했을 때부터 Java는 객체지향적으로 개발하는 패턴에 대해 수없이 많은 예제가 존재했고, 그 패턴이 명확했던 반면 Go는 명확한 패턴이나 깔끔하게 정의가 없다(이런 식으로 OOP 원칙을 이용해볼 수 있지 않을까~ 정도). 아마 애초에 Go는 객체 지향 언어로 설계하지 않았기 때문이 아닐까싶다.\n이를 계기로 Go에서는 OOP 원칙들이 어떻게 다양하게 적용할 수 있는지 좀 더 자세히 알아볼 수 있었다.\n참고  [번역] Go와 OOP - https://mingrammer.com/translation-go-and-oop/ Golang OOP tutorial - https://www.toptal.com/go/golang-oop-tutorial go object-oriented - https://golangkorea.github.io/post/go-start/object-oriented/  ","date":"2021-01-09T12:46:54+09:00","image":"https://umi0410.github.io/blog/golang/go-vs-java-oop/declaration_hu2809de309117ad963d612eaa5a399aab_35836_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/golang/go-vs-java-oop/","title":"Go vs Java - Go에서의 객체 지향"},{"content":"시작하며 개발 공부를 시작하고 여태까지 몇 년간 데드락을 실제로 접할 일은 없었다. 사실 동시성을 주의해야하는 작업을 해본 적도 없었고, 트랜잭션에 대한 개념도 없었기 때문일 수도 있다. 전공 과목에서 데드락에 대한 내용을 듣고서도 \u0026lsquo;아 이런 게 있구나\u0026rsquo; 정도로만 생각하고 넘겼었다. 하지만 Go 언어를 통해 개발을 하던 도중 Channel이나 Mutex로 인해 종종 데드락을 경험할 수 있었고, 이 경우 프로그램이 완전히 멈춰버리는 크리티컬한 문제가 발생하기도 했고, 디버깅하기 힘든 경우도 있었다. 그런 경험을 하면서 \u0026lsquo;데드락 이 녀석\u0026hellip; 만만치 않구나\u0026lsquo;라는 생각을 하곤 했다.\n이번 글에서는 Golang 즉 Go 언어를 통해 어떤 경우에 Deadlock이 발생할 수 있는지 실제 프로그램을 통해 알아보려한다. Mutex에 대한 Lock과 같이 일반적으로 발생할 수 있는 데드락부터 channel이나 goroutine에 대한 Cooperative scheduling과 같은 Go 언어에 특화된 내용까지를 정리해보았다.\nDeadlock이란  Deadlock이란 교착상태를 의미하며 두 개 이상의 작업이 서로 상대방의 작업이 끝나기만을 기다리고 있기 때문에 결과적으로 아무것도 완료되지 못하는 상태를 가리킨다. - 위키백과 -\n 데드락의 개념 자체는 그리 어렵지 않고, 예시를 통해서 쉽게 이해가 가능하다. 위키 백과에 나온 예시를 인용하자면 하나의 사다리에 위에서 내려오려는 사람, 아래에서 올라가는 사람이 동시에 올라가있으면 아무도 내려오거나 올라가지 못하는 경우를 예로 들 수 있다.\n교착 상태의 조건 더 자세히 정의나 의미에 대해 설명할 것은 없을 듯하고, 학문적으로는 주로 아래의 4가지 조건을 통해 발생한다고 설명하는 것 같다.\n 상호 배제 (Mutual exclusion) - 하나의 자원을 동시에 사용하지 못하도록 하는 것 점유 상태로 대기 (Hold and wait) - 하나의 자원을 소유한 상태로 다른 자원을 기다리고 있는 상태 선점 불가 (No preemption) - 컴퓨터 분야에서의 선점은 한국말의 선점과 다소 다른 의미. process 혹은 goroutine의 자원을 빼앗는 것을 선점이라고 함. 선점 불가란 그럴 수 없는 상태. 순환성 대기 (Circular wait) - 각 프로세스가 순환적으로 다음 프로세스가 요구하는 자원을 가지고있다. 쉽게 2개의 프로세스를 예로 들면 A는 B가 소유 중인 자원을, B는 A가 소유 중인 자원을 얻으려고 대기 중인 상황을 말함.  Deadlock의 개념 자체가 어렵다거나 위의 4가지 조건을 암기하는 것이 중요한 것은 아니라고 생각한다. 중요한 것은 실제로 어떤 경우에 데드락이 발생할 수 있을 지 파악하고 주의하는 것과 그 경우 어떻게 해결할 수 있을지 인지하는 것이라고 생각한다. 그럼 Go 언어로 간단한 프로그램을 짜보며 알아보자.\nGo에서 발생할 수 있는 Deadlock Channel 주로 채널을 접한 지 얼마 안 되어 그 동작 방식을 잘 이해하지 못한 채 사용할 경우 채널로 인한 데드락이 발생한다.\nUnbuffered channel  unbuffered-channel-1.png \n특히나 unbuffered channel에 대한 미숙한 사용은 자주 데드락을 야기한다. unbuffered channel에서 발생하는 일반적인 데드락의 의미인 2개 이상의 작업 서로의 작업이 완료되기를 대기하는 교착 상태와는 약간 다르다고 볼 수도 있다. 왜냐하면 sender와 receiver 중 누군가가 먼저 작업을 끝내야지 그 다음으로 누군가가 작업을 수행할 수 있는 것이 아니라 서로 동시에 협력해야만 unbuffered channel에 대한 대기를 끝낼 수 있는데 이 경우는 동시에 협력해줄 그 누군가(receiver)가 없는 경우이기 때문이다.\n unbuffered-channel-2.png \n그렇다고해서 위와 같이 자기 혼자 send와 receive를 하려해봤자 Unbuffered channel은 sender와 receiver가 모두 ready여야 작업을 진행할 수 있기 때문에 불가능하다. (Unbuffered channel의 동작에 대해 좀 더 궁금하신 분들은 제가 번역에 참여한 A Tour of Go를 참고해주시면 감사하겠습니다!)\n unbuffered-channel-3.png \n따라서 다른 goroutine에서 A에 대한 receiver 역할을 해주면 된다.\nBuffered channel  Buffered channel을 이용하면 어떨지 좀 더 자세히 들어가보자. 독자분께서 Go의 channel에 대해 별 관심이 없다면 패스~!\n  buffered-channel-1.png \n맨 처음에 Unbuffered channel에서는 위와 같이 한 goroutine에서 sender와 receiver 역할 모두를 수행하려해도 이미 sender에서 block이 걸려버려 deadlock을 야기한다고 했으나 buffered channel에서는 그렇지 않다! buffered channel은 buffer size까지는 입력 작업이 블락되지 않기 때문이다.\n buffered-channel-2.png \n하지만 buffer size를 넘어서는 순간부터는 receiver가 channel 내의 아이템을 꺼내어 줄 때까지 block되어버리므로 주의해야한다.\nMutex.Lock의 중첩 Mutex의 구현에 따라 다르겠지만 Go에서는 Lock이 걸린 자물쇠에 다시 자기가 Lock을 걸려해도 Unlock이 될 때까지는 Lock을 걸 수 없다. 즉 어떤 Mutex에 Lock을 건 것이 자신(Goroutine)이라 해도 해당 Mutex에 또 다시 Lock을 걸려하면 그 작업은 Mutex가 Unlock 될 때까지 블락되고 결과적으로는 Deadlock 상태가 되어버린다.\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) var ( scores = []int{10,30,20} Mutex = \u0026amp;sync.Mutex{} ) // Lock을 건 뒤 안전하게 topScore를 구함. func GetTopScore() (playerIndex, topScore int){ Mutex.Lock() topScore = -1 for idx, score := range scores{ if topScore \u0026lt; score{ playerIndex = idx topScore = score } } Mutex.Unlock() return } // GetTopScore는 이미 Lock을 이용해 thread safe하게 작업하는데 // 그걸 인지하지 못하고 실수로 그 밖인 Award에서 Lock을 걸어버림. func Award(){ Mutex.Lock() playerIdx, topScore := GetTopScore() Mutex.Unlock() fmt.Printf(\u0026#34;%d가 %d의 점수로 우승했습니다!\u0026#34;, playerIdx, topScore) } func main(){ Award() } // deadlock 발생 위의 예시에는 thread-safe하게 top score를 구하는 GetTopScore 함수가 선언되어있다. 하지만 Award 함수에서 이미 GetTopScore는 scores에 대해 thread-safe함에도 불구하고 Lock을 걸고 GetTopScore를 수행하려하기 때문에 데드락이 발생한다. 이 경우에는 inner인 GetTopScore 혹은 outer인 Award 둘 중 한 곳에서만 Lock, Unlock 작업을 수행하도록 해주어야한다.\ninner에서 Lock/Unlock을 담당하는 게 옳을 지 outer에서 Lock/Unlock을 담당하는 게 옳을 지는 잘 모르겠다. inner에서 Lock/Unlock을 담당하면 outer에서는 데드락으로 인해 절대 Lock/Unlock을 못한다는 단점이 있지만, 해당 작업은 언제나 Atomic하게 thread-safe하다는 것이 보장된다는 장점이 있다. 반면 outer에서 Lock/Unlock을 담당하도록하면 비교적 자유자재로 Lock/Unlock을 명령할 수 있는 반면 thread-safe해야할 내부 작업이 unsafe해질 수도 있다. 실수로 Lock/Unlock 작업을 잊어버릴 수 있기 때문이다. 하지만 무엇보다도 inner에서 Lock/Unlock을 담당할 지 outer에서 담당할 지를 정확히 정할 수 없는 이유는 outer도 결국엔 또 다른 outer의 inner가 될 수 있기 때문이라고 생각한다.\nCooperative scheduling 방식에서의 무한 Loop Goroutine scheduler가 Goroutine을 스케쥴링 하는 방식은 Go 1.14 이전까지는 Cooperative 방식이었으나, Go 1.14 부터는 Preemptive한 방식으로 바뀌었다고 한다. OS가 Go process의 thread를 스케쥴 하는 방식은 OS 마다 다르겠지만 대체로 preemptive할 것이고 여기서 얘기하려는 스케쥴러는 Goroutine을 스케쥴링하는 Goroutine scheduler임을 주의하자.\nGoroutine은 syscall와 mutex, channel, 함수 콜 등으로 인해 switch 될 수 있는데, cooperative 방식을 이용하는 경우에는 이러한 goroutine switch 조건에 해당하지 않는 경우 임의로 goroutine switch 함수를 호출하지 않는 한은 같은 스레드 내의 다른 goroutine은 절대로 실행될 수 없다. preemption(선점) 즉 다른 goroutine을 block 상태로 만들어버리고 자신이 CPU를 선점하는 것이 불가능했기 때문이다. 하지만 Go 1.14부터는 약 10ms를 기준으로 preemption을 수행하고 있다. Asynchronous preemption이라고 부르는 것 같은데, 정확히 왜 asynchronous인지, 기존의 preemptive schedule과는 무엇이 다른지는 찾아봤으나 제대로 설명되어있는 곳을 찾지 못했다.\n앞으로 이어지는 내용은 데드락에 대한 예시라기 보다는 \u0026lsquo;cooperative 스케쥴링과 preemptive 스케쥴링의 차이로 인해 데드락이 발생할 수도 있고 발생하지 않을 수도 있구나\u0026rsquo;에 대해 알아보는 예시이다.\npackage main import ( \u0026#34;fmt\u0026#34; ) func Foo(){ fmt.Println(\u0026#34;No Deadlock. 나도 실행될 수 있어!\u0026#34;) } func main(){ go Foo() dummy := 0 // 그냥 for 문 안에서 dummy 작업을 수행하기 위한 변수  for { dummy += 1 } } for 문 안에서는 함수콜도 syscall도 channel/mutex도 아닌 goroutine swtich와 관련 없는 dummy += 1 작업만을 수행하고 있다. 이 경우 go Foo()를 통해 goroutine을 생성하긴 하지만 그 goroutine은 바로 실행되는 것이 아니라 대기 상태이며 만약 스레드가 하나라면 cooperative 스케쥴링으로는 Foo라는 goroutine이 실행될 수가 없을 것이다.\n# Go 버전 \u0026gt;= 1.14 일 때에도 Deadlock 발생 $ GOMAXPROCS=1 GODEBUG=asyncpreemptoff=1 go run main.go  GOMAXPROCS=1 - Go 프로그램이 사용할 최대 OS Thread 개수를 1개로 제한함으로써 여러 스레드에서 Goroutine이 실행되는 것을 방지. (데드락을 야기해보려는 조건) GODEBUG=asyncpreemptoff=1 - Go 1.14부터 적용된 Asynchronous preemptive scheduling을 사용하지 않는 옵션  이 경우 Go 버전이 1.14 이상이라면 Foo는 실행되지 않는다.\n# Go 버전 \u0026gt;= 1.14인 경우 async preemption으로 인해 Deadlock 발생 X $ GOMAXPROCS=1 go run main.go No Deadlock. 나도 실행될 수 있어! 하지만 asyncpreemptoff 옵션을 생략하면 기본적으로 선점형 스케쥴링이 지원되므로 이 경우엔 Deadlock이 발생하지 않고 Foo() 함수가 실행되는 것을 \u0026ldquo;No Deadlock. 나도 실행될 수 있어!\u0026rdquo; 라는 Stdout을 통해 볼 수 있다.\npprof를 이용한 goroutine schedule 시각화 하지만 정말로 async preemption이 동작했기 때문에 데드락에 빠지지 않은 것인지 다른 이유 때문인지는 그닥 직관적으로 와닿지 않는다. 그래서 몇몇 외국 블로그의 글에서 봤던 pprof라는 도구를 사용해봤고, 처음엔 사용법이 다소 어려웠지만 조금 익숙해지니 너무나도 편리했다. pprof는 net/http 패키지 하위에 존재하고, Goroutine scheduling, syscall log, CPU 사용 등을 시각화해서 보여주는 간편한 디버깅 도구이다. 요청을 날리면 요청 이후 N초 간의 goroutine scheduling에 대한 정보를 기록해 보여주는 기능을 이용해보았다.\n pprof-debugging-1.png \nEnd Stack Trace를 통해 어떤 작업으로 인해 goroutine이 잠시 중단되고 CPU를 다른 goroutine에게 양보하게 되는지 알 수 있다. 놀랍게도 dummy에 대한 무한 루프 진행 도중 처음으로 async preemption이 발생한 뒤 이어서 Foo 함수를 실행하는 goroutine이 CPU를 점유하게 된다는 것을 시각적으로 볼 수 있다.\nWall Duration은 해당 고루틴 블럭을 수행한 시간으로 보여지고 약 10ms를 기준으로 preemptive하게 switch가 일어날 수 있다는 여러 블로그의 글들과 유사하게 15,021,904ns, 즉 약 15ms만에 asyncPreempt라는 이벤트로 인해 goroutine switch가 일어났다. cooperative 스케쥴링만을 이용하는 경우에는 데드락으로 인해 한 번도 Foo를 실행하는 goroutine이 수행되지 못한다는 것도 시각화해서 제공해보고싶었지만, 당연하게도 그 데드락으로 인해 일정 기간동안 runtime을 관찰한 뒤 그 정보를 저장하는 goroutine 조차 실행할 수 없어 그 정보를 얻을 수 없었다!\n# Go의 버전을 1.13으로 제한해본다.FROMgolang:1.13WORKDIR/appCOPY . .ENTRYPOINT [\u0026#34;go\u0026#34;]CMD [\u0026#34;run\u0026#34;, \u0026#34;main.go\u0026#34;]$ docker build . -t tmp \u0026amp;\u0026amp; \\  docker run -it --rm -e GOMAXPROCS=1 tmp 그렇다면 정말 Go의 1.14 이전 버전은 Preemptive한 방식이 아니라 Cooperative한 방식을 이용하고, 이 경우 데드락이 발생할까? 이전 글들과 마찬가지로 Docker를 이용해 간편하게 Go의 버전을 변경해서 실행해보자. golang:1.13 이미지를 이용해보았다.\n1.13 버전 이하로는 GODEBUG=asyncpreemptoff=1 옵션을 설정하지 않아도 cooperative한 스케쥴링만을 지원하므로 프로그램이 데드락 상태에 빠져 Foo()가 실행되지 못함을 알 수 있다.\n참고 사항) GOMAXPROCS=1 옵션을 주는 이유는?  The GOMAXPROCS variable limits the number of operating system threads that can execute user-level Go code simultaneously. - go rutime package 문서 -\n GOMAXPROCS는 Go 프로그램이 사용할 수 있는 최대의 OS 스레드 개수를 의미한다. 만약 OS 스레드가 2개 이상으로 생성된다면 위의 경우 main goroutine과 함수 Foo를 실행하는 Goroutine이 서로 다른 스레드에 배치될 것이고, 그 경우 Go의 스케쥴러가 Cooperative한 방식을 이용한다해도 OS 스케쥴러가 Preemptive하게 각각의 Go 스레드(OS Level)를 스케쥴하기 때문에 Foo goroutine도 실행될 수 있는 기회가 주어진다. 이 경우에는 데드락이 발생하지 않는다. 따라서 우리는 데드락을 발생시켜보고자 GOMAXPROCS를 1로 제한한다.\n마치며 Go를 이용해 실제 프로그램을 짜보며 어떤 경우에 Deadlock이 발생할 수 있는지 알아보았다. 이 글에선 서로 같은 Mutex를 이용하는 경우를 예시로 들었지만 서로 다른 두 Mutex를 통해 서로의 작업을 기다리는 경우의 Deadlock도 거의 유사하며 일반적으로 말하는 Deadlock에 가장 가까운 경우이긴할 것이다. 그래도 Go로 개발을 하면서 아직 서로 다른 Mutex를 이용했던 경우가 딱히 없었기에 같은 Mutex에 중첩으로 Lock을 걸었던 경우를 예시로 들어보았다.\nChannel이나 Cooperative scheduling의 경우는 어느 정도 Go에 한정적인 내용이고 특히나 스케쥴링은 런타임이나 고루틴 스케쥴 방식까지 내려가는 세부적인 내용이긴하지만 Go에 특히 관심 있으신 분들께는 나름 재미있는 내용이 되지 않았을까싶다.\n그리고 이전 글에선 testing의 benchmark를 이용해 좀 더 정확하고 편리한 벤치마킹을 도입해봤다는 점과 이번엔 추가적으로 pprof를 이용해 goroutine 스케쥴링을 시각화해봤다는 점에서 Golang으로 적용해보는 컴퓨터 사이언스라는 이 시리즈를 처음 시작했을 때에 비해 컴퓨터 사이언스 뿐만 아니라 디버깅 기술이나 스케쥴링 방식 등등 다양한 주제에 대해서도 공부해보고 적용해볼 수 있었던 것 같아 뿌듯하다. 다소 Go만의 지엽적인 내용으로 여겨질 수도 있겠지만 추후에 내가 어떤 언어를 공부하든 어떤 기술을 공부하든 이러한 경험들을 얼마든지 녹여낼 수 있을 것이라 생각한다!!\n참고   Go: How are Deadlocks Triggered? https://medium.com/a-journey-with-go/go-how-are-deadlocks-triggered-2305504ac019\n  Go: What Does a Goroutine Switch Actually Involve? https://medium.com/a-journey-with-go/go-what-does-a-goroutine-switch-actually-involve-394c202dddb7\n  Goroutine and Preemption https://medium.com/a-journey-with-go/go-goroutine-and-preemption-d6bc2aa2f4b7\n  Go: Asynchronous Preemption https://medium.com/a-journey-with-go/go-asynchronous-preemption-b5194227371c\n  데드락 정의\n https://namu.wiki/w/%EB%8D%B0%EB%93%9C%EB%9D%BD?from=Deadlock https://ko.wikipedia.org/wiki/%EA%B5%90%EC%B0%A9_%EC%83%81%ED%83%9C    Go runtime documentation https://golang.org/pkg/runtime/\n  ","date":"2021-01-31T18:25:54+09:00","image":"https://umi0410.github.io/blog/golang/go-deadlock/pprof-debugging-1_hu27a13bf8dcdafb0aaff224765e3a83ab_59369_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/golang/go-deadlock/","title":"Go 언어로 적용해보는 Computer Science - Deadlock"},{"content":"시작하며 저번 학기에 컴퓨터 구조를 수강하면서 간과하고 있던 로우 레벨의 지식에도 흥미가 생겼었다. 그 중 CPU와 Memory, Disk의 역할에 대해 알아볼 수 있었고 캐시는 CPU와 Memory 사이에 위치해 메모리 대신 빠르게 CPU에게 데이터를 제공하는 녀석이라고 배웠다.\n이전에는 주로 캐시라고 하면 주로 CDN과 같은 네트워크에서 쓰이는 캐시들밖에 몰랐다. 그렇다보니 L1 캐시, L2 캐시 같은 얘기를 들으면 OSI 7계층과 연관 지어 \u0026lsquo;음..? L2 캐시는 스위치에서 쓰는 캐시인가..?\u0026rsquo; 라는 상상을 하곤했다.\n이번에는 Go를 통해 배열에 여러 차례 접근하는 프로그램을 만들어보고 벤치마킹을 통해 캐시라는 녀석이 어떤 효과를 가져다주는지 직접 확인해보려한다.\n캐시란 캐시는 아주 다양한 문맥에서 사용된다. 공통적으로 \u0026ldquo;사용자가 요청할 것 같은 데이터를 작고 빠른 저장소에 저장해놓음으로써 좀 더 빨리 해당 데이터를 제공한다\u0026quot;는 목적을 갖는다. CDN, DB, REST API, Memory, CPU 등등 다양한 곳에서 쓰일 수 있을 것 같다. 그 중 이번에는 CPU와 메모리 사이의 캐시에 대해 알아보겠다.\nCPU와 메모리 사이의 캐시는 메모리의 데이터를 얻기 위해 메모리에 직접 접근하지 않고 캐시라는 빠른 저장소를 이용해 해당 데이터를 얻게끔해준다. 예를 들어 변수 a=10 이라는 데이터가 메모리에 존재한다해도 a의 값을 얻기 위해 메모리에 직접 접근하기 보다는 가까우면서 빠르게 이용 가능한 캐시에서 데이터를 가져올 수도 있다는 것이다. 사실 캐시의 개념적인 측면에서 보면 메모리 또한 디스크 대신 빠르게 값을 전달해주기 위한 경우일 수 있으니 캐시 기능을 한다고 볼 수 있다. 그리고 CPU와 메모리 사이에 정말 캐시라는 이름을 갖는 녀석들은 프로세서 속에 있는 L1 캐시, 프로세서 옆에 있는 L2 캐시, 프로세서들이 공유하는 L3 캐시가 있긴 하지만 이는 시대가 지나면서 얼마든지 변할 수 있는 내용들이기 때문에 어떤 캐시가 어디에 있고 누구랑 누가 공유하는지와 같은 세부 내용은 크게 중요하진 않을 것 같다.\n물리적인 크기나 거리는 속도와 반비례할 수 밖에 없다. 거리가 멀면 정보가 전달되는 속도가 느려지고 크기가 크면 여러 Mux나 Gate를 이용한다는 것이기 때문에 느려진다. 그렇기때문에 캐시는 작고 가까워야한다. 데이터를 요청하는 녀석은 CPU이기 때문에 캐시는 CPU 속 혹은 그 근처에 위치한다. 또한 작아야하기때문에 모든 정보를 담을 수 없고, 사용자가 요청할법한 데이터만을 담아야한다. 이 때 어떻게 사용자가 요청할 법한 데이터를 정할까? 이는 공간 지역성과 시간 지역성이라는 중요한 두 가지 성질을 기반으로 한다.\n이외에도 태그나 충돌 같은 개념들이 있긴하지만 실제로 벤치마킹해보기도 쉽지 않고 다소 지엽적인 내용이라 간단히만 정리해보면 태그 없이 주소값을 모듈러(나머지)연산해서 cache line index를 결정하고 그것만을 이용해 데이터를 저장하면 한 line 내에 저장할 워드(Word)에 대한 충돌이 발생할 수 있다. cache line이 20개인 캐시는 0번지와 20번지가 같은 line이므로 충돌이 발생해 계속해서 같은 line에 서로의 데이터가 번갈아 저장될 수 있다는 것이다. 하지만 태그를 이용하면 cache line 수는 줄어들더라도 한 line내에 여러 태그의 정보를 저장할 수 있게되어 cache line이 10개인 cache의 한 line에 0번지와 20번지의 데이터가 다른 태그로 저장되어 불필요한 충돌을 방지할 수 있다는 장점이있다. 간단히 설명하기는 힘든 내용이라 좀 더 자세히 알고싶다면 Direct mapped cache나 Fully associative cache 등으로 검색해보기를 권장한다.\nSpatial locality Spatial locality(공간 지역성)이란 지금 요청 받은 데이터와 가까운 곳에 위치한 데이터는 높은 확률로 다시 요청 받게 된다는 성질이다. 예를 들어 100번지의 a=10과 108번지의 b=20이 존재할 때 변수 a를 요청하면 이후 a와 가까운 주소에 저장된 b 또한 높은 확률로 요청된다는 것이다.\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { var ( a int = 10 b int = 20 c int = 30 ) fmt.Printf(\u0026#34;a: %p\\nb: %p\\nc: %p\\n\u0026#34;, \u0026amp;a, \u0026amp;b, \u0026amp;c) } /* Output: a: 0xc000100010 b: 0xc000100018 c: 0xc000100020 */ a, b, c의 크기는 8바이트로 주소값 또한 8바이트가 차이난다. (16진법이기에 20과 18의 차이는 8이다.) 즉 대체로 비슷한 시기에 할당된 변수는 근접한 메모리 주소를 갖게 된다. 우리는 비슷한 시기에 할당한 변수 혹은 연속된 배열 요소에 자주 빠른 시일 내에 접근을 하지 맨 위에서 선언한 변수와 저 멀리 맨 밑에서 선언한 변수를 마구잡이로 왔다 갔다 하면서 작업을 하지 않는 편이기 때문에 공간 지역성을 근거로한 캐시가 효력을 갖게 된다. 만약 공간적으로 먼 맨 위의 변수와 맨 아래의 변수를 자주 번갈아가며 접근한다면 그것은 시간지역성을 띄는 경우이다.\nTemporal locality Temporal locality(시간 지역성)이란 최근에 요청했던 데이터는 높은 확률로 다시 요청 받게 된다는 성질이다. 예를 들어 100번지의 a=10과 9999번지의 b=20은 서로 주소적인 거리는 멀지만 둘 다 최근에 호출됐다면 캐시에 적재하겠다는 것이다. 캐시는 주로 직사각형 형태로 생겼으며 가로(행)는 연속된 주소의 데이터를 저장하는 공간 지역성, 세로(열)는 최근에 호출된 데이터를 저장하는 시간 지역성을 담당한다.\n두 지역성 비교 12칸의 캐시가 있다고 가정하자. 가로로 4칸 세로로 3칸 존재한다면 공간/시간 지역성의 균형이 잡힌 캐시라고 볼 수 있다.(경우에 따라 다르겠지만)\n balanced-cache.png \n시간 지역성은 최근 불린 데이터는 다시 불릴 확률이 높다는 것이고 이는 연속된 공간이 아닌 다양한 공간(주소)의 데이터를 캐시에 저장한다는 말이기도 하다. 0번지 부근, 16번지 부근, 24번지 부근의 다양한 공간의 데이터를 저장할 수 있으면서 그 녀석들간의 주변 데이터도 제공하는 공간지역성도 만족한다.\n두 가지 지역성에 의해 다양한 캐시들이 데이터를 적재하고 제공한다. 요점은 캐시는 빠르게 동작해야하고 그러기 위해선 크기가 작고 가까워야하며 크기가 작기 때문에 모든 데이터를 담을 수 없으니 알짜 데이터만을 담아야하는데 그 알짜는 지역성을 기반으로 선별된다는 것이다. 크기가 한정적이기 때문에 한 지역성을 키우면 한 지역성은 작아질 수밖에 없다.\n공간 지역성에 치우친 캐시 구조  spatial-locality-biased-cache.png \n한정된 크기의 캐시 속에서 공간지역성을 극대화시켜버리면 당연히 인접한 공간의 자료만 이용할 수 있고, 최근에 불린 데이터들은 안중에도 없고 인접한 공간의 데이터만을 저장하게 된다. 예를 들어 다음과 같은 시간 지역성이 필요한 경우에 제대로 기능을 할 수 없다.\n 0~11번지 사이의 데이터가 한 번 접근 ⇒ 캐시에 0~11번지 적재 이후 12번지의 데이터에 접근 ⇒ 캐시에 데이터가 없기때문에 0~11번지의 데이터 대신 12~23번지의 데이터를 캐시에 적재 다시 최근에 접근했던 데이터인 0번지의 데이터에 접근 시도 ⇒ 0번 데이터는 최근에 접근했던 데이터임에도 시간 지역성이 활용되지 못함 ⇒ 캐시에서 데이터를 찾을 수 없음.  시간 지역성에 치우친 캐시 구조  temporal-locality-biased-cache.png \n위의 경우 최근에 호출된 다양한 주소의 데이터들을 캐시에 저장해준다. 하지만 캐시의 크기는 한정되어있기 때문에 세로가 길어지면 가로는 짧아진다. 즉 최근 접근을 시도한 다양한 주소의 데이터를 저장할 수 있지만 그 데이터의 인근 데이터에 대한 저장은 많이 할 수 없다는 것이다.\n예를 들어 위의 그림과 같은 경우 최근 0, 3, 12, 18, 6, 20번지의 데이터에 접근했고 이후에도 해당 번지에 대한 데이터를 캐시를 통해 이용할 수 있다. 바로 내가 최근에 접근했던 데이터이기때문이다. 하지만 만약 18번지의 데이터에 접근한 경우 높은 확률로 공간지역성에 의거 19, 20, 21, \u0026hellip; 번지의 데이터에 접근하겠지만, 이 예시는 시간지역성에 치우쳐져 19번지의 데이터만을 캐시에서 제공받을 수 있다.\n프로그램을 통한 벤치마킹 저번에 Mutex, Semaphore를 직접 벤치마킹해보면서 Go의 내장 벤치마크 기능을 이용하면 좀 더 편리하게 결과를 보여줄 수 있을 것 같았기에 이번에 Go의 내장 벤치마크 기능을 이용해봤다.\n1000행 1000열의 2차원 int형 배열의 어떠한 요소에 접근해서 +1 하는 작업을 1000회 수행하는 것을 하나의 싸이클로 하는 벤치마크를 작성했다. 2차원 배열은 가로로는 연속적인 주소값을 갖기에 공간 지역성을 활용할 수 있지만 세로로는 N * (int형 자료형의 크기)씩 차이 나는 주소값을 갖기 때문에 공간 지역성을 활용하기 힘들고, 최근 접근했던 주소라면 시간 지역성은 활용할 수 있다.\n공간 지역성 (가로로 연속적인 데이터)\n 공간 지역성을 사용하는 경우 가로로 연속된 요소에 접근. 즉 연속된 주소를 갖는 1000개의 요소에 접근 공간 지역성을 사용하지 않는 경우에는 세로로 요소에 접근. 즉 1000 * int 자료형의 크기만큼 차이나는 연속되지 않은 주소의 1000개의 요소에 접근  시간 지역성 (연속적 주소와 상관없이 최근에 불린 데이터)\n 시간 지역성을 사용하는 경우 주소값이 근접하진 않지만 4개 혹은 16개의 데이터에만 계속해서 접근 시간 지역성을 사용하지 않은 경우에는 계속해서 처음 접근하는 데이터에만 접근  package main import ( \u0026#34;testing\u0026#34; ) var ( Size int = 1000 ) func generateArray() [][]int{ arr := make([][]int, Size) for i := 0; i \u0026lt; Size; i++{ arr[i] = make([]int, Size) for j := 0; j \u0026lt; Size; j++{ arr[i][j] = 0 } } return arr } func BenchmarkSpatialLocality(b *testing.B){ b.Run(\u0026#34;공간지역성 사용\u0026#34;, func(b *testing.B) { arr := generateArray() b.ResetTimer() for n := 0; n \u0026lt; b.N; n++{ for i := 0; i \u0026lt; Size; i++{ arr[0][i] += 1 } } }) b.Run(\u0026#34;공간지역성 X\u0026#34;, func(b *testing.B) { arr := generateArray() b.ResetTimer() for n := 0; n \u0026lt; b.N; n++{ for i := 0; i \u0026lt; Size; i++{ arr[i][0] += 1 } } }) } func BenchmarkTemporalLocality(b *testing.B){ b.Run(\u0026#34;시간 지역성 적극 사용. 최근 접근한 데이터 4개.\u0026#34;, func(b *testing.B) { arr := generateArray() b.ResetTimer() for n := 0; n \u0026lt; b.N; n++{ for i := 0; i \u0026lt; Size; i++{ arr[i%4][0] += 1 } } }) b.Run(\u0026#34;시간 지역성 조금 사용. 최근 접근한 데이터 16개.\u0026#34;, func(b *testing.B) { arr := generateArray() b.ResetTimer() for n := 0; n \u0026lt; b.N; n++{ for i := 0; i \u0026lt; Size; i++{ arr[i%16][0] += 1 } } }) // 사실 완벽하게 새로운 데이터는 아님. 벤치마킹하는 동안 계속해서 반복되기 때문에  b.Run(\u0026#34;시간 지역성 X. 새로운 데이터에만 접근.\u0026#34;, func(b *testing.B) { arr := generateArray() b.ResetTimer() for n := 0; n \u0026lt; b.N; n++{ for i := 0; i \u0026lt; Size; i++{ arr[i][0] += 1 } } }) } goos: linux goarch: amd64 pkg: playground/unix-socket/cache BenchmarkSpatialLocality/공간지역성_사용 50000 894 ns/op BenchmarkSpatialLocality/공간지역성_X 50000 6561 ns/op BenchmarkTemporalLocality/시간_지역성_적극_사용._최근_접근한_데이터_4개. 50000 1918 ns/op BenchmarkTemporalLocality/시간_지역성_조금_사용._최근_접근한_데이터_16개. 50000 4153 ns/op BenchmarkTemporalLocality/시간_지역성_X._새로운_데이터에만_접근. 50000 6564 ns/op 결과를 확인해보니 간단한 배열 내의 요소들에 대한 연산인데도 꽤나 차이가 컸다.\n마치며 저번 학기에 컴퓨터 구조를 수강하면서 CPU-Memory 캐시의 효과를 직접 배열에 대한 프로그램을 통해 보여주는 예시를 보고 신기했던 기억이 있어서 이렇게 벤치마킹 프로그램을 작성해봤다. 다른 CS 주요 지식들에 비해 어려울 것은 없는 편이고 우리가 쉽게 접해오던 내용이라 더 이해하기 쉽지 않았을까 싶다.\n참고  캐시란 - https://ko.wikipedia.org/wiki/캐시 캐시가 동작하는 아주 구체적인 원리 - https://parksb.github.io/article/29.html cache mapping - https://m.blog.naver.com/jkssleeky/220478400046  ","date":"2021-01-27T15:25:54+09:00","permalink":"https://umi0410.github.io/blog/golang/go-cpu-cache/","title":"Go 언어로 적용해보는 Computer Science - Cache"},{"content":"시작하며 Go 언어를 처음 배울 때 channel이나 goroutine에 대해 배우면서 Concurrency 즉 동시성으로 인해 많이 힘들었던 기억이 난다. \u0026lsquo;동시성과 병렬성이 도대체 뭐가 다르다는 것이냐..!\u0026rsquo; Rob Pike 아저씨가 동시성에 관해 했던 세미나 영상들을 보며 같이 스터디 하던 멤버들과 멘붕에 빠지기도 했고, 이런 저런 의견 교류도 했다가 며칠 지나고 보면 다시 병렬성과 헷갈리고, 제대로 된 한글 자료는 찾기 힘들었다. \u0026lsquo;동시성은 사람이 느끼기에 동시처럼 느껴지는 것이고 병렬성은 실제로 동시적인 시점에 대한 것이다.\u0026rsquo; 라는 내용이 많았지만 와닿지는 않았다.\n그리고 이번에 다뤄볼 동시성은 주로 \u0026ldquo;제어\u0026ldquo;라는 단어와 함께 언급되는 \u0026ldquo;동시성 제어\u0026ldquo;에서의 동시성이나 Concurrency와는 조금 다른 의미를 갖는다고 생각한다. 동시성 제어에서의 동시성은 그냥 동시에 무언가에 접근하는 것을 어떻게 제어하거나 막겠냐는 의미일 뿐이지만 동시성-병렬성에서의 동시성은 어떻게 작업들이 동시에 수행되느냐에 초점을 맞추는 느낌이기 때문이다.\n 동시성 제어의 예시 - 여러 client가 하나의 db row에 access할 때 어떻게 제어할 것인가 이번에 다룰 동시성의 예시 - 어떤 작업을 동시성을 이용해 수행하는 것이 좋을까 병렬성을 이용해 수행하는 것이 좋을까  Golang을 처음 접한 지 벌 써 반년이 넘은 것 같다. 처음 한 4개월 가량은 거의 머릿 속에 아른거리는 물음표 같은 내용이었는데, 꾸준히 공부하다보니 조금 알 것 같다. 이번 글에서는 그러한 내용을 추상적인 말보단 예시와 코드로 명확히 정리해보고자 한다.\nConcurrency is not parallelism 위의 문장은 동시성과 병렬성에 대해 알아보려하면 매번 보게 되는 문장인다. 그렇다. 동시성은 병렬성과 다르다. 이에 대해 설명할 때 영어권에서는 아래의 두 문장이 자주 등장하는 듯 하다.\n  Concurrency is about dealing with lots of things at once.\n  Parallelism is about doing lots of things at once.\n  한 번에 dealing with하는 것과 한 번에 doing하는 것이라고 한다. 하지만 잘 와닿지 않는다\u0026hellip; 아마 저 글을 보고 이해할 수 있는 한국인은 극히 드물지 않을까 싶다.\n예시로 보는 동시성 vs 병렬성 따라서 좀 더 쉽게 예시로 설명해보려 한다. 예를 들어 사회적 거리 두기로 인해 우리 Gopher(고퍼)가 친구들과 떠나려는 예약했던 여행이 취소되었고, 항공권, 호텔, 렌트카 예약을 모두 취소해야하는 상황이라고 해보자.\n 동시성(Concurrent)의 예시 - 만약 모든 취소 작업이 고객센터 1대1 채팅으로 이루어지는 경우. Gopher1 혼자 모든 취소 작업을 동시적으로 수행 가능  Gopher1 혼자 항공권 취소 신청, 호텔, 렌트카의 고객센터 1대1 채팅에 예약 취소 신청을 함 상담사분이 답장을 주는 대로 작업을 진행 Gopher1 혼자서 세 개의 취소 작업을 동시에 진행하는 셈이다.   병렬성(Parallelism)의 예시 - 만약 모든 취소 작업이 고객센터 전화 상담으로 이루어 지는 경우. Gopher1 혼자 모든 취소 작업을 동시에 진행할 수는 없고, Gopher 1, 2, 3이 병렬적으로 진행할 순 있음.  뒤에서 설명하긴 하겠지만 전자의 경우에만 동시성이 일어날 수 있는 이유는 Gopher가 1대1 채팅으로 작업을 진행할 때는 한가한 반면(CPU 점유율이 낮고, 상담사의 메시지를 기다리느라 많은 블락킹이 존재) 전화로 작업을 진행할 때에는 여러 명과 전화할 수 없기 때문이다.(CPU 점유율이 높음.)\n항공권 취소 전화 도중 잠시 지연되는 시간에 전화를 끊고 호텔 고객 센터에 전화를 거는 방식으로 Gopher1 혼자 Concurrent하게 작업을 진행할 수 있겠지만 이 경우 계속해서 전화를 끊고 다시 걸고 상황 설명을 하는 동안에 지연시간이 발생해서 효율이 좋지 않다. 이 지연 시간을 항공권 취소 전화 =\u0026gt; 호텔 예약 취소 전화로의 context switch penalty로 볼 수 있다.\n프로그램으로 알아보는 Concurrency vs Parallelism 동시성과 병렬성을 비교하는 한글 자료는 찾기 힘든 편이고, 영어로 된 자료는 꽤나 많이 찾아볼 수 있지만, 그래서 \u0026ldquo;언제\u0026rdquo;, \u0026ldquo;무엇을\u0026rdquo;, \u0026ldquo;왜\u0026rdquo; 써야하는 지에 대한 내용은 잘 찾아보기 힘들다. 게다가 간혹 Concurrent programming의 장점이라면서 parallelism을 설명하는 경우도 있어 혼란스러웠다.\n concurrency-vs-parallelism.jpg \n동시성이란 한 작업이 완료된 뒤 다음 작업이 수행되는 것이 아니라 계속해서 적절히 switch 되며 동시에 진행되어나가는 것을 의미한다. 그 결과 일정 기간동안 수행한 작업들에 대해 말할 때 그 작업들은 동시에 수행됐다고 할 수 있게 된다. 한 순간에는 엄밀히 따지면 한 작업이 수행된다.\n병렬성이란 여러 코어에서 작업이 병렬적으로 진행되는 것을 의미한다. 그 결과 일정한 기간이 아닌 순간을 놓고 봤을 때에도 여러 작업이 병렬적으로 동시에 수행되게 된다.\n코어 수 보다 많은 스레드가 이용될 경우는 동시성과 병렬성이 함께 이용된다고 볼 수 있다. 즉 여러 스레드에 걸쳐 여러 작업이 완료 되지 않았지만 적절히 switch되며 진행된다는 의미이다.\n프로그래밍적으로 봤을 때 같이 동시성은 블락이 많이 걸리는 작업에 유리하고, 병렬성은 CPU bound한 작업에 유리하다고 생각된다. 그럼 좀 더 자세히 동시성과 병렬성이 \u0026ldquo;언제\u0026rdquo; \u0026ldquo;왜\u0026rdquo; 유리한지 알아보자.\nConcurrency - Block이 많은 작업에 유리 예상: 블락이 많은 작업의 경우 Core 숫자에 상관 없이 대체로 많은 thread(혹은 goroutine)을 생성할 수록 작업이 빨라질 것이다.\nblock이 많은 작업이 뭐가 있을까? 네트워크 IO 블락이 많이 걸릴 작업으로서 https://example.com 에 요청/응답을 얻는 작업을 64번 수행해보겠다. core의 숫자를 1개로 제한하여 concurrency의 효과를 관찰해보기 위해 4개의 코어로 8개의 스레드를 하이퍼스레딩하는 내 랩탑이 아닌 AWS EC2 t2.micro에서 벤치마크를 수행했다.\n프로그램 코드 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/stretchr/testify/assert\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;testing\u0026#34; ) func Benchmark동시성(b *testing.B) { for _, goroutineNum := range []int{1,4,8,16,32,64}{ b.Run(fmt.Sprintf(\u0026#34;%d개의 goroutine\u0026#34;, goroutineNum), func(b *testing.B) { for i := 0; i \u0026lt; b.N; i++{ do동시성(b, goroutineNum) } }) } } func do동시성(b *testing.B, goroutineNum int){ wg := \u0026amp;sync.WaitGroup{} totalRep := 64 for n := 0; n \u0026lt; goroutineNum; n++{ wg.Add(1) go func(num int) { for j := 0; j \u0026lt; totalRep / goroutineNum; j++{ resp, err := http.Get(\u0026#34;https://example.com\u0026#34;) assert.NoError(b, err) assert.Equal(b, 200, resp.StatusCode) } wg.Done() }(n) } wg.Wait() } Core 1개의 AWS EC2 t2.micro에서의 벤치마킹 ubuntu@ip-172-31-12-2:~/concurrency$ CGO_ENABLED=0 go test ./concurrency_test.go -bench=. -benchtime=1x goos: linux goarch: amd64 Benchmark동시성/1개의_goroutine 1 9426630119 ns/op Benchmark동시성/4개의_goroutine 1 2354737545 ns/op Benchmark동시성/8개의_goroutine 1 1192283559 ns/op Benchmark동시성/16개의_goroutine 1 604589767 ns/op Benchmark동시성/32개의_goroutine 1 313062480 ns/op Benchmark동시성/64개의_goroutine 1 182304298 ns/op PASS ok command-line-arguments 28.809s 64번 요청/응답 작업을 수행할 때 맨 위의 경우는 1개의 goroutine이 64번, 맨 밑의 경우에는 64개의 goroutine이 각각 1번씩 수행하는 식으로 진행했다. 1개의 코어를 갖는 t2.micro에서도 64개의 goroutine으로 concurrent하게 작업을 진행하는 것이 더 속도가 빨랐다. 네트워크 IO로 CPU가 놀고있는 시간이 많았기 때문에 하나의 코어로도 concurrent하게 수 많은 작업을 효율적으로 진행할 수 있었다.\nParallelism - CPU bound한 작업에 유리 예상: CPU bound한 작업의 경우 코어의 개수까지는 goroutine이 늘어날 수록 성능이 좋을 것이다.\n(아직 코어와 프로세서, vCPU의 차이는 명확히는 모르겠다)\nCPU Bound한 작업의 경우는 정말 동시성보단 병렬성을 이용할 때 더 좋은 성능을 보일까? 일종의 제곱 연산으로 CPU를 혹사시키는 작업을 정의했다. 병렬성의 경우 어떤 경우까지가 병렬성이고, 어떤 경우까지가 동시성 + 병렬성인지 헷갈릴 수 있기에 정리해본다.\n 코어 1개에 여러 혹은 goroutine =\u0026gt; 당연히 병렬성이 일어날 코어들이 없으니 동시성 코어 N개에 N개 이하의 여러 goroutine =\u0026gt; 병렬성 코어 N개에 N개 이상의 여러 goroutine =\u0026gt; 동시성 + 병렬성. 하지만 CPU Bound 한 작업의 경우 동시성은 별 이점을 가져다 주지 않는다.  프로그램 코드 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;testing\u0026#34; ) func Benchmark병렬성(b *testing.B) { for _, goroutineNum := range []int{1,4,8,16,32,64}{ b.Run(fmt.Sprintf(\u0026#34;%d개의 goroutine\u0026#34;, goroutineNum), func(b *testing.B) { for i := 0; i \u0026lt; b.N; i++{ do병렬성(b, goroutineNum) } }) } } func do병렬성(b *testing.B, goroutineNum int){ wg := \u0026amp;sync.WaitGroup{} totalRep := 1024 * 1024 dummyNumber := 12345 for n := 0; n \u0026lt; goroutineNum; n++{ wg.Add(1) go func(num int) { for j := 0; j \u0026lt; totalRep / goroutineNum; j++{ dummyNumber = (dummyNumber * dummyNumber) % 100000 } wg.Done() }(n) } wg.Wait() } Core 1개의 AWS t2.micro에서의 벤치마킹 ubuntu@ip-172-31-12-2:~/concurrency$ CGO_ENABLED=0 go test ./parallelism_test.go -bench=. -benchtime=10x goos: linux goarch: amd64 Benchmark병렬성/1개의_goroutine 10 15228154 ns/op Benchmark병렬성/4개의_goroutine 10 14048765 ns/op Benchmark병렬성/8개의_goroutine 10 14038413 ns/op Benchmark병렬성/16개의_goroutine 10 13916091 ns/op Benchmark병렬성/32개의_goroutine 10 13877358 ns/op Benchmark병렬성/64개의_goroutine 10 14082735 ns/op PASS ok command-line-arguments 0.946s t2.micro의 경우 코어가 1개이기 때문에 병렬성이 존재할 것도 없이 동시성만이 존재한다고 했듯이 goroutine이 많아져도 성능이 좋아지지 않는다. 되려 불필요한 goroutine을 관리하기 위한 코스트와 미미하겠지만 존재할 goroutine에 대한 context switch로 인해 작업이 더 더뎌지기도 하는 결과를 볼 수 있다.\nCore 4개로 8개의 vCPU를 가진 나의 랩탑에서의 벤치마킹 $ go test ./parallelism_test.go -bench=. -benchtime=10x goos: linux goarch: amd64 Benchmark병렬성/1개의_goroutine-8 10 10263785 ns/op Benchmark병렬성/4개의_goroutine-8 10 7342331 ns/op Benchmark병렬성/8개의_goroutine-8 10 4872064 ns/op Benchmark병렬성/16개의_goroutine-8 10 5000452 ns/op Benchmark병렬성/32개의_goroutine-8 10 4830541 ns/op Benchmark병렬성/64개의_goroutine-8 10 4968710 ns/op PASS ok command-line-arguments 0.419s 내 랩탑은 실제 core는 4개지만 8개의 스레드까지 병렬적으로 처리가 가능하므로 8개라고 볼 수 있다.\n따라서 8개의 goroutine까지는 병렬성에 의해 성능이 점점 좋아지고, 8개 초과의 goroutine부터는 병렬성과 동시성이 모두 적용되겠지만 현재의 작업은 CPU bound하기 때문에 동시성은 그닥 이점을 가져다주지 못하고 EC2의 경우와 마찬가지로 과잉되는 goroutine으로 인해 오히려 성능을 저하시킬 수 있다. 코드에 첨부는 하지 않았지만 극단적으로 과잉되는 goroutine은 아래와 같은 성능 저하를 야기하기도 했다.\nBenchmark병렬성/8개의_goroutine-8 10 580169905 ns/op Benchmark병렬성/16개의_goroutine-8 10 602391376 ns/op Benchmark병렬성/1024*1024개의_goroutine-8 10 879079810 ns/op (한편으론 1024*1024개의 goroutine을 관리하는 데에도 성능 저하가 저정도 뿐이라니 대단하단 생각도 든다\u0026hellip;!)\n마치며 동시성과 병렬성은 Go를 공부하면서 컴퓨터 구조를 공부하면서 너무도 많이 파고들었던 내용이라 술술 적힐 줄 알았는데 시간이 좀 지나서인지 가물가물한 내용도 좀 있었고 내용도 쉽지 않았으며 좋은 예시를 어떻게 들어야할 지 많이 고민이 됐던 것 같다. 이런 저런 생각들로 인해 설명이나 진행이 그렇게 깔끔히 된 것 같진 않지만 그래도 결론이 깔끔하게 나와서 다행이다. 사실 이런 코어나 고루틴, 스레드의 개수에 따라 병렬성이나 동시성을 구분 짓기보다 \u0026lsquo;동시성은 주로 구조와 관련되고 병렬성은 실행과 관련된다.\u0026lsquo;와 같은 철학적인 내용도 간간히 나오긴 하지만 몇 달 째 그닥 와닿지 않고, 간간히 등장할 뿐 오피셜한 내용은 아닌 듯하여 생략했다.\n그리고 이번엔 벤치마크 테스트에 한글 이름을 도입해봤는데 읽기도 편하고 나름 귀여운 것 같다. ㅎㅎ 종종 개발하면서 테스트 코드 짤 때에도 한글 테스트 케이스를 사용해보려한다.\n참고 Concurrency is not parallelism https://blog.golang.org/waza-talk\nConcurrency is not parallelism slide - Rob Pike https://talks.golang.org/2012/waza.slide#1\nBack to the Essence - Concurrency vs Parallelism https://homoefficio.github.io/2019/02/02/Back-to-the-Essence-Concurrency-vs-Parallelism/\n[ 제 4회 파이썬 격월 세미나 ] 동시성과 병렬성 - 이찬형 https://youtu.be/Iv3e9Dxt9WY\n","date":"2021-02-04T12:46:54+09:00","image":"https://umi0410.github.io/blog/golang/go-concurrency-vs-parallelism/concurrency-vs-parallelism_hu86023f6654c19803b1eea4e88bc97fcd_191924_120x120_fill_q75_box_smart1.jpg","permalink":"https://umi0410.github.io/blog/golang/go-concurrency-vs-parallelism/","title":"Go 언어로 적용해보는 Computer Science - Concurrency vs Parallelism"},{"content":"시작하며 개발 공부를 처음 시작한 지 언제 안 됐을 무렵, 의경 복무를 하며 자바로 TCP Socket을 이용해 옆 컴퓨터와 채팅을 하는 프로그램을 만들어 보는 것을 시작으로 docker나 mysql과 같은 다양한 오픈소스들을 이용해보면서나 네트워크를 공부하면서 다양하게 들어왔던 소켓이지만 정확히 어떤 역할을 하는지 어떤 종류가 있는지 어떻게 동작하는지 알지 못했다.\n오늘은 그렇게 알쏭달쏭한 존재였던 소켓을 크게 Unix Domain Socket와 Network Socket 두 가지로 나눠 정리해보고 Unix Domain Socket과 TCP를 사용하는 Network Socket을 벤치마크해보고 어떤 차이가 있는지 확인해보도록 하겠다. 주로 내가 소켓을 접했던 자료나 이슈 상황에서는 UDP보다는 TCP가 많이 등장했었기 때문에 UDP에 대한 내용은 거의 제외할 것이므로 대부분이 Network socket과 Unix domain socket 각각의 stream(network socket에선 tcp를 이용하는 경우에 해당) type socket에 관련한 내용일 것이다.\nSocket 이란 소켓은 어떠한 통신에서의 Endpoint(끝점) 역할을 한다. 끝점이 없으면 어디와 어디가 통신하는지 어디에 데이터를 써야하는지 알 수 없다.\n소켓을 마치 전구 소켓과 같이 소켓이라고 부르는 이유는 소켓에 올바르게 데이터를 적기만 하면 통신을 위한 세부적인 동작은 소켓이 알아서 수행하기 때문이다. 따라서 우리는 통신을 하기 위한 Socket을 올바르게 생성하고, 그 곳에 데이터를 올바르게 적거나 그곳의 데이터를 올바르게 읽기만 하면 된다. 실제 데이터 전송은 소켓이 알아서 수행해준다.\n소켓은 같은 호스트 내에서 IPC를 위해 사용되는 Unix domain socket과 네트워크 통신을 하기 위해 사용되는 Network socket으로 분류할 수 있다. 간혹 unix domain socket과 TCP를 이용하는 Network socket, UDP를 이용하는 Network socket 이렇게 세 가지로 분류하는 경우를 본 것 같은데 이는 잘못된 분류라고 생각한다. Network socket 뿐만 아니라 Unix domain socket 또한 stream(Network socket의 경우 TCP를 이용하는 경우에 해당) 타입과 datagram 타입(Network socket의 경우 UDP를 이용하는 경우에 해당)으로 사용될 수 있기 때문이다.\n서버 소켓과 클라이언트 소켓  server-client-socket.jpeg \n소켓을 역할의 측면에서 분류한다면 서버 소켓과 클라이언트 소켓으로 나눌 수 있다. 동일한 구조의 소켓이지만 생성되는 시기나 역할이 다를 뿐이다.\n 서버 소켓 - 클라이언트 소켓의 연결 요청을 받아들이기만 할 뿐. 실제 서버 측에서의 데이터의 송수신은 서버 소켓이 클라이언트 소켓의 연결 요청을 수락하면서 새로 만들어지는 소켓을 통해 수행. 클라이언트 소켓 - 클라이언트가 서버와 통신하고자 할 때 생성하는 소켓. 클라이언트는 실제 데이터 송수신도 이 소켓을 통해 수행.  서버 소켓과 클라이언트 소켓이 단순한게 1:N으로만 통신하면 이렇게 서버 소켓이 클라이언트 소켓의 요청을 수락한 뒤 새로 소켓을 만들 필요 없지 않을까싶지만 그렇게 되면 서버가 각 클라이언트와 통신할 때 하나의 소켓을 이용하므로 올바르게 원하는 클라이언트와 통신할 수 없을테니 좋은 방식이 아닐 것이다.\n그렇다면 과연 정말 서버에서는 연결을 accept 한 뒤 소켓을 새로 생성할까? 확인해보자.\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;net\u0026#34; \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; ) var ( network = \u0026#34;tcp\u0026#34; address = \u0026#34;0.0.0.0:8080\u0026#34; ) func main() { os.Remove(address) socket, err := net.Listen(network, address) if err != nil { log.Fatal(err) } for { connectedSocket, err := socket.Accept() if err != nil{ log.Fatal(err) } go func() { for i := 0; i \u0026lt;= 10; i++{ connectedSocket.Write([]byte(\u0026#34;pong\\n\u0026#34;)) time.Sleep(time.Second) } connectedSocket.Close() }() } } 간단하게 TCP 통신프로그램을 하나 만들어봤다. 단순히 1초 간격으로 pong을 5번 출력한 뒤 연결을 끊는 프로그램이다.\n요청 전\n$ netstat --tcp Active Internet connections (w/o servers) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 ip-172-31-34-13.ap-:ssh 124.50.93.166:60082 ESTABLISHED tcp 0 0 ip-172-31-34-13.ap-:ssh 124.50.93.166:60084 ESTABLISHED tcp 0 0 ip-172-31-34-13.ap-:ssh 124.50.93.166:60078 ESTABLISHED 요청 수락 후 통신 중 - 소켓이 하나 생성되어 ESTABLISHED 상태\n$ netstat --tcp Active Internet connections (w/o servers) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 ip-172-31-34-13.ap-:ssh 124.50.93.166:60082 ESTABLISHED tcp 0 0 ip-172-31-34-13.ap-:ssh 124.50.93.166:60084 ESTABLISHED tcp 0 0 ip-172-31-34-13.ap-:ssh 124.50.93.166:60078 ESTABLISHED tcp6 0 0 ip-172-31-34-1:http-alt 124.50.93.166:42434 ESTABLISHED 통신 종료 - 생성되었던 소켓이 TIME_WAIT 상태. 잠시 후 사라진다.\n$ netstat --tcp Active Internet connections (w/o servers) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 ip-172-31-34-13.ap-:ssh 124.50.93.166:60082 ESTABLISHED tcp 0 0 ip-172-31-34-13.ap-:ssh 124.50.93.166:60084 ESTABLISHED tcp 0 0 ip-172-31-34-13.ap-:ssh 124.50.93.166:60078 ESTABLISHED tcp6 0 0 ip-172-31-34-1:http-alt 124.50.93.166:42434 TIME_WAIT  깔끔한 네트워크 정보를 보기 위해 순수한 AWS EC2 t2.micro 인스턴스에서 작업해보았다. 맨 위 3줄에 나온 소켓 정보는 ssh 접속으로 인한 소켓 정보이다.\n또한 서버 소켓은 출력되지 않았는데 이는 우리는 평소에 listen 중인 포트나 서버 소켓을 보기 위해 netstat에 -l 옵션을 전달한 반면 이번엔 \u0026ndash;tcp 옵션을 통해 listen 중인 서버 소켓이 아닌 클라이언트의 요청을 수락한 뒤 생성되는 소켓을 보려하고있기 때문이다.\n 4번 째 줄의 소켓 정보가 바로 우리가 생각하는 서버 소켓이 client의 연결 요청을 accept 후 생성하는 socket이다. 클라이언트의 요청을 accept 후에 새로운 소켓이 생성되며 통신 종료 후 잠시 기다린 뒤 사라진다.\nNetwork Socket Network socket은 네트워크 통신이 필요한 작업을 수행할 때 이용하는 소켓의 한 종류로 다시 동작 방식에 따라 TCP 프로토콜을 이용하는 stream socket과 UDP 프로토콜을 이용하는 datagram socket으로 구분할 수 있다. 사용자는 socket에 데이터를 적기만 하면 네트워크와 관련된 작업은 socket이 알아서 수행해준다. 읽을 때에도 마찬가지이다.\nTCP socket이라고 부르는 사람도 있고 stream socket, TCP/IP socket이라 부르는 사람도 있는 것 같다. 정확한 명칭은 모르겠지만 사용하거나 이해하는 데에는 무리가 없을 것 같다. TCP를 이용하는 stream type의 Network socket과 stream type의 Unix domain socket은 사용 방법이 매우 유사하다. 둘 다 stream type이고, 소켓에 데이터를 적은 뒤의 작업은 소켓이 알아서 수행해주기 때문이다. 각각의 소켓을 이용해 서버를 띄우는 작업은 인자의 값만 조금 달라질 뿐이다. 이는 글의 하부의 코드에서 확인해볼 수 있다.\nSocket과 Port Network socket에 대해서는 socket과 port의 구분이나 역할이 애매하게 느껴질 수 있다. 통신을 할 때 IP 주소를 이용해 목적지인 Host를 찾을 수는 있지만 그 Host의 어떤 프로세스과 통신하려는 것인지는 알 수 없다. 올바른 프로세스를 찾을 수 있도록 프로세스와 어떠한 숫자를 매핑시키는데 이 숫자를 Port 번호라고 한다. 예를 들어 123.123.123.123:8080으로 요청을 보내는 것은 123.123.123.123의 IP 주소를 갖는 Host의 8080번 포트에 맵핑된 프로세스에 요청을 보내는 것이다.\n이 때 Port와 프로세스를 그냥 연결할 수는 없고 Socket이라는 녀석이 필요하다. Socket은 실질적으로 어떤 프로세스를 어떤 포트에 맵핑시킬지에 대한 정보가 필요하고 네트워크 작업을 알아서 수행한다.\n조금 비유를 해보자면 회사내에 어떤 부서가 있고 외부에서 해당 부서와 작업하기 위해선 어떠한 고유한 부서 번호가 필요하고, 이때 외부와 해당 부서간에 오가는 통신을 담당하는 담당자가 있어야하는 경우에 비유해 볼 수 있다. \u0026ldquo;어떤 부서\u0026quot;는 프로세스이고 \u0026ldquo;고유한 부서 번호\u0026quot;는 포트 번호, 통신을 담당하는 담당자는 소켓에 해당한다.\nUnix Domain Socket Unix Domain Socket은 IPC(Inter-Process Communication, 프로세스 간 통신)의 여러 방법 중 가장 자유로우면서 사용하는 데에 있어 제한이 별로 없는 방법이다. 네트워크 소켓과 달리 같은 호스트 내의 프로세스 간 통신을 담당하기 때문에 아무런 네트워크 작업이 필요 없다. 하지만 TCP나 UDP를 이용하는 Network socket을 이용할 때와 인자 값만 조금 바꾸어 동일한 방식으로 사용이 가능하다.\n🌈 상상의 나래 - 우리가 알게 모르게 겪었던 Unix domain socket의 Permission오류에 대해 $ mysql stat /var/lib/mysql/mysql.sock stat: cannot stat \u0026#39;/var/lib/mysql/mysql.sock\u0026#39;: Permission denied $ docker docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock 주로 우리가 MySQL이나 Docker를 localhost에서 사용할 때 unix socket에 대한 permission 에러를 겪은 적이 있을 것이다. MySQL이나 Docker를 비롯한 많은 오픈소스들이 아마 성능상의 이점을 위해 localhost에서 서비스를 이용할 경우 Network socket이 아닌 Unix domain socket을 많이들 이용하는 것으로 알고있다. 그렇기 때문에 주로 MySQL 설치 이후 Remote에서는 접속이 되는데 localhost에서는 권한 문제로 접속이 안된다거나 Docker 설치 이후 사용자를 docker group에 넣어주는 경우가 많이 있다.\n아마도 네트워크 통신을 통해 서버 소켓에 접근하는 네트워크 소켓과 달리 unix domain socket은 client process가 직접 server의 socket file에 접근하기 때문에 이때 쓰기 권한 때문에 그런 권한 오류가 생기는 것이 아닐까라는 상상의 나래를 펼쳐본다.\n# ubuntu 사용자로 unix domain socket server 프로세스를 띄운 경우 # ubuntu 사용자로는 잘 접속이 된다. ubuntu@ec2 $ go test -v === RUN TestSocket === RUN TestSocket/tcp 2021/02/13 12:59:06 pong === RUN TestSocket/unix 2021/02/13 12:59:06 pong --- PASS: TestSocket (0.00s) --- PASS: TestSocket/tcp (0.00s) --- PASS: TestSocket/unix (0.00s) PASS ok uds\t0.004s # server가 생성한 unix domain socket에 write 권한이 없는 # guest1이라는 사용자로는 server와 unix domain socket으로는 통신할 수 없고, # 앞서 오픈소스를 이용하며 겪었던 에러와 마찬가지로 권한 이슈가 발생한다. # 하지만 TCP 소켓을 이용한 경우는 원활히 ping-pong test가 성공한 것을 볼 수 있다. guest1@ec2 $ go test -v === RUN TestSocket === RUN TestSocket/tcp 2021/02/13 12:59:01 pong === RUN TestSocket/unix main_test.go:21: Error Trace:\tmain_test.go:21 Error: Received unexpected error: dial unix jinsu.sock: connect: permission denied Test: TestSocket/unix --- FAIL: TestSocket (0.00s) --- PASS: TestSocket/tcp (0.00s) --- FAIL: TestSocket/unix (0.00s) # 하지만 재미있게도 guest1도 socket에 write할 수 있도록 권한을 수정해주니 # guest1도 unix domain socket으로 무리 없이 통신이 가능했다! guest1@ec2 $ sudo chmod 777 jinsu.sock guest1@ec2 $ go test -v === RUN TestSocket === RUN TestSocket/tcp 2021/02/13 12:59:06 pong === RUN TestSocket/unix 2021/02/13 12:59:06 pong --- PASS: TestSocket (0.00s) --- PASS: TestSocket/tcp (0.00s) --- PASS: TestSocket/unix (0.00s) PASS ok uds\t0.004s 따라서 ubuntu 사용자로 TCP socket과 Unix domain socket 두 가지 방법으로 서버 역할을 할 수 있는 프로세스를 띄운 뒤 ubuntu 사용자와 guest1 사용자로 통신 테스트를 진행해보았다.\n서버가 생성한 unix domain socket은 srwxrwxr-x 의 형식과 권한을 갖고 있기 때문에 guest1은 이 소켓에 대해 read와 execute 권한 뿐이고, write는 불가능하기에 unix domain socket을 이용해서는 통신할 수 없다. 따라서 우리가 평소에 오픈소스를 localhost에서 이용하면서 종종 맞이했던 소켓에 대한 permission error을 만나볼 수 있었다! 반면 socket에 대한 접근 권한이 필요 없는 TCP로는 통신이 가능했고, 놀랍게도 guest1에게 소켓 파일에 대한 write 권한을 부여해주자 Unix domain socket으로도 통신이 가능해진 것을 볼 수 있다.\n상상의 나래 정리: unix domain socket을 통해 접속을 시도할 때에는 unix domain socket file에 대한 접근을 하는 프로세스가 해당 socket file에 대한 적절한 permission을 갖고 있어야한다.\n 또한 구글링 도중 보았던 재미있는 예시는 database를 통해 authentication/authorization을 수행하는 일반적인 서비스와 달리 Unix domain socket을 이용하는 경우에는 linux user 시스템을 이용해서도 권한/인증 관리를 수행하는 경우도 존재할 수 있다는 것이었다.\n 벤치마킹 Unix domain socket vs Network socket Unix domain socket과 Network socket의 성능을 비교하는 벤치마크를 작성했다. 두 경우 모두 Stream type의 socket을 이용하도록 했고, Network socket의 경우는 특히나 이 경우 TCP 프로토콜을 이용하게 된다.\n// main.go // 한 프로세스 내에서 TCP Socket과 Unix Domain Socket 두 가지를 이용해 // 통신할 수 있는 프로그램 package main import ( \u0026#34;errors\u0026#34; \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net\u0026#34; \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; ) func main() { go RunTCPSocketServer() go RunUnixDomainSocketServer() for { log.Println(\u0026#34;Dummy waiting...\u0026#34;) time.Sleep(time.Minute) } } func RunTCPSocketServer(){ log.Println(\u0026#34;Run TCPSocketServer...\u0026#34;) run(\u0026#34;tcp\u0026#34;, \u0026#34;0.0.0.0:8080\u0026#34;) } func RunUnixDomainSocketServer(){ log.Println(\u0026#34;Run UnixDomainSocketServer...\u0026#34;) serverSocketName := \u0026#34;jinsu.sock\u0026#34; os.Remove(serverSocketName) run(\u0026#34;unix\u0026#34;, serverSocketName) } // TCP socket을 이용해 서버를 띄우든 Unix Domain Socket을 이용해 서버를 띄우든 // 간단히 인자 값만 변경해서 이용할 수 있다! func run(network, address string){ buf := make([]byte, 1024) socket, err := net.Listen(network, address) if err != nil { log.Fatal(err) } for { connectedSocket, err := socket.Accept() if err != nil{ log.Fatal(err) } go func() { for{ n, err := connectedSocket.Read(buf) if err != nil{ if !errors.Is(io.EOF, err){ log.Fatal(err) } else{ log.Print(\u0026#34;[Error]\u0026#34;, err) break } } log.Println(network, \u0026#34;Client sent:\u0026#34;, n) connectedSocket.Write([]byte(\u0026#34;pong\u0026#34;)) } connectedSocket.Close() }() } } // main_test.go // tcp socket과 unix domain socket을 이용해 성공적으로 서버와 통신이 // 이루어지는지 테스트하는 테스트 코드 // tcp와 unix domain socket의 성능을 비교하는 벤치마크 코드 package main import ( \u0026#34;errors\u0026#34; \u0026#34;github.com/stretchr/testify/assert\u0026#34; \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net\u0026#34; \u0026#34;testing\u0026#34; ) func TestSocket(t *testing.T){ args := [][]string{ []string{\u0026#34;tcp\u0026#34;, \u0026#34;0.0.0.0:8080\u0026#34;}, []string{\u0026#34;unix\u0026#34;, \u0026#34;jinsu.sock\u0026#34;}, } for _, arg := range args{ t.Run(arg[0], func(t *testing.T) { conn, err := net.Dial(arg[0], arg[1]) assert.NoError(t, err) buf := make([]byte, 1024) _, err = conn.Write([]byte(\u0026#34;ping\u0026#34;)) if err != nil{ log.Fatal(err) } n, err := conn.Read(buf) if err != nil{ if ! errors.Is(io.EOF, err){ log.Println(err) } } log.Println(string(buf[:n])) }) } } func BenchmarkSocket(b *testing.B) { args := [][]string{ []string{\u0026#34;tcp\u0026#34;, \u0026#34;0.0.0.0:8080\u0026#34;}, []string{\u0026#34;unix\u0026#34;, \u0026#34;jinsu.sock\u0026#34;}, } for _, arg := range args{ b.Run(arg[0], func(b *testing.B) { //log.Println(\u0026#34;Out\u0026#34;) // b.N과 상관 없이 args의 길이에 따라 출력됨.  conn, err := net.Dial(arg[0], arg[1]) assert.NoError(b, err) buf := make([]byte, 1024) for i := 0; i \u0026lt; b.N; i++{ //log.Println(\u0026#34;In\u0026#34;) // b.N만큼 수행됨  _, err := conn.Write([]byte(\u0026#34;ping\u0026#34;)) assert.NoError(b, err) _, err = conn.Read(buf) if err != nil{ if !errors.Is(io.EOF, err){ b.Fail() } } } }) } } $ go test -bench=. -benchtime=100000x goos: linux goarch: amd64 pkg: uds BenchmarkSocket/tcp 100000\t104537 ns/op BenchmarkSocket/unix 100000\t83456 ns/op PASS ok uds\t18.808s 10000번의 ping-pong을 수행하는 벤치마크를 수행했다. 그 결과 TCP socket을 이용할 경우는 한 번의 ping-pong에 약 104537ns, unix domain socket을 이용할 경우는 한 번의 ping-pong에 약 83456ns가 소요된 것을 보아 예상대로 네트워크 통신이 일절 필요하지 않은 unix domain socket이 좀 더 성능이 좋은 것으로 보여졌다. 사전에 자료 조사를 할 때에는 unix domain socket이 tcp socket을 이용할 때보다 약 2배가량 성능이 우월할 것이라고 들었는데, 그렇게 많은 차이가 나는 것 같지는 않다. 하지만 어느 정도 데이터의 크기나 버퍼의 크기에 따라 달라질 수도 있을 것 같고 더 깊은 원리들이 존재할 것 같아 더 자세히는 측정해보지 못했다.\n마치며 사실 이번에 다룬 소켓이라는 주제는 Go로 적용해보는 Computer Science라는 이 시리즈를 정리해나가던 초기에 2번 째 글로 시도했던 주제였는데 당시엔 Go benchmark도 처음 사용해봤었고 내용이 어려웠던 터라 벤치마크도 제대로 되지 않고 정리도 하기 힘들어서 중단했던 주제이다. 하지만 이번엔 같은 내용에 대해 두 번째 정리해서인지 전보다 내용도 잘 이해됐고, 벤치마킹 코드와 결과도 깔끔하게 나왔던 것 같아 뿌듯하다.\n그리고 그 동안 도커나 MySQL 같은 오픈소스들을 사용하면서 간혹 소켓에 대한 오류를 접할 때나 이런 저런 글에서 소켓 관련한 내용이 등장할 때 정확히 어떤 역할인지 이유가 뭔지 자세히 알지 못했는데 이번 기회덕에 앞으로는 좀 더 잘 이해해볼 수 있을 것 같다.\n참고   소켓 http://www.ktword.co.kr/abbr_view.php?nav=\u0026m_temp1=280\u0026id=742\n  네트워크 소켓 https://ko.wikipedia.org/wiki/%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC_%EC%86%8C%EC%BC%93\n  소켓 프로그래밍 https://recipes4dev.tistory.com/153\n  UDS (Unix Domain Socket) http://www.dreamy.pe.kr/zbxe/CodeClip/119393\n  [Linux/UDS/Unix Domain Socket] UDS https://yaaam.tistory.com/entry/LinuxUDSUnix-Domain-Socket-UDS\n  Unix file type https://en.wikipedia.org/wiki/Unix_file_types\n  C# - .NET Core Unix Domain Socket 사용 예제 https://www.sysnet.pe.kr/2/0/11963\n  Unix Domain Socket https://www.joinc.co.kr/w/Site/system_programing/IPC/Unix_Domain_Socket\n  Unix Domain Socket - UDP https://www.joinc.co.kr/w/Site/system_programing/IPC/Unix_Domain_Socket_UDP\n  What is the difference between Unix sockets and TCP/IP sockets? https://serverfault.com/questions/124517/what-is-the-difference-between-unix-sockets-and-tcp-ip-sockets\n  ","date":"2021-02-13T14:10:54+09:00","image":"https://umi0410.github.io/blog/golang/go-socket/server-client-socket_huc5be35f5857cc34672b838b13057c03e_282874_120x120_fill_q75_box_smart1.jpeg","permalink":"https://umi0410.github.io/blog/golang/go-socket/","title":"Go 언어로 적용해보는 Computer Science - Socket (Unix Domain Socket, Network/TCP/UDP Socket)"},{"content":" preview.png  사진 출처: https://tv.naver.com/v/16972079\n시작하며 원래 간간히 재미삼아 개발 컨퍼런스들을 찾아보곤 해왔는데 그냥 쓰윽 듣기만 하니까 머리에 남는 게 별로 없는 것 같아 가끔은 이렇게 내용을 정리해보는 글을 써볼까한다. 나는 학교 시험과는 그닥 맞지 않는 스타일 같아서 시험 기간이 되면 더 이런 실무적인 내용들이 괜히 더 끌리는 것 같다. 소프트웨어 공학 수업을 듣다가 지루해져서 네이버 d2를 돌아다니던 중 네이버 데뷰 2020에 Golang 관련 영상이 있길래 \u0026ldquo;오잉?! 네이버 채용에 가끔 Golang이 뜨긴 하던데 Golang 어떻게 쓰려나?!\u0026quot; 싶은 마음에 영상을 시청해봤다.\n내용을 간단히 요약하자면 기존에는 C++로 검색 엔진 라이브러리를 개발해왔고 해당 라이브러리를 이용하는 C++ 검색 서비스를 개발해왔다고 하는데, 이를 Golang으로 마이그레이션하면서 성능은 유지하면서 Golang 특유의 간결하고 강력한 기능들(unit test, benchmark, fmt, build 등등)과 함께 데브옵스 문화에 잘 녹아들 수 있었고 팀원들도 편리함을 느끼며 생산성까지 잡을 수 있었다는 이야기이다.\n 본 글은 https://tv.naver.com/v/16972079 영상을 바탕으로 해당 내용을 정리해본 글입니다.\n 검색 서비스의 특징 자세히 검색 엔진이 어떻게 구성되어 있는지, 내부적으로 데이터를 어떻게 저장하고 질의하는지, 검색 서비스는 검색 라이브러리를 어떻게 사용하는지 등등 이런 내용들을 잘 몰라서인지 자세히 검색 엔진이나 검색 서비스가 어떤 식으로 동작하는지, 어떤 형태로 개발되는지에 대해서는 감이 잘 잡히지 않는다. 혹시 자세히 코드를 볼 수 있었다면 좋았겠지만, 아무래도 그러기는 힘들겠지\u0026hellip;ㅜㅜ\n아무튼 검색 서비스는 단언 성능이 중요했으며 주로 질의를 분석하고, 검색 결과를 정렬하는 방식, 랭킹에 대한 모델링 등의 로직이 구현되어 있다고 한다.\n각 서비스들은 자신들의 고유한 검색 로직을 구현하고, 실제 검색을 수행하는 것은 검색 엔진 라이브러리라고 한다. 점점 검색 엔진의 규모와 신규 검색 서비스들이 증가하게 되면서 관리가 어려워졌다. 서비스들의 크기와 수가 증가하면서 높은 생산성을 유지하기 위해 개발 프로세스를 표준화하고 자동화했지만 추가적으로 개발 언어적인 관점에서의 고민이 들기 시작했다고 한다.\n개인적으로는 아무래도 서비스가 많아질 수록 데브옵스 문화가 많이 녹아들어야한다고 생각하는데 아무래도 많은 것들을 자동화하기 위해서는 언어 차원에서 강력하고 편리한 CLI 기능들일 잘 지원되어야한다고 생각하고, 지속적인 통합을 위해서도 편리한 테스트가 지원되어야한다고 생각한다. C++을 많이 해보진 않아 잘 모르겠지만, 아무래도 Go가 간단한 명령어들을 통해 IaC도 곁들이며 많은 작업들을 자동화하기 더 편리하고, unit test를 작성해가며 TDD식으로 개발하기에도 편리하지 않을까 싶다.\n개인적인 의견이었고, 어쨌든 이 팀에서는 결국 효율적으로 DevOps 문화에 녹일 수 있으면서 어느 정도 높은 성능이 보장되고 생산성을 유지하기도 쉬운 언어를 생각하게 되었고 Go가 적합하다고 판단했다고 한다.\nGo의 특징  golang-features.png \n(이해를 돕고자 발표 PPT에서 하나 캡쳐해왔습니다.)\n  성능이 높으면서 GC를 지원한다.\n discord-golang.png \n출처 - 디스코드 블로그(https://blog.discord.com/why-discord-is-switching-from-go-to-rust-a190bbca2b1f)\n 과거 디스코드에서는 Go의 GC로 인해 간혹 CPU가 Spike치는 경험을 했고 Rust로 옮겨갔다는 이야기가 있는데, 이에 대해서는 Go community에서도 다양한 의견이 있었던 것 같다. 최근 버젼의 Go에서는 해당 이슈가 픽스되었다는 의견도 있고, 그 정도 Spike가 있어도 GC가 가져다 주는 생산성이 더 크기 때문에 감수해야한다는 의견도 있었던 것 같다. 하지만 여태까지는 네이버에서 Golang을 이용하면서 모니터링해봤을 때는 GC로 인한 말썽은 없었다는 것 같다!    Pointer의 개념이 여전히 존재하지만 배우기 쉽다.\n 이 부분 같은 경우는 내가 Docker를 처음 배우던 시절에 느꼈던 것과 유사한 것 같다. 사용법이 어려운 게 아니라 그게 왜 필요한 지 이해하고 잘 사용하는 게 어려운 느낌이다. 근데 Go가 어려운 것은 보통 \u0026ldquo;어떻게 해야 Concurrency programming의 장점을 잘 이끌어 낼 수 있을까?\u0026rdquo;, \u0026ldquo;channel을 통한 communication, synchronization을 어떻게 해야 잘 이용할까?\u0026rdquo; 와 같은 내용들인데 물론 Go가 그런 부분에서 장점을 가진 것은 맞지만 이런 부분을 꼭 이용하지 않더라도 이제는 Go가 충분히 그 외의 매력도 많다는 것이 점점 인정받고 있는 것 같아 다양하게 사용되고 있는 것 같다. 예를 들어 prometheus나 docker cli 같은 Go로 작성된 유명한 오픈소스들의 코드를 까봐도 그닥 \u0026lsquo;와 이걸 동시성을 이용해서 풀어냈네..!', \u0026lsquo;와 이걸 channel들로 pipeline을 만들어서 진행하네?!' 이런 부분은 찾기 힘들었던 것 같다.    패키지 관리가 쉽다.\n 따로 npm이나 pypi 같은 곳에 업로드 할 필요도 없고 Git을 기반으로 해서 release나 commit 단위로도 패키지의 버젼을 지정할 수 있어 개인적으로도 아주 맘에 든다! 패키지를 설치할 때에도 $ go get ... 명령어를 이용하면 되지만 보통은 그냥 Jetbrains사의 Goland IDE에서 그냥 Alt + Enter를 연타해서 이용 중이다 ㅎㅅㅎ    Unit test를 기본적으로 지원해주고 테스트 하기가 아주 편하다.\n 내가 처음 TDD를 시작하고 Unit test 작성을 시작했던 것도 Go를 공부하면서 였다! 몇 년 전 node.js를 막 공부했을 때 Test code를 짜볼까싶어서 test framework들에 대해 알아봤는데 그냥 테스트 프레임워크를 고르는 것부터가 힘들어서 \u0026lsquo;에잇\u0026hellip;ㅎㅎ 테스트는 다음에..ㅎㅎ\u0026rsquo; 했던 기억이 난다. 요즘은 주로 Spring으로 Test code를 짜고 있다. 너무나도 편리하다. 하지만 Spring은 그런 편리함을 누리기 위해 알아야할 내용이 너무도 많은 게 사실이다. test시에는 어떤 profile을 이용할지, Configuration은 어떤 걸 사용할 것인지, Bean을 어떻게 초기화 할 것인지 등등을 설정해줘야하고 어떻게 동작하는지 알아야한다. 그리고 솔직히\u0026hellip; 한 번 그렇게 공부하고 난 뒤에 테스트를 짜다가 다른 Layer의 코드에 대한 테스트를 작성하려면 다시 \u0026lsquo;음..? 어떻게 동작하더라\u0026hellip;..\u0026rsquo; 하고는 다시 내용들을 찾아봐야하는 경우도 많았던 것 같다. 반면 Go는 간결하고 직관적으로 쉽게 쉽게 테스트 할 수 있다. 별로 고민할 것들도 없다. 다만 초큼 아쉬운 건 mocking을 하려면 다 interface로 선언해야한다는 것과 mocking type을 자동으로 관리하기 힘들고 mocking library를 선택하고 익혀야한다는 것..?    SWIG를 도입했다. C/C++ 등의 라이브러리를 Go(혹은 그 외의 다양한 언어)에서 사용할 수 있도록 추상화 시켜주는 오픈소스\nSWIG에 대한 설명을 하긴 하지만 결국 요점은 C++ ⇒ Go 로 넘어갈 때 기존 코드를 모두 버린 것이 아니라 다른 방법은 없을까 알아보고 적절한 기술을 빠르게 채택했다는 점이 중요한 점 같다.\n만약 C++⇒ Golang을 고려중인 기업이나 개발자분이 계시다면 좋은 참고자료가 될 것 같긴하다.\n나에게는 별로 필요 없으니 패스하겠다~!\nSWIG를 이용했지만 그럼에도 존재했던 Go와 C++의 언어적 차이 Go에서 C++로 마이그레이션하면서 목표는 \u0026ldquo;Go는 Go답게!\u0026quot; 이용하는 것으로 정했다고 한다. 이 파트도 거의 SWIG 특화 내용이라 패스하도록 하겠다.\n다만 무슨 작업을 할 때 저런 식으로 슬로건이나 목표를 정해놓고 나아가는 것은 좋은 방향인 것 같았다.\n가끔 작업을 하거나 프로젝트를 진행하다보면 일관성이 흐려지는 경우가 있는 것 같다. 예를 들면 얼마전 토스의 컨퍼런스에서도 결제 SDK, API를 만들면서 API Naming 같은 것을 할 때의 팀 내 룰을 설명해줬던 것 같다. 그 정도로 협업을 할 때 일정한 룰을 정해놓고 그것을 준수하는 것은 참 중요할 것 같다는 게 다시 한 번 와닿았다!\nShared memory를 이용해 멀티 프로세스로 작업하던 기존 방식 기존의 Apache MPM Prefork를 이용하던 방식에서는 멀티 프로세싱을 통해 병렬성, 동시성을 이용했다고한다. 이때 여러 프로세스가 공유하는 Shared cache를 이용했고 이 캐시는 Key-value 형식의 Map이었다고 한다.\n하지만 Go는 멀티프로세싱 방식보다는 Goroutine을 이용한 동시성 프로그래밍과 channel을 이용한 goroutine간의 communication을 권장한다.\n기존에는 multi-process에서 shared memory 형태의 shared cache를 이용했는데 이것을 유사하게 이용할 방법을 잘 찾지 못했다고 한다. 그 이유는 Goroutine간의 데이터 공유는 직접 공유가 아닌 channel을 이용한 communication 방식을 이용할 것이 대체로 권장(즉 기존의 방식과 너무 다른 방식)되었고, 그 이외의 방법들은 서버/클라이언트 방식이나 gRPC 방식이었기 때문이라고 한다. 그리고 결과적으로는 이런 방식들보다는 Memory상의 SQLite를 이용하기로 했다고 한다.\n아마 여러 Goroutine이 동시적으로 접근해도 SQLite도 나름의 DB라서 여러 goroutine이 동시적으로 동작해도 동시성 문제가 없을 것이고, 드라이브가 아닌 메모리를 바탕으로 하도록 설정도 가능해 속도도 빠르기 때문이 아닐까 싶다. 하지만 개인적으로는 여기서 이해되지 않는 내용이 꽤 많이 존재한다. Go는 goroutine간의 데이터 공유를 주로 communication을 이용할 것이 권장된다고 하셨는데 그런 경우는 다음과 같지 않을까 싶다.\n goroutine이 서로간에 데이터를 ping pong하며 주고 받으며 동작하는 경우 여러 스레드(혹은 고루틴 혹은 프로세스)가 동시에 이용 가능하도록 동기화가 지원되는 큐처럼 이용할 경우 semaphore처럼 작업량을 조절하려는 경우  하지만 이런 communicate 방식 말고도 Go에도 충분히 Mutex가 제공되고 게다가 Read/Write에 대해 다르게 Lock을 걸 수 있는 기능도 있는데, 굳이 SQLite3를 사용할 필요가 있었을까 싶기도하다. 아니면 다른 관점에서 봤을 땐 Go의 Mutex 기능을 이용하는 것이 그리 쉬운 편은 아니라는 점에서 너무 Go스러운 방식을 적극 활용하는 쪽 보다는 그냥 일반적인 개발자들이 친숙한 SQLite와 Database적인 특징을 이용하는 쪽이 협업하기에 더 좋을 수도 있을 것 같다.\n성능 Go가 C++ 보다 나은 성능을 보여주냐보다는 Go가 C++만큼의 성능을 낼 수 있는가의 관점에서 평가했다고한다. 그리고 Go는 기존의 C++ 검색엔진 만큼의 작업 처리량을 보여줬다고한다.\n기본적으로 Go Binary가 큰 편이고 Shared memory를 이용하던 것을 SQLite3를 쓴 점에서 메모리의 차가 좀 크게 난 것 같다고 한다.\n생산성이 정말 좋아졌는가? 4명이서 시작한 킥오프, 5개월만에 검색엔진을 Go기반의 모듈로 전환 완료\n Go는 C++에 비해 러닝 커브가 낮다고 생각 테스트를 짜는 것도 돌리는 것도 쉬움 go build, go test, go test -bench, godoc 등의 기본 명령어를 통해 빌드, 테스트, 벤치마크, 문서화등을 이용가능 gofmt나 goimport 통일된 기능과 같은 깐깐한 Compiler로 인해 깔끔한 코드 유지 가능. 패키징이 아주 간단하다. go.mod에 한 줄만 추가하면 됨. 소스도 까보기 쉬움.  C++ ⇒ Golang으로 마이그레이션 한 뒤 고객의 소리 (검색 엔진 라이브러리를 이용한 타 부서 사람들의 의견)  로직 파악이 쉽고, 다른 개발자의 코드를 Import 하기 쉬웠다. TDD를 적용해볼 수 있었다. 표준 라이브러리(marshaling 등등)이 파워풀했다. 단순하고 러닝 커브가 적어서 좋았다.  개인적으로는 확실히 Go 코드가가 엄청 읽기 쉽긴하다. 불필요한 코드는 애초에 컴파일러가 허가해주지 않으니 읽는 사람 입장에서는 모든 코드가 필요한 내용들로 구성되어있으니 간결하다. 또한 강한 Type 강제 언어답게 어떤 메소가드가 어떤 것을 리턴하는지, 어떤 인자를 필요로 하는지가 분명하다. 게다가 Type 언어이면서도 Java와 달리 앞서 말했듯이 간결하면서도 강력하고 통일된 기능들을 제공한다. 약간 자바는 기능도 엄청 많은 대신 설정해야하는 것도 너무 많아 번거로울 때가 있는 반면 Go는 \u0026ldquo;우리가 표준으로 정의한 대로 해!\u0026ldquo;라고 강제하는 느낌이 있는데 그 표준이 꽤나 깔끔해서 반박의 여지가 적은 것 같다. 마치 CISC(Complex Instruction Set Architecture)의 형태로 발전하다 \u0026ldquo;아 몰라! 너무 복잡해. 다시 간단하게 해!!!\u0026quot; 라며 RISC의 형태로 발전된 느낌이랄까\u0026hellip;ㅎㅎ\n총평 및 의견 실제로 기존 C++ 라이브러리를 Go로 마이그레이션 해보았으며, Go를 모르던 타부서 개발자들도 Go로 마이그레이션된 라이브러리를 무리 없이 이용했다는 썰이 흥미로웠고, 당근마켓이 그러했듯 네이버에도 어쩌면 Go 붐이 이를 수 있지 않을까싶다. 만약 그렇게 된다면 나중에 네이버에 지원하는 신입 중엔 꽤 Go를 잘 이해하고 잘 사용할 자신이 있을 듯 하다..!\n성능이 C++보다 잘 나오느냐의 관점보다는 C++만큼의 작업 처리량을 낼 수 있느냐는 점도 좀 재미있었다. 이제는 Go도 그 매력을 인정받으며 점점 대중화되고 있기 때문에 \u0026ldquo;꼭 Go를 써야할 절대적인 이유가 있어? 다른 언어들도 그런 이유는 어떻게든 커버 가능하잖아.\u0026quot; 의 관점에서 바라보지 않는다는 것이다. Go가 다방면에서 좋은 장점이 있기 때문에 \u0026ldquo;꼭 Go만의 절대적인, 유일한 장점, 기능을 써먹는 케이스가 아니더라도 기존 작업량을 따라 갈 수만 있으면 성능도 괜찮고, 관리도 편리하고, 러닝 커브도 적고, 테스트도 쉬운 좋은 언어인데 도입해볼만하지 않아?\u0026quot; 라는 관점에서 접근하게 된 것이다.\n실제로 Go를 적용해봤는데 발표자분이 속한 팀에서도 또 그 외의 부서에서도 사람들이 잘 따라온다고하니 역시 Go가 러닝 커브는 높지 않고 간결한 언어인 것 같구나 싶기도 하고, 사실 한 언어를 잘 하는 것도 힘들 수 있는데 곧잘 다들 Go라는 언어를 익히신다니 역시 Naver의 개발자들이구나 싶은 생각도 들었다.\n","date":"2021-06-03T15:46:54+09:00","image":"https://umi0410.github.io/blog/conferences/naver-deview-2020-cpp-to-golang/preview_hu5e425158693deacd0d5ef6841733ed22_379145_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/conferences/naver-deview-2020-cpp-to-golang/","title":"Naver Deview 2020 C++ to Golang 리뷰"},{"content":"시작하며  이 글은 Go 1.15 버전을 바탕으로 개발하며 겪은 이슈에 대해 설명하고 있으며 Go 1.16에서는 해당 이슈가 개선될 것이라고 합니다.\n  memory-leak-graph.png \n진행 중인 쿠뮤라는 프로젝트에서 Go를 이용해 이미지에 대한 url 해싱, 리사이징, 섬네일, 센터 크롭 작업을 하는 이미지 프로세싱 마이크로서비스를 개발하고있었다. 1차적으로 어느 정도 개발이 끝난 뒤 벤치마크 겸 부하 테스트 겸해서 얼마나 해당 마이크로서비스가 잘 버티면서 작업을 수행하는지 확인해보려 했으나 Memory가 한 번 치솟게되면 어느 정도 이하로 떨어지지 않는 이슈가 발견되었다.\n허용할 수 있는 양보다 많은 요청을 보낼 경우 애플리케이션 레벨 이전에서 요청을 차단해주지 않으면 애플리케이션 단에서는 터지거나 문제가 생기는 것은 당연하겠지만 사진처럼 작업을 다 수행한 뒤에도 메모리가 제대로 해제되지 않는 것이 이슈였다.\n이슈의 원인을 파헤치기 위해 했던 노력들\u0026hellip; 이 이슈를 잡아보려 여러 가지 디버깅 작업을 해봤으나 계속해서 메모리 이슈가 발생했고, 각종 커뮤니티에서 도움을 구해보고자했다. Go 오픈 카톡방, Reddit, Slack 등에서 의견을 구해보았다. 재미있는 경험이었으며 덕분에 원인을 파악할 수 있던 것 같다.\n slack-thread.png \n reddit.png \n어떤 상황에 Memory Leak이 발생한 것일까? 그럼 자세히 어떤 이슈가 있었고, 어떻게 그 상황을 분석할 수 있었으며 어떻게 해결할 수 있을지 알아보도록하겠다.\ntl;dr - 자세한 묘사보다는 그냥 딱 원인/결과만 궁금하신 분들을 위한 요약\n 원인 - 힙 메모리에 메모리를 할당받았는데 혹시 재사용할 수도 있어 OS에게 바로 메모리를 반환하지 않음. 결과 - OS가 메모리 부족하니 달라고 하면 그때 힙 메모리 사이즈를 실제로 줄인다. (하지만 go 1.16 버전부터는 개선될 예정이다.) 정확히 이러한 것을 메모리 누수라고 하는지는 명확하지 않다. Go 런타임이 놀고 있는 메모리를 갖고는 있지만 언제든 OS에게 반환해줄 수는 있기 때문이다. (하지만 매끄럽게 OS에게 반환해주는 느낌은 아니긴했다.)  package main import ( \u0026#34;time\u0026#34; ) func main(){ for routine := 0; routine \u0026lt; 10; routine ++{ DoFloat() } for { // dummy waiting..  time.Sleep(3*time.Second) } } func DoFloat(){ var tmp [400000000]float64 tmp[0] = 0 // tmp에 접근하지 않으면 unused variable이 되기 때문에 dummy한 access 작업 수행 } 처음엔 이미지 리사이징 시 메모리가 제대로 해제되지않는 것을 보고 이슈를 발견했지만, 사람들에게 도움을 구하고자 할 때, 이슈에 대해 설명할 때 전체 프로그램 코드를 첨부할 수는 없었기에 문제 상황을 간단하게 표현할 수 있는 코드를 짜보고자했다. 이리 저리 프로그램을 간소화하면서 커다란 배열 생성시에도 같은 메모리 이슈가 발생한다는 것을 알게되었다.\n그래서 아주 간단한 배열 생성 예시를 통해 사람들에게 이 이슈에 대해 설명해보고자했다. 이 예시에서는 8바이트의 float64로 이루어진 400000000칸의 배열 tmp를 선언한다.\n unit-conversion.png \n8바이트가 400000000칸이면 400000000 * 8 / 1024(KB) / 1024(MB) / 1024(GB) = 약 2.9GB 혹은 그대로 1000단위 씩으로 나눠 3.2GB을 할당하는 것이다.\n원래는 스택 메모리에 할당된 뒤 다른 곳에서 이 녀석을 참조하는 일이 없기 때문에 바로 release되어 스택 메모리에서 점유가 해제되어야한다. 하지만 몇몇 경우에 스택이 아닌 힙에 데이터가 저장될 수 있다고 하는데, 이 경우에는 너무 큰 값을 선언하여 스택이 아닌 힙에 데이터가 저장되었고, 힙이 할당받은 메모리를 해제해주지 않아 생기는 문제였다. 사실 이런 현상도 정확히 메모리 누수 혹은 Memory Leak라고 하는지는 잘 모르겠다. 왜냐하면 메모리에서 해제할 수 없는 수준으로 그 값의 주소를 잃어버려 실제로 그 공간이 누수가 되는 것이 아니라 아직 딱히 OS가 부담을 느끼지 않기 때문에 Go Runtime의 Heap에서 해당 주소는 비워뒀지만 OS에게 반납은 안 한 상태인 것이기 때문이다.\n top-memory-leak.png \n실행해보면 10번의 DoFloat() 후에 그냥 for loop에서 time.Sleep 중이기에 CPU를 거의 사용하지 않고 있는 반면 내 Laptop의 16GB의 Memory 중 20.5%인 3.2GB를 사용 중인 것을 확인할 수 있었다.\n해결 해보기 Go runtime은 OS에게 더 이상 이 메모리가 필요하지 않다고 알려주기만 하지 그 Memory를 실제로 회수할 지 말지는 OS에게 달려있다.  The Go runtime only advises the OS when it no longer needs memory and it is up to the OS to reclaim it - @justinisrael\n 내가 Reddit에서 사람들께 여쭤봤던 글에 담긴 한 댓글을 인용해보았다. 과연 저 말이 사실일까? 프로세스를 여러번 띄워보았다.\n top-multiple-process.png \n프로세스를 하나 띄웠을 때에는 아까는 Dummy waiting 중이면서도 Memory를 20% 가까이 점유하고 있었는데 여러 프로세스를 띄우면서 메모리가 부족해지자 놀고있던 Heap memory를 반환하여 거의 약 0.6%의 메모리만 점유 중인 것을 볼 수 있다.\n마지막 프로세스는 아직 DoFloat() 작업을 진행 중이므로 여전히 19.8%의 메모리를 점유 중이고, 작업이 완료된 뒤에도 OS가 Reclaim(다시 메모리를 가져가는 것)하기 전까지는 약 20%대를 유지하는 것으로 보여졌다.\n하지만 좀 더 자세히 기록해보고싶었다. Go의 내장 패키지 runtime의 Memory 관련 기능을 이용하면 될 것 같았다. runtime.ReadMemStats(*runtime.MemStats)를 이용하면 런타임 도중 자신의 런타임 상황을 알아볼 수 있다. 자세한 사용법은 구글링을 통해 쉽게 얻을 수 있으니 지면 관계상 생략한다.\n runtime-memory-leak.png \n즉 힙 메모리가 확보간 공간은 거대한 배열을 생성하는 함수인 DoFloat을 진행하는 동안은 실제로 Allocate 할당하여 사용하는 반면 DoFloat을 모두 마친 뒤에는 힙 메모리는 공간을 확보하고는 있지만 Idle(놀고 있는) 상태로 존재하는 것이었다.\n나는 이러한 경우를 처음 맞이했지만, 각종 가비지 컬렉터가 있는 언어에서 각각의 가비지 컬렉터나 런타임을 구현하는 방법에 따라 이런 식으로 힙 메모리를 재사용하는 경우를 대비해 한 번 할당받은 메모리를 완전히 OS에게 반환하지 않는 경우가 있다고 한다.\nGo 1.16 버전부터는 메모리 계산 방식이 바뀔 것이다. (그래서 괜찮을 것이다.)  \u0026ldquo;There was a change to how memory is calculated on 1.16\u0026rdquo; - @gopj\n 과연 사실일지 확인해보자. 간단히 Docker를 통해 나의 Local 환경에서 별 다른 세팅 없이 Go 버전을 다르게 하여 실행할 수 있었다. 실제로 이 이슈를 커뮤니티에 제기하기 전에도 Docker를 이용해 1.2, 1.3, 1.4 등의 버전에서도 실행해보았다. 나의 랩탑은 1.5 버전을 사용 중이었다. 1.16 버전은 아직 rc는 Release Candidate의 줄임말로 보통 배포 후보 버전을 의미한다.\n# DockerfileFROMgolang:1.16-rcWORKDIR/appCOPY main.go main.goENTRYPOINT [\u0026#34;go\u0026#34;]CMD [\u0026#34;run\u0026#34;, \u0026#34;./main.go\u0026#34;]# 이미지 빌드 후 컨테이너 실행 $ docker build . -t tmp \u0026amp;\u0026amp; docker run --rm --it tmp  go-1.16-memory-not-leaking.png \n1.16-rc 버전을 사용하자 결과적으로 다른 메모리 부하가 심한 프로세스를 실행시키지 않았지만 수십 초 이내에 사용하지 않는 힙 메모리가 OS에게 반환되었다!!!\n 하지만 여전히 runtime.MemStats에서는 HeapMemory에 Idle한 메모리 크기가 크게 잡혀있었는데, 이 부분은 rc 버전이기 때문에 runtime까지 완전히 기능이 개발되지 않아서인지 이전에도 메모리는 반환했지만 메모리를 표시하기 위한 계산의 문제만 개선이 된 것인지는 확실하진 않지만 여튼 이슈에 대해서는 파악할 수 있었다!\n 마치며 개발이나 운영을 하면서 이런 저런 이슈들이 있었지만 이번 경우처럼 로우 레벨스럽게 들어가서 런타임 동안 메모리 관리가 어떻게 되는지까지 탐구해본 적은 드물었던 것 같다. 도저히 원인을 모르겠어서 \u0026lsquo;하 결국 포기하고 이 마이크로서비스는 Lambda로 돌려야하나\u0026hellip;\u0026rsquo; 싶었다. 하지만 몇 시간을 고생하고 직접 의견을 구하러 다니면서 새로 배우게 된 내용도 많았고, 외국에 계신 몇몇 개발자 분들과도 이렇게 소통할 수 있다는 것이 신기했다. (그리고 참 세세한 지식까지 겸비한 분들이 많다는 것이 놀라웠다\u0026hellip;)\n결과적으로 해당 이슈는 아마 Go의 1.16 버전이 패치되면 완전히 해결 가능할 것 같고 그 이전에도 사실 Host에서 메모리 부담을 느끼면 Go runtime이 안 쓰고 있는 힙 메모리를 반환해준다고 하니 큰 문제는 없을 것 같다. 하지만 실제로 Host가 부담을 느끼는 선 이전에 메모리를 반환받고 싶다면 Pod level에서 메모리 리소스를 제한해볼 수는 있을 것 같다.\n혹시 다음에도 이런 흔치 않은 고된 이슈를 맞이한다면 그 원인과 해법을 이렇게 또 기록할 수 있기를 바래본다.\n","date":"2021-01-27T15:25:54+09:00","image":"https://umi0410.github.io/blog/golang/go-memory-leak-issue/memory-leak-graph_hu1f014e9e399a42abc11cd4e02aac56df_8940_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/golang/go-memory-leak-issue/","title":"개발 썰 - Go Memory Leak(메모리 누수) 관련 이슈"},{"content":"✋ 시작하며 Go를 공부하기 시작한 지도 벌써 몇 달이 지난 것 같다. 데브옵스 인턴을 마치면서 특히나 관심있었던 Go를 공부하기 시작했었고, 지난 몇 달간 AWS KRUG내의 소모임인 AUSG의 스터디 활동으로 Go를 주제로 공부해왔다. Golang의 꽃이라고 할 수 있는 요소들이 몇 개 있었는데 나는 그 중 goroutine과 channel에서 매력을 느꼈고 그를 바탕으로한 concurrency pattern들에 대해 이래 저래 많이 알아봐왔다.\n하지만 concurrency pattern이 뭐고, 어떻게 사용하는지에 대해서는 여러 글을 찾아볼 수 있었지만 이게 왜 좋고 언제 쓰면 좋을지에 대한 내용은 찾아보기 힘들었다. 항상 어떤 기술을 접할 때 \u0026ldquo;왜 좋은데?\u0026ldquo;와 \u0026ldquo;언제 쓰면 좋은데?\u0026ldquo;를 많이 따지는 편이라서 늘 궁금증에 남아있었다.\n그러던 중 얼마 전 나의 궁금증을 해소시켜주는 간단한 댓글을 보게 되었고, 그를 바탕으로 몇 가지 서치를 해본 결과 concurrency pattern 중 하나인 pipeline pattern을 언제 쓰면 좋을지 알아보았다.\n❓ Pipeline pattern이란?  illustration.png \nPipeline pattern은 golang의 Concurrency pattern 중 하나이다. 쉽게 말하자면 함수의 인자에 값을 전달해 작업하는 것이 아니라 channel 을 통해 실시간으로 함수(혹은 goroutine)들끼리 커뮤니케이션을 하며 작업을 진행하는 것이다. 실시간으로 함수들끼리 커뮤니케이션하며 data를 전송하기 때문에 data streaming 같은 느낌이라고 볼 수 있겠다.\n일반적으로는 한 함수에서 작업이 모두 끝난 뒤 그 결과값을 리턴하고, 또 다시 그 결과 값을 인풋으로 어떤 함수가 작업을 하고 해당 작업이 모두 끝난 뒤 또 다른 결과값을 리턴하는 형태로 진행이 된다. 하지만 Pipeline pattern에서는 한 작업이 모두 완료되지 않았다하더라도 해당 작업의 부분 부분의 데이터를 담는 channel을 리턴하고, 다른 작업은 해당 channel을 인풋으로 하여 부분 부분의 데이터를 channel에서 꺼내 바로 바로 작업한다.\n python-generator.png \ngenerator을 이용한 lazy evaluation. 0, 1, 2를 모두 Insert한 뒤 square and print 하는 것이 아닌 하나씩 실시간으로 진행\n이는 마치 Python의 Generator을 통한 Lazy evaluation을 이용할 때와 유사하게 볼 수는 있겠다. 하지만 많은 차이점이 존재하긴 할텐데 우선 generator는 thread-safe하지 않은(stackoverflow 참고) 반면 channel은 여러 goroutine에서 한 channel에 접근해도 thread-safe하다. \u0026lsquo;thread-safe한가\u0026rsquo;가 중요한 이유는 후에 잠깐 소개할 fan-in fan-out pattern이 바로 pipeline pattern이 여러 goroutine에서 이루어지는 경우이기 때문이다.\n(사실 내가 Pipeline pattern을 어디에 사용할 지 몰랐던 이유는 Lazy evaluation과 같은 개념이 없었기 때문이 아닐까싶기도 하다.)\nPipeline pattern의 흔한 예시로 square 하는 예제를 설명하는 글들 종종 Pipeline pattern의 예시를 알아보면서 square 작업을 하는 예제들을 몇 개 봤던 것 같은데, 그 중 참고할 수 있는 예시들을 제시해본다. square 작업이 pipeline pattern에 효율적이라기보다는 그냥 임의의 예시라고 생각한다.\n이 글은 Pipeline pattern을 언제 사용하면 좋을지에 초점을 맞추었기에 Pipeline pattern이 뭔지, 그 사용법은 어떻게 되는지 등의 내용을 모르는 상태라면 아래 글들을 추천한다. (특히 Aidan Bae님의 블로그에는 pipeline pattern 외에도 다양한 Go 관련 내용을 잘 적혀있다.)\n Aidan Bae님의 아주 간단한 Pipeline pattern 예시 - https://aidanbae.github.io/code/golang-design/pipeline/ GoBlog의 간단한 내용부터 꽤나 어려운 내용까지 들어가는 것 같은데, 내용도 길고 짧은 코드에 억지로 많은 내용을 끼워넣은 느낌.. https://blog.golang.org/pipelines  🤔 언제 쓰는 게 좋을까? 앞선 lazy evaluation 관련된 내용과 이 글의 댓글(go로 작업을 stream하라는 글에 \u0026ldquo;너 정말 저렇게 stream하는 게 좋다고 생각해? 이 경우 아니면 그닥 잘 모르겠는데?\u0026ldquo;라는 댓글)이 나의 이해를 도와주었다. Pipeline architecture를 사용해야하는 7가지 이유라는 글을 읽어보기도 했지만 여기의 내용은 실질적으로 왜 그 장점을 갖는지에 대한 설명이나 논리가 부족했다. (다소 Go와 Pipeline pattern에 대한 무조건적 사랑으로 느껴짐..ㅎㅎ\u0026hellip;)\n내 생각에는 아래 두 가지 경우에 Pipeline pattern을 이용하는 것이 좋을 것 같다.\n Input 작업을 모두 수행하는 데에 오랜 시간이 걸리는 경우 작업하는 데이터의 양이 너무 커서 Memory를 많이 점유하므로 쪼개서 바로 바로 처리하고 싶은 경우  \u0026lsquo;디버깅하기 쉽다\u0026lsquo;거나 \u0026lsquo;코드를 알아보기 쉽다\u0026rsquo; 등등의 글을 보긴했던 것 같은데 개인적으로는 그냥 pipeline과 channel을 안 쓰고 그냥 반복문을 돌리면서 데이터를 통째로 작업하는 게 디버깅이나 가독성면으로 훨씬 유리하다고 생각한다. 즉 Concurrent pattern의 장점은 특정 use case에서의 성능적인 측면이라고 생각한다.\n그리고 이건 여담인데 go의 channel을 통한 data 전달이 일반적으로 빠른 건 절대 아니다. channel은 우선 thread safe하기때문에 일반적인 작업에서는 thread-unsafe한 방식보다 느린 것이 당연하다. 또한 channel은 단순한 lock 기능 외에도 다른 goroutine(혹은 thread 혹은 function) 간의 데이터 전송, channel에 대한 반복, 열었는지 닫았는지에 대한 체크 등등 다양한 기능도 갖고있기때문에 mutex에 비해서도 훨씬 느린 듯하다. 그렇기 때문에 위에서 나열했듯 Input 작업이 오래 걸리거나 Memory 점유량을 줄이기 위한 경우에는 channel을 이용한 Pipeline pattern으로 실시간으로 작업하면 유리할 것이라고 생각한 것이다.\n그리고 추가적으로 앞에서 Fan-in Fan-out pattern에서는 여러 Goroutine에 대한 Pipeline pattern이 적용된다고했는데, Fan-in Fan-out이란 여러 goroutine이 하나의 channel에 값을 넣거나 빼는 구조를 말한다. 이 구조의 장점은 일반적인 Pipeline Pattern에서 각 단계가 하나의 goroutine만을 이용하는 것이 아니라 여러 goroutine을 이용할 수 있다는 점이다. CPU bound한 작업이 아닌 IO Block이 주요 latency를 차지하는 경우는 Goroutine을 늘려주면 concurrent하게 작업할 수 있기때문에 성능이 좋아지더라. Fan-in Fan-out pattern에 대해서는 기회가 된다면 더 자세히 다뤄보겠다.\n참고\n Mutex보다 channel이 느리다는 벤치마크 글 - http://www.dogfootlife.com/archives/452 Mutex를 쓸 수 있는 상황이면 Mutex를 쓰는 것이 좋을 수도 있다 - https://github.com/golang/go/wiki/MutexOrChannel  예시 프로그램 프로그램 설명 Input 작업을 수행하는 데에 오랜 시간이 걸리는 경우를 예시로 들기 위해 MySQL에서 Gopher에 대한 데이터를 조회한 뒤 Gopher들의 연령을 +10 증가시키는 예시를 만들어봤다.\nInput 작업을 모두 수행하는 데에 오래 걸리고, 작업 내용을 조금씩 쪼갤 수 있다는 전제조건을 위해 DB Query를 할 때 전체를 한 번에 Select 하는 것이 아니라 1개씩 Select하는 다소 비효율적이고 비현실적인 상황을 이용하긴하지만 간단하게 Pipeline pattern의 효율을 극대화시켜보고자했던 이유이므로 양해를 부탁드린다.\n예를 들어 Pipeline pattern을 통해 1000개의 Row를 Query하고 난 뒤 1000개를 Update하는 것이 아니라 1000개의 Row를 Query하면서 1개 1개의 Row를 얻어올 때 마다 바로바로 Update 작업에 Row를 넘겨줘 실시간으로 작업할 수 있게해주는 것이다.\n Pipeline pattern: 한 item 씩 실시간으로 전달하며 진행 Sequential pattern: 딱히 Pattern이라기엔 좀 그렇지만 그냥 일반적으로 Sequential하게 진행하는 경우를 의미. 이 예시에선 전체 Query 완료 후 전체 Update하는 방식.  Docker를 이용해 간단하게 MySQL 서버를 띄웠고, Gorm을 이용해 DB Query와 update를 수행했다.\n예시 프로그램 코드\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/brianvoe/gofakeit/v6\u0026#34; \u0026#34;gorm.io/driver/mysql\u0026#34; //\u0026#34;gorm.io/driver/sqlite\u0026#34; \t\u0026#34;gorm.io/gorm\u0026#34; \u0026#34;gorm.io/gorm/logger\u0026#34; \u0026#34;math/rand\u0026#34; \u0026#34;time\u0026#34; ) var ( NumTotalData int = 100 // 초기화할 데이터 \tNumManipulateData int = 100 // fetch 후 map을 적용할 데이터 수 ) type Gopher struct { gorm.Model Name string Age int } func initializeDB(db *gorm.DB) { // Migrate the schema \tdb.AutoMigrate(\u0026amp;Gopher{}) // Create initial data \tfor i := 0; i \u0026lt; NumTotalData; i++ { gopher := \u0026amp;Gopher{Name: gofakeit.Name(), Age: rand.Intn(60) + 1} gopher.ID = uint(i + 1) db.Save(gopher) } fmt.Println(\u0026#34;Initialized DB\u0026#34;) } type SequentialPattern struct { DB *gorm.DB } type PipelinePattern struct { DB *gorm.DB } func (sp *SequentialPattern) Execute() { start := time.Now() sp.Map(sp.FetchData()) fmt.Println(\u0026#34;sequential\u0026#34;, sp) fmt.Println(\u0026#34;Elapsed:\u0026#34;, time.Now().Sub(start)) fmt.Print(time.Now().Sub(start)) gophers := make([]Gopher, 0) sp.DB.Limit(NumTotalData).Find(\u0026amp;gophers) fmt.Printf(\u0026#34;Average age(Num: %d): %d\\n\u0026#34;, len(gophers), func() int{ totalAge := 0 for _, gopher := range gophers{ totalAge += gopher.Age } return totalAge/len(gophers) }()) } func (sp *SequentialPattern) FetchData() []*Gopher { gophers := make([]*Gopher, NumManipulateData) for id := range gophers { gophers[id] = \u0026amp;Gopher{} sp.DB.Where(\u0026#34;id \u0026gt; ?\u0026#34;, id).Limit(1).Find(\u0026amp;gophers[id]) } return gophers } func (sp *SequentialPattern) Map(gophers []*Gopher) { for _, gopher := range gophers { gopher.Age += 10 sp.DB.Save(gopher) } } func (pp *PipelinePattern) Execute() { start := time.Now() pp.Map(pp.FetchData()) fmt.Println(\u0026#34;pipeline\u0026#34;, pp) fmt.Println(\u0026#34;Elapsed:\u0026#34;, time.Now().Sub(start)) gophers := make([]Gopher, 0) pp.DB.Limit(NumTotalData).Find(\u0026amp;gophers) fmt.Printf(\u0026#34;Average age(Num: %d): %d\\n\u0026#34;, len(gophers), func() int{ totalAge := 0 for _, gopher := range gophers{ totalAge += gopher.Age } return totalAge/len(gophers) }()) } func (pp *PipelinePattern) FetchData() \u0026lt;-chan *Gopher { ch := make(chan *Gopher) go func() { for id := 0; id \u0026lt; NumManipulateData; id++ { var gopher Gopher pp.DB.Table(\u0026#34;gophers\u0026#34;).Where(\u0026#34;id \u0026gt; ?\u0026#34;, id).Limit(1).Find(\u0026amp;gopher) ch \u0026lt;- \u0026amp;gopher } close(ch) }() return ch } // 고퍼들을 모두 10살 더 먹게 함. func (pp *PipelinePattern) Map(gophers \u0026lt;-chan *Gopher) { for gopher := range gophers { gopher.Age += 10 pp.DB.Save(gopher) } } func main() { // $ docker run -it --name mysql --rm -e MYSQL_ROOT_PASSWORD=root -e MYSQL_DATABASE=concurrency -p 3306:3306 mysql \t// 로 mysql 서버를 띄울 수 있다. \tdb, err := gorm.Open(mysql.Open(\u0026#34;root:root@tcp(127.0.0.1:3306)/concurrency?charset=utf8mb4\u0026amp;parseTime=True\u0026amp;loc=Local\u0026#34;), \u0026amp;gorm.Config{Logger: logger.Default.LogMode(logger.Silent)}) if err != nil { fmt.Println(err) panic(\u0026#34;failed to connect database\u0026#34;) } initializeDB(db) // Execute with PipelinePattern \tfunc() { fmt.Println(\u0026#34;==============================\u0026#34;) for rep := 0; rep \u0026lt; 10; rep++ { pattern := \u0026amp;SequentialPattern{DB: db} pattern.Execute() } fmt.Println(\u0026#34;==============================\u0026#34;) }() fmt.Print(\u0026#34;DB 부하를 가라앉히기 위해 5초간 휴식\u0026#34;) time.Sleep(5 * time.Second) // Execute with PipelinePattern \tfunc() { fmt.Println(\u0026#34;==============================\u0026#34;) for rep := 0; rep \u0026lt; 10; rep++ { pattern := \u0026amp;PipelinePattern{DB: db} pattern.Execute() } fmt.Println(\u0026#34;==============================\u0026#34;) }() } 📉 벤치마크 결과 비교  line-chart-1.png \nSequential하게 진행할 경우 모든 Item에 대한 Query가 완료될 때 까지 Update 작업은 지연되게 된다. 그리고 Query가 모두 완료되어야 비로소 Update 작업을 시작할 수 있게되므로 좀 더 작업 시간이 오래걸리는 편이다.\n반면 Pipeline으로 진행할 경우 한 Item을 Query 하자마자 바로 바로 Upadte 작업이 이루어질 수 있기때문에 작업 시간이 더 짧은 경향이 있다.\n하지만 이 정도 차이는 뚜렷한 성능 차이로 보기엔 다소 미미한 것 같았다. 그래서 좀 더 IO latency가 긴 상황을 이용해봤다. DB로 localhost에서 docker mysql server를 이용하는 것이 아니라 운영 중이던 k8s cluster의 mysql pod를 kubectl port forward하여 이용해보았다. (정확히는 모르지만 remote db + kubectl port forawrd 를 이용하는 경우 network latency가 아주 커져 극단적으로 좋은 예시 상황이 될 수 있을 것 같았다.)\n line-chart-2.png \n위에서 가정한 remote db + port forward의 극단적 상황은 작업 진행 pattern에 따라 latency를 2배 가량 차이나게 했다.\n즉 한 단계에서 오랜 시간이 소모되어 다음 단계가 지연되는 경우 Pipeline을 이용하면 좋은 것 같다. 이는 마치 컴퓨터 구조에서 MIPS Processor의 Pipeline을 공부할 때와 유사한 듯한 느낌을 줬다.\n마무리  이 글이 정확한 내용은 아닐 수 있지만 조사해본 선에서 벤치마크 해보며 작성해봤습니다. 혹시 글의 내용 중 잘못된 내용에 대한 피드백을 제시해주신다면 기회가 닿는 한 열심히 다시 알아보겠습니다~!\n 평소 궁금했던 \u0026ldquo;Go의 concurrent pattern은 어떨 때 쓰면 좋을까?\u0026rdquo; 라는 의문 중 pipeline pattern 에 대해 이렇게 알아봤다.\nPipeline pattern이 CPU Bound한 작업보다는 IO latency로 인해 오랜 시간이 소모되는 작업에 더 효율적이라고 생각한 이유는 CPU Bound한 작업에서는 CPU가 이미 혹사당하고 있기 때문에 굳이 여러 goroutine을 schedule, switch하거나 channel을 통한 동기처리를 하면서 작업을 진행하기보다는 그냥 일반적인 방식으로 순차적으로 작업을 진행하는 게 좋을 수 있기 때문이다. 워낙에 얼마나 CPU bound한 작업인지, 얼마나 많은 goroutine을 이용하는지 등등 다양한 경우에 따라 달라지기 때문에 딱 잘라말할 수도 없고, 나도 많이 부족하기 때문에 정확히는 모르겠다. 하지만 추측컨대 적어도 io latency로 인해 작업들이 지연되는 경우에는 pipeline pattern이 좋은 듯하다.\n추가적으로 Fan-in Fan-out pattern 또한 channel을 바탕으로 data를 전달하면서 작업하기 때문에 일종의 Pipeline pattern이 적용된 패턴인듯하여 기회가 된다면 어떤 경우에 단순히 Pipeline 패턴을 이용하는 것보다 여러 Goroutine을 이용해 작업하는 Fan-in fan out pattern이 좋을지 비교해보는 글을 적어보고싶다.\n참고  Aidan Bae님의 Pipeline pattern에 대한 소개 - https://aidanbae.github.io/code/golang-design/pipeline/ Go Blog의 Pipeline pattern에 대한 소개 - https://blog.golang.org/pipelines Pipeline은 ~~~ 이런 상황에 도움이 될 껄?하는 댓글 - https://www.reddit.com/r/golang/comments/7rjdw6/go_go_go_stream_processing_for_go/ Mutex와 Channel 비교 - https://github.com/golang/go/wiki/MutexOrChannel Mutex와 Channel의 속도 차이 - http://www.dogfootlife.com/archives/452 Pipeline architecture를 사용하는 7가지 이유(개인적으론 공감 안 감) - https://labs.bawi.io/7-reasons-to-use-pipeline-architecture-93346f604b87 python generator 소개, 사용법- https://wikidocs.net/16069 python generator는 thread-safe하지 않다. - https://stackoverflow.com/questions/1131430/are-generators-threadsafe  ","date":"2021-01-11T12:46:54+09:00","image":"https://umi0410.github.io/blog/golang/go-concurrent-pattern-pipeline/illustration_hu58467cd983dc70e6751a1d8a840dc2b9_64082_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/golang/go-concurrent-pattern-pipeline/","title":"Go의 Pipeline pattern. 언제 사용해야할까? - Golang concurrent patterns"},{"content":" mzc_logo \n안녕하세요. 이번에 메가존 클라우드의 클라우드 원 팀에서 데브옵스 인턴으로 근무를 하게되었던 박진수입니다. 너무 좋은 팀원들과 많은 경험을 하며 단시간에 성장할 수 있었고, 일하는 동안 매 순간 순간이 너무 행복했었기에 이렇게 인턴 후기를 작성해봅니다. 한 학기를 쉬고 2020.04.13~2020.08.31 까지 인턴으로 근무 했고, 다시 2020-2학기부터는 학교 복학을 하게되었습니다. 기술적인 얘기는 이곳 저곳에 많으니 항상 제가 가장 중요시하는 \u0026ldquo;느낀 점\u0026rdquo; 과 \u0026ldquo;배운 점\u0026rdquo; 을 위주로 적어보겠습니다!! 글은 위의 목록의 순서로 진행해보겠습니다.\n지원동기 저는 AWS와 배포에 관해 흥미가 있어 종종 AWSKRUG 라는 그룹의 소모임에 가서 핸즈온이나 세미나에 참여하곤 했는데요. 그곳에서 저희 팀원들을 만나게 되었고, 그것이 인연이 되어 채용으로까지 이어질 수 있었습니다.\n평소 AUSG 라는 대학생 AWS 사용자 모임의 일원으로서 참여하며 AWS 를 이용한 클라우드 인프라, CI/CD 파이프라인 구축을 통한 자동화, 컨테이너 등에 관심이 많았는데, 마침 메가존 클라우드의 저희 CloudOne 팀에서는 팀의 모든 마이크로서비스 및 기타 서비스들을 EKS라는 AWS의 Managed Kubernetes Cluster Service 위에 자동화시켜 배포를 하고있었고, 개발 인프라를 이전하려던 참이었기에 저의 관심사와 향후 목적에 잘 부합했습니다.\nMegazone Cloud: CloudOne Team?  Megazone Cloud: 국내 최초 \u0026amp; 최대 AWS 프리미어 컨설팅 파트너, AWS 컨설팅, 구축, 운영 및 빌링 서비스 제공. 메가존클라우드는 2009년부터 클라우드를 차세대 핵심 사업으로 성장시키며 ‘클라우드 이노베이터(Cloud Innovator)’로서 고객님들의 클라우드 전환의 과정마다 최선의 선택을 하실 수 있도록 다양한 서비스를 제공하고 있습니다. - 출처(https://www.megazone.com/)\n  spaceone_preview_2 \n AWS EKS를 이용한 Kubernetes 환경, 마이크로서비스 아키텍쳐 Composition API를 이용하는 최신 Vue 기술 Terraform을 통한 IaC 자동화된 CI/CD, 다양한 배포 전략, 인프라 모니터링 HTTP2를 이용한 gRPC API  진행했던 업무들  링크 형식이므로 링크를 눌러 읽어주시면 됩니다.\n  2020.04: Stargate 라는 개발 인프라를 구축  terraform 을 통한 개발 인프라 구축 EKS 클러스터에 jenkins, spinnaker, grafana 등등의 개발 도구들을 배포함 종종 발생하는 장애 상황을 멘토님과 트러블슈팅함   2020.05: 개발 CI/CD 파이프라인 구축  Experiment 환경에 배포 =\u0026gt; Test, CI =\u0026gt; Development 환경에 배포를 자동화 Spinnaker와 Jenkins, Github Action을 이용한 자동화 파이프라인 구축   2020.06: Argo Project들을 PoC  argo cd, argo(workflow), argo-event 등의 다양한 프로젝트에 대한 PoC를 진행   2020.07: spaceone-helm Helm3 Chart 개발  개별로 배포되고 운영되던 우리 팀의 서비스인 SpaceONE을 패키지로 배포할 수 있게해주는 Helm Chart를 개발   2020.08: spacectl 설계, 개발 참여  우리 팀의 서비스인 SpaceONE에 대한 API 작업을 수행하는 CLI 도구에 대한 설계와 개발에 참여함.    느낀 점 그 동안 혼자 개발 및 평소 관심분야였던 클라우드, 컨테이너 등등의 주제로 공부해왔었는데, 과연 이 내용들이 정말 실무에 도움이 될 지, 제가 잘 나아가고 있는 건지 확신이 들지 않았습니다. 하지만 CloudOne팀에서 다양한 경험을 하면서 \u0026lsquo;제가 공부해온 길이 틀리지만은 않았구나\u0026rsquo;라는 느낌을 받을 수 있었고, 보완해야할 부분들은 보완하면서 불확실한 자세가 아닌 확신과 열정을 가진 자세로 좀 더 몰두할 수 있을 것 같습니다!\n배우고 느낀 내용이 너무 많아 글이 다소 길어졌습니다. 기술을 거부감 없이 접하되 기술이 다가 아닌, 팀원들을 위해 솔선수범하는 개발자, 팀원들의 생각을 읽어줄 수 있고, 잘 이해해줄 수 있는 개발자가 되기 위해 노력해야겠단 생각이 듭니다. 쉽진 않겠지만, 나아가고 싶은 방향은 정해진 것 같아 다행입니다!\n","date":"2020-09-04T12:46:54+09:00","image":"https://umi0410.github.io/blog/megazone-cloud/index/mzcloud-logo_huaed2850ad2750bd21c0ea378f4b3d958_6575_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/megazone-cloud/index/","title":"메가존 클라우드 데브옵스 인턴 후기"},{"content":"Stargate라는 개발 인프라 구축기 저희 팀의 개발 인프라는 위와 같습니다. 마침 제가 입사할 쯤이 기존에 존재하던 인프라를 새로운 환경으로 이전해야할 시점이었습니다. 덕분에 저는 저희 개발 환경을 처음부터 구축하고, 우리의 서비스를 배포해보고, 그 후 운영하면서 여러 경험들을 할 수 있었습니다.\nTerraform으로 처음 접해 본 IaC  친구: IaC가 뭔 지 알아? 써봤어?\n본인: 뭐 인프라를 코드로 관리한다는 건데, 나도 몰라 ㅋㅋ\n본인: 아마 너무 고급 기술이라 대학 졸업할 때 까진 못 써볼 듯?\n 위의 대화는 제가 입사하기 약 일주일 전에 나눴던 대화인데, 입사 후에는 어느 덧 IaC와 꽤나 친근해진 것 같아 유머삼아 유머로 가져와봤습니다. 메가존 클라우드의 클라우드 원팀에서 일하기 전까지는 IaC는 저와는 거리가 먼 토픽이었고, Kubernetes 또한 minikube로 몇 가지 Object들을 배포해본 것이 다였습니다. 하지만, 저는 저희 팀에서 근무하게 되면서 terraform을 통해 위의 차트에 그려진 모든 인프라를 구축하고 파이프라인을 구축하게 됩니다.\nterraform을 사용하며 느꼈던 점은 \u0026lsquo;장점만 존재하는 기술은 드물 것이다\u0026rsquo;라는 점입니다. 분명 대부분은 장점이 있으면 그에 따른 단점이 존재할 것이고, 개인적으로는 테라폼도 장단점이 공존하고있다는 느낌을 받았습니다. 저의 주관적인 느낌에 따른 가장 큰 장점과 단점을 몇 개만 설명해보겠습니다.\nTerraform 단점 우선 단점부터. \u0026ldquo;사소한 인프라 변경도 코드에 반영하려면 문서를 찾아봐야하고, plan 내용을 검토해야한다\u0026rdquo;.\n저희는 평소에는 EKS의 로그를 켜놓지 않았습니다. 그런데 언젠가 EKS 로그를 CloudWatch를 이용해 분석해야한 적이 있는데, AWS 콘솔에서 바로 눈 앞에 로그 설정 칸이 있었음에도 EKS 로그를 설정하는 terraform Docs를 찾아본 뒤 plan을 분석한 뒤 apply 했어야합니다. 물론 아마도 로그 설정 쯤이야 잠깐 콘솔로 설정했다가 콘솔로 해제하면 그 한 번쯤은 문제가 없었겠지만, 그런 식으로 이번 한 번만, 이것쯤이야 하면서 코드가 아닌 매뉴얼로 직접 인프라 형상을 제어하는 경험이 쌓이게되면 형상이 깨져서 다시 형상을 맞추기 힘들수도 있고, 애초에 그런 수작업이 많이 들어가야하는 경우가 있다면 오히려 IaC를 이용할 필요가 없다고 생각했기 때문에 최대한 인프라는 terraform code로만 작업한다는 저의 원칙을 지키기 위함이었습니다.\n즉 자동화보다는 수작업이 편한 경우는 굳이 IaC라는 컨셉을 이용하는 것이 더 불편한 경우도 존재할 수 있을 것 같다는 생각이 들었습니다.\nTerraform 장점 \u0026ldquo;내가 사용하고 있는 인프라 전체를 한 눈에 보기 쉽다\u0026rdquo;\n물론 클라우드 서비스 하나에 대한 정보를 보고싶으면, 웹 콘솔에 들어가서 확인하는 것이 편하겠지만, 내가 이용하고 있는 AWS내의 모든 클라우드 서비스에 대한 정보나 설정을 보기에는 terraform 코드나 output, state 등이 더 알아보기 편할 수 있습니다.\n\u0026ldquo;혹시 장애가 생긴 경우 그 원인을 추적하기 쉽다.\u0026quot;\n수작업으로 클라우드 인프라를 관리하는 경우에 자신이 모르는 어떤 변동사항이 있고, 그 변동사항이 어떤 버그를 야기하고 있다면, 수작업으로 작업을 진행하는 경우에는 그 변동사항을 알아채고 트러블슈팅하기 쉽지 않을 것입니다.\n하지만 테라폼 코드를 통해 인프라를 관리하면 변동사항을 테라폼이 알아서 잡아주고, 혹은 코드에 대한 커밋 내역 등을 통해 변동 사항을 체크해 볼 수도 있을 것입니다. 따라서 그 변동사항에 맞는 트러블 슈팅을 하기 쉬울 것 입니다.\n개발 관련 다양한 서비스 배포 테라폼으로 인프라를 구축한 뒤에는 위의 차트에서 오른쪽에 작게 Stargate 라는 곳에 적힌 서비스들을 배포했습니다. 이 부분에 대해서는 너무나도 하고싶은 얘기가 많지만, 지루해질 수 있으니 그 중 기억에 많이 남거나 애착이 가는 서비스들에 대한 리뷰를 간단히 적어보겠습니다.\n Nginx Ingress Controller  로컬에서 minikube로만 개발하다가 처음으로 Ingress를 사용하게 되었습니다. helm을 이용한 게 아니라, helm으로 만들어진 manifest를 수동으로 하나하나 설정해서 설치하고 관리했습니다. 설정이 꽤나 복잡했었기에 설정을 많이 변경하는 경우 helm으로 설치하는 건 어떨까싶습니다\u0026hellip; 일하면서 Nginx로 L7 로드밸런싱 뿐만 아니라 L4 로드밸런싱도 몇 번 다룰 일이 있었고, 여러 Nginx Ingress Controller을 배포할 일도 있었고, gRPC 통신을 위한 설정을 해야할 일도 있었는데, 점점 수작업으로 하다보니 설정이 너무 복잡해져서 헷갈렸던 적이 있습니다. 뭔가 단점만 적은 것 같은데, K8s의 Ingress Controller로서 성능적인 부분은 제가 잘 모르겠지만, 크게 불편함 없이 잘 사용했습니다.   ALB Ingress Controller  서비스에 대해 직접 L7 로드밸런싱을 수행할 경우 이용합니다. 헷갈렸던 점은 보통은 Nginx Ingress Controller는 사실 Ingress는 설정이고, 컨트롤러가 실제로 로드밸런싱을 수행하는데, ALB는 ALB Ingress Controller가 Ingress를 통해 ALB를 만들고 실제 로드밸런싱은 ALB Ingress Controller가 아니라 ALB가 한다는 점이었습니다. 설정에 따라 다르지만 기본적으로는 노출시키고자 하는 서비스는 NodePort급 이상으로 서비스가 열려있어야하는데, 실수로 ClusterIP로 노출시켜 ALB가 서비스를 제대로 찾지 못한 적이 종종 있었습니다. 주의..!   Cert Manager  자동으로 cert를 발급해주고 갱신해주는 서비스입니다. Let\u0026rsquo;s Encrypt를 이용했고 무료입니다. 정말 정말 편리했습니다! DNS, HTTP Challenge, TLS에 대해 많이 배울 수 있었습니다. TLS 통신 과정은 공부해도해도 정확한 순서는 까먹게 돼서\u0026hellip; 다시 공부해봐야겠습니다.   Jenkins  편한데, 관리하기 귀찮을 수 있을 것 같습니다. 결정적으로 코드로 관리할 수도 없는데, UI도 그렇게 직관적인지는 잘 모르겠습니다.   Spinnaker  정말 편리합니다. Jenkins test 결과를 CD에 이용할 수도 있고, 편리하고 다양한 문법, 파이프라인 종류, 직관적인 UI/UX. 사실 Argo를 도입하고 싶었지만, Argo는 약간 가벼운 쿠버 환경에 대한 배포용 같은 느낌, IaC는 적극 도입되었으나 현실에 도입은 쉽지 않은 기술적 이상향 같은 느낌이 컸고, 현실적으로는 좀 더 안정적인 Spinnaker를 유지하게 됐습니다. 좀 느리다고 생각했는데, 이 글을 쓰면서 생각해보니 개발에서는 K8s Deployment의 Grace Period 때문인 것 같아 그걸 좀 줄여볼 껄 싶습니다. 즉 다시 생각해보니 별로 느리지 않은 듯합니다. 배포 자체는 안정적이고 좋은데, 설정할 때 버그가 종종 있습니다. 버그인지 제 실수인지는 모르겠지만 Pipeline stage 에서 Bake라는 모드가 \u0026ldquo;우와 신박하다!\u0026rdquo; 라고 생각했지만, 설정이 제대로 되지 않거나, OAuth login 실패에 대한 처리, Artifact 경로 설정이 너무 번거로운 점 등의 단점이 있었습니다.   Keycloak  우리의 개발툴들에 대한 인증을 사내 계정으로 연동시켜주는 Single Sign On 기능을 지원했습니다. 새로운 팀원은 각 서비스에 별도의 가입 과정 없이, 앱 레벨에서 따로 권한 부여가 필요한 것이 아니라면 사내 계정으로 바로 이용이 가능했습니다. 문서가 제대로 정리된 게 다소 부족한 느낌이었어서 세밀한 설정이나 정확한 작동원리를 파악하기는 쉽지 않았던 점이 조금 아쉽습니다.    인프라 구축에 대한 정리 내용이 너무 길어지면 읽기 힘들 것 같아 최대한 느낀 점 위주로 간단히 정리해보려고 노력해보았습니다. 인턴으로 일을 하기 전에 가벼운 마음 속 목표가 하나 있었습니다.\n \u0026lsquo;어떤 걸 어느정도 사용해보면 그것에 대한 주관적인 평가를 내릴 수 있는 사람이 되고싶다.'\n 누군가 \u0026ldquo;도커가 설치하기 정말 쉽더라구요.\u0026rdquo;, \u0026ldquo;minikube 써보니 정말 편리하더라구요.\u0026rdquo;, \u0026ldquo;Jenkins 보다는 travis가 편리더라구요.(혹은 그 반대)\u0026rdquo; 이런 얘기가 나와도 과거에는 공감할 수도, 제 주관적인 평가를 내릴 수도 없었습니다. 저의 경험으로 구성된 모집단이 없었기 때문입니다. 하지만 저희 클라우드 원 팀에서 근무하면서 어떻게 보면 깊이는 다소 얕았을 지 몰라도 정말 다양한 서비스를 접하고 다채로운 경험을 할 수 있었던 것 같습니다! 덕분에 이제는 어떤 서비스를 접하든, 새로운 언어를 접하는 저만의 느낌을 가질 수 있고, 의견을 말할 수 있을 것 같습니다!\n","date":"2020-09-04T12:46:54+09:00","image":"https://umi0410.github.io/blog/megazone-cloud/stargate-infra/dev-architecture.svg","permalink":"https://umi0410.github.io/blog/megazone-cloud/stargate-infra/","title":"1. Stargate라는 인프라 구축기"},{"content":"Github Action을 사용하게 된 배경 저희 팀은 원래 CI용으로 Jenkins를 사용했습니다만 팀이 개발 중이던 서비스가 오픈소스가 목표인 프로젝트였고, Github Action이 빠르게 발전해나가면서 비용도 무료가 되었고, 좋은 Action들이 많이 생겨나고 있었기에 어느 정도 프로젝트 구조가 잡힌 뒤에는 Github의 Public Repository로 프로젝트를 관리하고 Github Action을 CI 도구로 채택하게되었습니다. Integration Test를 제외한 모든 빌드 및 일부 배포를 Github Action을 이용하게되었고, 대부분의 배포에는 사용하던대로 Spinnaker을 이용했습니다.\nGithub Action vs Jenkins 이 부분 역시 느낀 점 위주로 요약해보겠습니다.\n   Github Action Jenkins     내가 서버를 관리할 필요가 없다. 내가 직접 master을 띄우고, slave를 띄워우고, 관리해줘야한다.   VM이 배치되어 제공되는 데에 좀 시간이 든다. 내 커스텀 이미지를 사용할 수가 없다보니 반복되는 패키지 설치나 환경 설정을 매번 해야해서 좀 느리다. 내가 필요한 Plugin을 설치해놓거나 설정을 입력해 놓으면 매번 빌드할 때 따로 제공할 필요 없다.   Code로 관리가 가능하다! 처음엔 조금 어려울 수 있지만, 알고 나면 쓰기 너무 쉽다. 처음 접한 사람이 사용하기에는 Github Action보다 편리할 수 있지만, 그렇다고 훌륭한 UI/UX는 아닌 듯 하다.   요즘 핫하고, 빠르게 발전 중이다. 구식이다.    비용이 저렴하게 풀리고 있고(퍼블릭의 경우 아마 무조건 무제한 공짜), 원래 지원하지 않던 매뉴얼 트리거가 2020.07부터 제공되기 시작했다는 점, 누구든 오픈소스로 Github Action에서 남들이 사용할 수 있는 action 을 만들 수 있다는 점등을 보고 Github Action이 빠르게 발전 중이라는 생각이 들었습니다.\n그래서 어떤 CI/CD를 자동화하였나요?  ci-cd-pipeline.png \n세로로 길어서 좀 보기 불편하실 수도 있는데, 위의 차트가 저희의 깃헙액션에 대한 차트입니다. 레포지토리마다 조금씩 다른 부분이 있고, 프론트엔드의 경우 파이프라인이 다양했는데, 우선 백엔드의 깃헙액션 및 CI/CD 진행 방식에 대해 요약해보겠습니다. (Chart를 작성한 지가 좀 돼서 설명과 조금 다른 부분이 있을 수도 있습니다.)\n개발 스프린트 진행 시에는 아래와 같이 진행되었습니다. (\u0026lt;상황 설명\u0026gt; =\u0026gt; \u0026lt;Github Action 수행 내용\u0026gt; 형식으로 나했습니다.)\n 개별 Fork 후 Master Branch에 Pull Request =\u0026gt; lint, basic unit test 진행. 통과된 PR만 Merge열 Master에 Merge 혹은 Commit이 Push됨 =\u0026gt; CI Action이 실행됩니다. 개발용 docker registry 에 업로드 해당 registry에 업로드 된 것을 감지하고 Spinnaker가 Experiment 환경에 배포 Experimental 환경을 이용해 Jenkins가 Integration Test를 진행 Integration Test 성공 시에 Dev 환경에 배포.  이후 스프린트 막바지 QA 기간에는 Experiment, Dev 환경이 주로 QA 환경으로 사용되었고, 파이프라인은 다음과 같았습니다.\n 검증이 어느 정도 끝난 커밋에 대해 Git Tag를 {{VERSION}}-rc{{RC_NUMBER}} 형태로 달아 푸시 - Github Release가 생김 =\u0026gt; 도커 이미지 빌드 후 Production Docker Registry에 이미지 업로드 =\u0026gt; tag가 달린 커밋을 기점으로 자동으로 버전 명의 브랜치를 만듦. Production Docker Registry의 업로드를 감지하고 Dev에서 손수 QA 진행 문제가 있을 경우 rc number를 올려서 다시 태그를 달고 업로드 후 재차 QA 문제가 없을 경우 해당 태그를 바탕으로 rc를 지우고 실제 버전으로서 태그를 달아 푸시 =\u0026gt; 다시 업로드 Production은 매뉴얼 배포.  Github Action을 다뤄보면서 느낀 점 처음에는 Github Action에 그렇게 만족을 하지 못했습니다. 초기에는 Manual Trigger가 지원되지 않았던 데다가, 가뜩이나 Github Action을 잘 몰랐기에 한 번 Github Action을 수정하여 테스트 하고싶을 때 마다 커밋을 하나씩 날려야했던 게 불편했고, 브랜치를 따로 만들거나 fork를 떠서 Github Action 테스트 후 해당 workflow만 마스터에 머지하는 방식 등등 다양한 방식을 사용했었는데, 어느 정도 익숙해지고 Github Action에 Manual Trigger도 등장하게 되면서 꽤나 만족도가 높아졌습니다.\nJenkins와 달리 제가 서버를 이용하지 않아도 된다는 점도 맘에 들긴했는데, 종종 Github 서버가 죽는 일이 발생해서 난감했던 적이 있긴합니다.\n하나 재미있었던 점은 Github Action을 이용하면서 저희의 CI/CD 전략이 꽤나 고도화되었는데, 그 과정에서 팀원들과 자유롭게 의사소통하는 과정이 재미있었고, 저 또한 자유롭게 의견을 나눌 수 있었던 경험을 할 수 있었다는 것입니다.\n후에 저희 SpaceONE의 CLI API 클라이언트인 spacectl의 설계에도 Github Action의 구조를 모티브삼았는데 이때에도 Github Action에 대한 지식이 많은 도움이 되었고, 퇴사 후에도 개인적인 Github 활동을 하면서 자유롭게 Github Action을 사용할 수 있었기에 든든한 개발 도구를 얻은 느낌입니다. 코드로 제가 하고싶은 것을 뭐든 정의할 수 있고, 만들어져 있는 작업은 편하게 가져다 쓰면 되기 때문에 빌드나 배포에 관해 재미있는 번뜩이는 아이디어가 있을 때 바로 바로 적용할 수 있고, 실제로 현재의 Github Page도 Github Action을 통해 다양한 트릭을 이용할 수도 있었고, 빌드 후 배포 또한 자동화 되어있습니다!\n","date":"2020-09-04T12:46:54+09:00","image":"https://umi0410.github.io/blog/megazone-cloud/ci-cd-pipeline/ci-cd-pipeline_hu2d78a6f76281dd93e8329c8b5a44cef9_282268_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/megazone-cloud/ci-cd-pipeline/","title":"2. Github Action, Spinnaker을 이용한 CI/CD 파이프라인 구축기"},{"content":"Argo PoC를 진행하게 된 배경 입사 초기부터 Argo에 대해 간간히 이야기를 들어왔습니다. 저희는 원래 Spinnaker을 사용했는데, 쿠버네티스 환경에 좀 더 친화적이라는 Argo를 도입해보는 것은 어떨까에 대한 얘기였는데요. 어느정도 개발 인프라 구축이 완료되고 한가해지자 잠깐이나마 Argo를 사용해볼 수 있었습니다. 결과적으로 Argo 도입은 적합하지 않다고 판단이 되었고 따라서 충분히 써볼 수는 없었기에 기록용으로만 간단히 적어보겠습니다.\nArgo Project 구조  Argo가 워낙 빠르게 변화하는 서비스다보니 지금은 구조가 많이 변경되었을 수도 있습니다!\n  argo_1 \n argo_2 \n간편하게 사용할 수 있는 순서는 argo, argo(workflow), argo event 순이라고 생각되는데, 실질적인 파이프라인 구축은 argo event, argo (workflow), argo 순으로 이뤄지는 셈이라 후자의 순서에 맞춰 설명해보겠습니다.\nArgo Event (Official site)  argo-events-top-level.png \n셋 중 가장 베이비 프로젝트입니다. 빠르게 개발되고 있고, 변화하고있기에 제가 argo project들을 만났던 시절과 많이 구조가 달라져있습니다. 간단하게 말하자면 이벤트를 감지하여 어떤 작업을 수행할 수 있습니다. argo-events의 공식 홈페이지에서 제공되는 위의 이미지와 같이 다양한 이벤트 소스를 이용해 다양한 이벤트를 트리거할 수 있습니다. 예를 들어 AWS SQS에 메시지가 생기면, 그 메시지를 가져와서 어떤 작업을 수행할 수 있습니다. 혹은 웹훅 서버를 돌려서 웹훅 요청이 오면, 어떤 작업을 트리거할 수 있습니다.\n제가 구상한 파이프라인에서는 이벤트를 감지하는 역할로서 Argo Events를 앞에 두고, Argo Events가 Workflow를 생성는 작업을 트리거하는 방식으로 CI/CD에 이용할 수 있습니다.\nArgo, Argo Workflow (Official site) 셋 중 가장 오래된 프로젝트이고, 별도 많습니다. Argo 라는 이름을 가졌고 실질적으로는 Workflow 관련 프로젝트입니다. Workflow란 컨테이너를 이용해 진행되는 일련의 step들을 정의하는 CRD(K8s Custom Resource Definition 입니다.)\n처음엔 \u0026lsquo;굳이 Workflow가 필요할까\u0026rsquo; 싶었지만, 쿠버네티스 상에서 일련의 Job을 연속적으로 수행할 수 있는 방법이 현재까지는 없는 것으로 알고 있습니다. K8s Job의 Container에 대한 InitContainer을 지정함으로써 한 Job에 대한 두 Container의 순서를 명시할 수는 있지만, Workflow처럼 다양하게 일련의 Job을 연속적으로 수행하기는 힘든 것으로 알고 있습니다.\nArgoCD (Official site) ArgoCD만 보면 가장 간단하게 실제 CD에 적용할 수 있는 프로젝트 중 하나가 아닐까싶습니다. ArgoCD 자체에 대한 설정, 소스가 될 Repository에 대한 설정, Project, Application에 대한 설정 등등 모든 것이 Code IaC에 특화된 독특한 프로젝트입니다.\nUI가 직관적이고 알아보기 쉽지만, 반대로 대규모 애플리케이션이 될 경우 너무도 배포 현황을 보여주는 맵이 커지기 때문에 알아보기 쉽지 않을 수 있을 것 같습니다. GitOps(\u0026ldquo;GitOps의 핵심은 Git 저장소에 저장된 쿠버네티스 매니페스트 같은 파일을 이용하여, 배포를 선언적으로 한다는 것입니다. 출처\u0026quot;) 라는 말에 아마 빠지지 않고 등장하는 CD 도구인듯합니다. Git에 올라가는 Manifest가 그대로 K8s 클러스터에 적용됩니다. 마치 Github Page나 S3(웹호스팅 설정이 된 S3 Bucket)에 올린 파일들이 바로 하나의 웹 애플리케이션처럼 동작하는 것과 비슷한 느낌입니다. 간단하게 사용하기에는 ArgoCD가 참 좋아보였지만, 실제로 업무적으로 사용하지는 못하겠다고 판단한 이유가 몇 가지 있습니다.\n 애플리케이션 하나에 대한 배포는 쉽지만, 연속적인 배포나 다양한 배포 파이프라인을 구성하기는 어려웠다. Github Push 시에 webhook을 설정해서 ArgoCD의 배포를 트리거하도록 했는데 이 부분이 최적화가 덜 되었는지 어떤 repository든 하나의 ArgoCD 환경에서는 Git platform 당 하나의 secret만 설정이 가능했고, 한 repository가 push되면 다른 repository도 배포가 다 같이 이루어져버렸습니다. (이 부분은 패치됐을 수도 있습니다.) 소규모 애플리케이션은 배포 상황을 한 눈에 보기 쉽지만, K8s Object가 많아지면 많아질수록 점점 하나하나 눈에 보이지 않고, 잡다한 Object들(ConfigMap, Secret, \u0026hellip;)로 인해 인식하기 힘듭니다.  Argo Project를 조합한 CI/CD 파이프라인 구축기 파이프라인 수행 과정  (Argo Event) Github Push를 감지하고 Argo Workflow CRD를 생성 Argo workflow를 통해 다양한 step들을 이용하기 위해 바로 ArgoCD가 Github Push를 받지 않고, Argo Event가 받도록 했습니다. Argo Workflow의 step들 수행  Slack에 workflow 시작 알림 Experiment 환경에 배포 (Argo CD 이용) Interation Test 실행 트리거 (Jenkins 이용) Integration Test 결과가 성공이면 Dev 환경에 배포 (Argo CD 이용) 어느 step에서든 실패 시 Slack에 실패 알림    이 정도 Pipeline을 짜면 사실 사용할 정도는 될 수 있겠지만, \u0026ldquo;일\u0026quot;로서 본다면 굳이 현재 사용 중인 Spinnaker에 비해 장점이 뚜렷하게 느껴지지 않는 Argo를 위해 파이프라인을 이전하기에는 역부족이라는 판단이 들었습니다. 예를 들어 과정 목록에서는 간단히 묘사했지만, Spinnaker에서는 UI에서 알아서 처리되던 부분들을 하나 하나 webhook을 걸어주거나, 따로 bash script를 짜야하는 경우들이 많았습니다.\nArgo Project들에 대한 PoC를 진행하며 느낀 점 개인적으로는 많이 애정이 갔던 프로젝트들이고 신기했던 배포방식에 대한 소개였으며, 이 프로젝트들을 공부해보면서 다양한 아키텍쳐에 대해 경험해볼 수 있었던 것 같지만, 현실적인 벽에 부딪혀 도입을 할 수 없었던 아쉬움 컸습니다. 마치 좋아하는 분야가 있었지만, 현실의 벽에 부딪혀 꿈을 접고, 어떠한 현실적인 진로 나아가는 것과 같았달까요?\n그리고 개인적인 아쉬움은 위의 차트를 손수 그리며 PoC 문서를 작성했었지만, 결국은 PoC를 진행한 담당자인 저조차 \u0026ldquo;음\u0026hellip; 실사용은 힘들 것 같은데요\u0026quot;라는 의견을 냈던 터라 바쁜 일정 속에서 reject할 개념 증명 리뷰에 많은 시간을 할애할 수 없었기에, 팀원들과 이 내용을 공유할 수 없었던 점에 조금 아쉬움이 남습니다..! 만약 정말 Argo를 사용하는 방식이 뛰어난 방식이었다면, 저도 적극 추천하며 함께 리뷰해주길 기대했겠지만, 현실적으로 저희 팀의 배포 방식과는 맞지 않았다고 판단해서 그랬던 것이기에 괜찮습니다~!\n","date":"2020-09-04T12:46:54+09:00","image":"https://umi0410.github.io/blog/megazone-cloud/argo-poc/argo-events-top-level_hu911ca173f7b6efecb06ab37a91a70151_401443_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/megazone-cloud/argo-poc/","title":"3. Argo Project들에 대한 PoC(개념 증명) 진행"},{"content":" spaceone-helm-preivew.png \nSpaceONE Helm Chart란? spaceone-helm 은 저희 CloudOne 팀이 개발하는 서비스인 SpaceONE을 helm chart를 이용해 패키지화하는 프로젝트입니다. 원래의 저희 환경은 MicroService들을 개별 배포하고있었지만 오픈소스로 개발되는 저희 서비스를 저희 팀원들 뿐만아니라 다른 개발자들이 쉽게 개발할 수 있고, SpaceONE을 모르던 사용자들도 쉽게 SpaceONE을 구축해볼 수 있도록 하기 위해 패키지화도 진행하게되었습니다.\n일반적으로 만들어진 Helm Chart를 이용해보기만했지 직접 Chart를 만드는 것은 처음 해본 일이기도 했고, Chart를 개발하면서 새로운 환경에 저희 서비스를 배포해보다보니 삽질하며 고생도 꽤 했고, 무엇보다 프로젝트의 시작부터 퇴사 전까지의 작업들을 거의 제가 도맡아한 프로젝트였기에 개인적으로 애정이 많이 갔습니다 ^_^! 그리고 저도 이제는 저희 Chart를 이용해 SpaceONE을 구축형으로 손쉽게 이용할 수 있었습니다.\nspaceone-helm 설계 일단은 심플하게 사이드카 없이 저희의 마이크로서비스들만을 배포하고, 그 외에 필요한 서비스들도 최대한 Cloud Service에 의존하지 않고 그때 그때 손쉽게 서비스를 내렸다 올렸다 할 수 있도록 K8s 클러스터 위에 함께 배포하는 형식으로 Chart를 설계했습니다.\n무엇보다 chart 개발의 목적은 아래 두 가지 사항이 컸기 때문에, minikube 혹은 EKS 띄운 마이크로서비스들과 로컬에서 통신이 가능하도록 해야했고, 사용하기 편리한 구조를 만들기 위해 고심했습니다.\n 팀원이 아닌 개발자들도 자신의 부가적인 마이크로서비스를 로컬에 띄워 개발할 수 있도록 지원.  로컬에서도 쿠버네티스 클러스터 내의 서비스에 접속이 가능해야하고, 경우에 따라 클러스터에서도 본인의 로컬 서비스로 접속을 할 수 있어야함.   오픈소스로서 임의의 사용자가 서비스를 구축해보고자 시도할 때 손 쉽게 구축할 수 있도록 지원.  마치 유저들의 사용자 경험을 중요시해서 디자인, 기획을 하듯 Chart의 사용자들이 직관적이고 편리하게 구축할 수 있도록 Configuration values(values.yaml in Helm Chart)를 설계함.    spaceone-helm chart 구조 templates/ ├── backend │ ├── config │ │ ├── config-conf.yml │ │ ├── config-deployment.yml │ │ └── config-svc.yml │ ├── identity │ │ ├── identity-conf.yml │ │ ├── identity-deployment.yml │ │ └── identity-svc.yml │ ├── inventory │ │ ├── inventory-conf.yml │ │ ├── inventory-deployment.yml │ │ └── inventory-svc.yml │ ├── inventory-scheduler │ │ ├── inventory-scheduler-conf.yml │ │ ├── inventory-scheduler-deployment.yml │ │ └── inventory-scheduler-svc.yml │ ├── inventory-worker │ │ ├── inventory-worker-conf.yml │ │ ├── inventory-worker-deployment.yml │ │ └── inventory-worker-svc.yml #... Backend 생략 ├── consul, mongo, redis # 디테일한 파일구조는 생략 ├── frontend │ ├── console # 디테일한 파일구조는 생략 │ └── console-api # 디테일한 파일구조는 생략 ├── ingress │ └── ingress.yaml ├── initializer │ ├── initialize-spaceone-conf.yml │ ├── initialize-spaceone-job.yml │ ├── spacectl-apply-conf.yml │ └── spacectl-conf.yml └── supervisor └── supervisor ├── supervisor-conf.yml ├── supervisor-deployment.yml └── supervisor-roles.yml 저희 spaceone-helm chart는 꽤나 구조가 간단한 편은 아니라고 생각됩니다. 웹페이지 환경상 너무 부수적인 부분은 tree에서 생략하기도 했습니다. 꽤나 복잡한 구조를 가졌기에 패키지화하는 부분이 쉽지 않았지만, 그런 과정 속에서 많이 트러블 슈팅을 경험하고 성장할 수 있었던 것 같습니다.\n그리고 반대로 생각하면, 서비스가 패키지화하기도 쉽지 않을만큼 복잡한 구조를 가졌다면, 당연히 손수 배포하는 것은 그것보다 몇 배는 어려울 것이므로 누군가가 저희 서비스를 구축형으로 이용해주기 위해서는 패키지화가 필수라는 생각이 들었습니다.\n마이크로서비스들의 버전 관리를 시작 Helm Chart를 개발하고, CI가 고도화되기 전까지는 단순히 팀 내에서 개발을 진행하면서 개발환경에 대한 배포는 latest tag만을 이용해 자동으로 진행하고, QA를 진행하면서 커밋을 멈추고, 그 시점에 빌드된 Docker image의 Tag를 바탕으로 상용 환경에도 배포를 하곤했습니다. 하지만, Chart에서도 tag를 latest로 유지하거나 가독성이 좋지 않은 임의의 tag를 이용해 이미지를 제공하기는 힘들었고, 맞는 방향이 아니라고 생각했습니다.\n따라서 저희는 꼭 Helm을 통한 패키지 뿐만 아니라 롤백에 대한 안정성, 버전 간의 Update 내역 관리 등을 위해 {{MAJOR}}.{{MINOR}}.{{SPRINT_NUMBER}}-{{EXTRA_TAGS}} 형태를 통해 버전을 관리하고자했습니다.\nHelm Chart를 통해 패키지화하며 느낀 점 Helm Chart를 통해 저희 서비스를 패키지화하기 전까지는 마이크로서비스 형태로 관리되는 서비스를 새로운 환경에 완전하게 구축한다는 것은 쉽지 않았습니다. Container라는 것이 ReadOnly 레이어로 이미 구성이 완료된 이미지를 바탕으로 생성되기 때문에 언제 어디서든 구동이 가능하다는 것이 장점이겠지만, 현실적으로는 환경에 따라 이것 저것 설정해줘야하는 것이 있었기때문입니다.\n이러한 난관들은 어떠한 변수처리와 자동화를 통해 해결할 수 있을텐데, 그러한 작업을 해준 녀석이 바로 Helm이었고, 그때 그때 설정을 바꿈으로써 커스터마이징할 수 있다는 점이 알고는 있었습니다만 막상 저희 서비스에 도입해보니 꽤나 만족스러웠습니다.\n일화로 사내 네트워크가 막힌 상황에서도 받아놓은 이미지들만 있으면 네트워크 접속을 할 필요 없이 minikube와 로컬 서버를 이용해 minikube의 클러스터와 통신하면서 개발을 진행할 수도 있었습니다.\n누군가 제가 만든 서비스를 사용해준다는 것은 참 뿌듯한 일이었고, 앞으로도 저희 helm chart가 잘 발전되어 더욱 더 편리하게 구축할 수 있는 형태로 제공될 수 있기를 기대해봅니다!\n","date":"2020-09-04T12:46:54+09:00","permalink":"https://umi0410.github.io/blog/megazone-cloud/spaceone-helm/","title":"4. SpaceONE Helm Chart 개발"},{"content":"spacectl이란? 소개 spacectl 은 저희팀이 개발하는 서비스인 SpaceONE의 gRPC API request를 CLI로 손쉽게 수행할 수 있도록 해주는 도구입니다. 파이썬을 통해 개발했고 Click 이라는 모듈로 CLI 환경을 손쉽게 사용할 수 있었고, Jinja2를 통해 상세한 Manifest 들에서 변수 치환, 분기 등을 수행할 수 있었습니다.\n사용 예시 A simple example 간단하게 spacectl이 어떤 식으로 이용되는 도구인지 예시를 보여드리겠습니다. 아래의 커맨드를 이용해 손쉽게 SpaceONE의 다양한 마이크로서비스들의 API를 이용할 수 있습니다.\n$ spacectl list domain domain_id | name | state | plugin_id ... domain-abc123abc | umi0410| ENABLED | ... $ spacectl list server -p domain_id=domain-abc123abc server_id | name | provider ... server-abc123abc | foo | aws ... apply command spacectl apply command는 kubectl의 apply와 유사하게 없으면 만들고, 있으면 업데이트하고 혹은 단순히 어떤 API를 Execute하는 Task들의 플로우를 관리해주는 커맨드 입니다.\n 아쉽게도 퇴사 전에 마무리를 짓지는 못했습니다. ㅜ.ㅜ 퇴사 전까지 진행한 작업은 일부 리소스에 대한 CRU(create, read, update), 대부분의 리소스에 대한 Excute(execute할 API를 설정)까지입니다.\n # main.yamlimport:- mongo.yaml# 개별 yaml file에서는 terraform/ansible과 같이 수행할 Task들을 정의- root_domain.yaml- repository.yamlvar:domain_name:rootdomain_owner:adminadmin_username:adminadmin_password:admin$ spacectl apply main.yaml설계를 하며 느낀 점 이 프로젝트에 대한 실제 개발 업무 이전에는 꽤나 설계 업무가 많았습니다. 저는 그 동안은 혼자 주로 개발을 해왔고, 실행력 좋게 시작은 하지만 설계는 충분하지 않은 채 성급하게 실행에 옮겼던 경험이 많습니다. 또한 학생이었고, 개발 경력이 길지 않았기에 사실상 \u0026ldquo;개발 = 그때 그때 새로운 내용 공부\u0026quot;와 같은 느낌이었기에 애초에 설계를 하려해도 \u0026lsquo;뭐가 필요하고 뭐가 가능할 것이고 뭐가 힘들 것인가\u0026rsquo; 를 판단하기 어려웠습니다.\n하지만 팀원들과 함께 개발하면서 밥 먹을 때, 회의 할 때 틈틈히 설계 방식과 요령에 대해 상의했고, 처음으로 설계를 어느 정도 굳힌 뒤 개발에 들어들어갔던 경험이었습니다.\n인턴 기간 막바지에 이 설계에 참여하게 된 것은 정말 값진 경험이었다고 생각했습니다. 사실 단순히 데브옵스로서 일할 때는 남들과 의사소통할 일이 그리 많진 않았는데, 이 설계를 맡게 되면서 많은 회의와 대화를 하게되었습니다. 선배 개발자분들과 설계에 대해 잦은 회의를 하면서 제가 어떤 개발자가 되고싶은지 직접 느낄 수 있었던 것 같습니다. 그 배경에는 두 가지 충격이 있었습니다.\n \u0026lsquo;내가 남의 생각을 잘 읽는 편은 아니었나보군\u0026hellip;?' \u0026lsquo;선배 개발자분은 내가 개판으로 설명해도 어떻게 귀신같이 나보다 내 생각을 잘 읽으시지?' =\u0026gt; 마치 축구할 때 노련한 축구선수와 함께 뛰면서 저 행동을 귀신같이 예측하고서는 너무나도 잘 밀어주는 느낌을 받았습니다\u0026hellip;  제가 평소에 말을 잘하는 편이라고 생각했는데, 남의 생각을 이해하고 읽어내는 능력은 그리 뛰어나지많은 않구나라는 생각을 하게됐습니다. 설계 내용이 꽤나 추상적으로 구두로 진행되었기에 그랬을 수도 있겠지만, 큰 충격은 선배 개발자분의 노련함이었습니다.\n후에 누군가 어떤 개발자가 되고싶냐, 협업할 때 어떤 노하우가 있느냐 이런 내용을 물어보면 자신있게 남의 생각을 잘 이해하고, 알아주는 사람과 관련해 대답할 수 있는 사람이 될 수 있었으면 좋겠습니다.\n또한 설계 깊게 진행하기 전에 침착하게 자료 조사를 잘 해야한다는 것을 느꼈습니다.\n하나 일화로 원래는 설정 파일에서 ${{ tasks.umi0410.output }} 이런 식의 변수를 이용한 설정을 치환한 뒤 API를 수행해야할 때, 아마 template 언어들을 제가 원하는 대로 사용하기 힘들 것이라 생각하고, 하나하나 함수와 클래스를 만들어가곤했는데, 개발이 거의 완료되어갈쯤 Jinja2의 사용법을 다시 읽다보니 spacectl에 Jinja2를 적절히 사용할 수 있을 것 같았고, 정신이 번뜩 들어 몇 시간만에 수작업으로 짠 코드들을 덜어내고 Jinja2를 이용해 좀 더 깔끔하게 변수 치환 및 추가로 Jinja2의 built-in filter들을 이용할 수 있었습니다!\n개발하면서 느낀 점 역시 개발은 남이 만든 패키지를 잘 사용해야한다는 것을 느꼈고, 그냥 복사 붙여넣기만 잘하면 된다는 의미가 아니라, 그런 것들을 빠르게 가져와서 적용시키고 부분 부분 커스터마이징하기 위해서는 기본기가 탄탄해야한다고 느꼈습니다. 물론 수작업으로 만드는 것도 좋을 수 있겠지만, 다양한 엣지케이스가 존재할 수 있고, 그 모든 작업들을 문서로 상세히 설명하는 것이 아니라면, 제작자인 제가 아닌 누군가가 그 기능을 이용하기는 힘들 것입니다. Jinja2를 이용한 템플릿 기능 제공이 이와 관련된 경험이 될 수 있겠습니다.\n또한 남의 코드를 자세히 읽어보는 게 처음이었는데, 덕분에 파이썬 프로젝트를 수행할 때 어떤 식으로 디렉토리 스트럭쳐를 짜면 좋을 지 생각해볼 수 있었던 계기였던 것 같습니다. 제가 설계하고, 개발한 내용을 짧게나마 발표한 뒤 리뷰를 받고 수정을 하면 한층 더 코드의 구조와 사용이 간결하고 직관적으로 보인다는 느낌을 받을 수 있었습니다. 이렇게 다양한 가르침을 주신 저희 팀의 선배 개발자분들께 항상 감사드립니다.\nMegazone CloudOne 팀의 DevOps 인턴으로서 근무했던 내용에 대한 후기가 거의 끝났습니다. 끝으로 인턴 활동에 대한 종합적인 느낀점을 이어서 보시거나 다시 목차를 보고싶으신 분은 여기를 클릭해주세요.\n","date":"2020-09-04T12:46:54+09:00","permalink":"https://umi0410.github.io/blog/megazone-cloud/spacectl/","title":"5. SpaceONE CLI Client인 spacectl 설계 및 개발"},{"content":"🐶 시작하며  본 게시글은 AWS 대학생 유저그룹인 AUSG의 활동 중 하나로서 본인(박진수)이 작성한 게시물을 포워딩한 것입니다.\n  preview.png \n데브옵스 인턴으로 근무한 지가 벌써 두 달이 되어갑니다. 이것 저것 배운 것이 많았던 시간이었는데, 그 중 꽤나 삽질을 했던 Kubernetes 와 ELB를 이용하는 부분에 대해 정리를 해볼까합니다. jenkins, spinnaker, argo, terraform, ansible, github action, \u0026hellip; 등등 다양한 내용을 경험할 수 있던 시간이었지만, 그 중 kubernetes에서 무슨 작업을 하던 빼놓을 수 없으면서 어딘가 깔끔히 그 흐름이 정리된 곳을 보기 힘들었던 service를 ELB에 연결하기에 대한 내용을 정리해보겠습니다.\n본 포스트는 EKS를 통해 K8s를 이용할 때를 기준으로 설명합니다.\n💁🏻‍♂️ EKS 에서 ELB를 사용해 서비스를 노출킬 때 유의사항들  🧐 : \u0026quot; ELB, NLB, ALB 대체 뭐가 다른 거야..?ㅜㅜ 쿠버네티스를 쓸 때는 어떻게 얘네를 지정하는 거지..? kubectl expose deploy {{deployment_name}} --type=LoadBalancer 하면 그냥 작동은 하던데\u0026hellip;\u0026quot;\n EKS에서 주로 사용하는 ELB는 L4의 NLB와 L7의 ALB 입니다. ALB가 L7에 대한 좀 더 다양한 설정이 가능하기 때문에 조건이 많기도 하고, AWS의 ALB만을 위한 alb-ingress-controller라는 녀석이 직접 Ingress의 설정들을 관리해주기 때문에 설정할 수 있는 옵션도 많습니다. 좋게 보면 많은 설정을 할 수 있고, 나쁘게 보면 초보자에겐 귀찮을 수 있습니다. NLB는 비교적 설정이 적고 따라서 설정해줄 수 있는 항목도 적습니다.\n쿠버네티스에서 다양한 작업을 하면서 다양한 controller을 접하게 되고, 그렇게 될 수록 annotation으로 많은 설정을 하게 됩니다. k8s를 처음 접할 때에는 annotation에 대한 정의로서 아래와 같은 문장을 접할 수 있고, 마치 기능과 크게 상관이 없을 것처럼 느껴지기도 하지만 사실 EKS를 비롯한 여러 서비스에서는 annotation을 이용해 중요한 설정 등을 기입할 수 있기 때문에 잘 설정해주어야합니다. ELB또한 모든 설정이 annotation으로 동작한다.\n \u0026quot; Label을 사용하여 오브젝트를 선택하고, 특정 조건을 만족하는 오브젝트 컬렉션을 찾을 수 있다. 반면에, annotation은 오브젝트를 식별하고 선택하는데 사용되지 않는다. 어노테이션의 메타데이터는 작거나 크고, 구조적이거나 구조적이지 않을 수 있으며, 레이블에서 허용되지 않는 문자를 포함할 수 있다.\u0026quot;\n ⚠️ ALB를 사용할 때 유의할 점  어떤 옵션들이 있고, 기본적으로는 어떻게 설정되는 지에 대한 이해가 있어야 오류 과정을 추적하기 쉬우므로 기본적으로 ALB를 AWS Console에서 사용해본 뒤에 설정할 것을 추천합니다.\n  alb ingress controller가 생성할 ALB가 사용할 서브넷을 discover하기 위해서는 올바른 태그가 달린 subnet이 존재해야한다. node 혹은 alb ingress controller에 연결된 service account가 alb를 제어하기 위한 iam permission이 부여되어야한다. internet facing한 alb를 만들지 internal한 alb를 만들지 고민해봐야한다. alb ingress controller의 log를 통해 작업에 대한 log를 볼 수 있다.  ⚠️ NLB, CLB를 사용할 때 유의할 점 https://kubernetes.io/ko/docs/concepts/services-networking/service/#aws-nlb-support\nhttps://docs.aws.amazon.com/ko_kr/eks/latest/userguide/load-balancing.html\n NLB, CLB가 사용할 서브넷을 설정하기 위해서는 올바른 태그가 달린 subnet이 존재해야한다. 어느 부분에선가 NLB, CLB를 제어하기 위한 iam permission이 부여되어야한다. (어느 부분인지 확실히는 모르겠음. 따로 설정안해도 동작하는 것을 보아 worker node가 갖는 iam role에 permission이 붙어있을 것으로 예상됨)  🌎 ALB를 사용해 서비스를 노출시키는 방법  😊 ALB는 K8s에 친숙하지 않으신 분들께는 다소 진입장벽이 있을 수 있습니다. 그냥 서비스를 노출시킬 때는 굳이 사용할 필요 없는 Ingress 라는 오브젝트도 관리해야하고, alb-ingress-contoller라는 녀석도 배포해야하며 설정이 다양하기 때문이죠! 💦\n K8s에서 EKS를 사용해 ALB를 이용하고싶은 경우 alb-ingress-controller을 배포한 뒤, Ingress를 통해 사용할 alb에 대한 rule을 설정을 해주어야합니다.\nhttps://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/controller/setup/ 의 내용을 버릴 부분이 하나도 없습니다. 위 링크를 통해 alb-ingress-controller에 대한 개념을 잡고 배포해봅니다. alb-ingress-controller.yaml의 인자를 적절히 수정해주어야합니다.\nALB가 아닌 k8s cluster 상에서 L7 LoadBalancer를 이용하는 경우에는 nginx ingress controller등을 이용하며 nginx 에 적용할 rule을 Ingress라는 K8s Object를 통해 설정합니다. ingress controller의 설정에서 자신의 class name을 적어주고, Ingress에서는 어떤 class name의 ingress controller에서 자신(Ingress이자 Rule)을 적용하도록 할 지를 annotation을 통해 설정하거나 ingressClassName이라는 필드를 통해 설정합니다.(ingress 설정에 대한 참고 - https://kubernetes.io/ko/docs/concepts/services-networking/ingress/#인그레스-클래스)\n이와 같은 경우에는 ingress controller가 직접 ingress에 명시된 rule을 이용했지만, alb-ingress-controller의 경우는 alb-ingress-controller가 nginx-ingress-controller처럼 직접 웹서버의 역할을 하는 것이 아닌, ingress에 명시된 rule을 이용하는 ALB를 생성하고 관리하는 역할을 한다는 것입니다. 이 부분이 처음에는 다소 헷갈리게 느껴질 수 있기에 길게 서술해보았습니다.\n실제로 ingress를 생성한 뒤 앞에서 배포한 alb-ingress-controller의 log를 보면 alb를 관리하기위한 여러 작업을 수행중인 모습을 볼 수 있습니다.\n그럼 이제 실제로 alb ingress controller을 통해 alb를 이용해보겠습니다.\nALB를 원활히 제어하기 위한 permission 부여  ALB iam 정책 참고  https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/alb-ingress.html https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.2/docs/examples/iam-policy.json    ALB를 제어하기 위해서는 aws의 리소스에 대한 어떠한 permission이 필요합니다. node에 부여할 수도 있고, IAM User에 부여한 뒤 alb ingress controller의 설정에서 해당 IAM User의 Key를 부여할 수도 있고, Service account와 IAM Role을 OIDC(OpenID Connect를 이용해 Service account와 IAM Role을 연결시키는 작업)를 이용해 엮은 뒤, alb ingress controller pod에 해당 Service Account를 부여할 수도 있지만 뒤의 방법들은 좀 튜토리얼치고 투머치한 감이 있기때문에, 간단히 node에 Permission을 부여하도록하겠습니다.\n iam.png \nworker node들이 이용하는 IAM Role에 Policy를 추가한 모습.\nalb가 사용할 subnet에 적절한 태그 달기 https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/alb-ingress.html 에 나와있듯이 ELB가 이용하는 서브넷을 자동으로 설정되도록 하기 위해서는 사용하고자 하는 서브넷에 아래와 같은 태그들을 달아주어야한다.\nkubernetes.io/cluster/\u0026lt;cluster-name\u0026gt; = shared | owned # Required kubernetes.io/role/internal-elb = 1 | \u0026#34;\u0026#34; # Optional, for internal alb kubernetes.io/role/elb = 1 | \u0026#34;\u0026#34; # Optional, for internet-facing alb a | b와 같은 표현은 a 나 b중 한 값을 가져야한다는 의미로 표현한 것입니다.\n tag.png \ninternet facing ALB만 이용할 것이기 때문에 kubernetes.io/role/internal-elb tag는 생략하고 태그를 달아준 모습.\nsubnet에 ALB를 사용하기 위한 태그를 제대로 달아주지 않을 경우 alb-ingress-controller 에서 아래와 같은 로그를 보게 됩니다. ALB가 생성되지도 않습니다.\ncontroller.go:217] kubebuilder/controller \u0026#34;msg\u0026#34;=\u0026#34;Reconciler error\u0026#34; \u0026#34;error\u0026#34;=\u0026#34;failed to build LoadBalancer configuration due to failed to resolve 2 qualified subnet for ALB. Subnets must contains these tags: \u0026#39;kubernetes.io/cluster/umi-dev\u0026#39;: [\u0026#39;shared\u0026#39; or \u0026#39;owned\u0026#39;] and \u0026#39;kubernetes.io/role/internal-elb\u0026#39;: [\u0026#39;\u0026#39; or \u0026#39;1\u0026#39;] alb ingress controller 배포하기.  슬슬 읽기 귀찮아질 타이밍입니다. \u0026lsquo;요놈이 IAM policy도 만들고, 서브넷에 엄한 태그를 달더니 이제는 하,,, 뭘 또 배포하라고 하는구나 아이고 내 눈아,,,\u0026rsquo; 싶겠지만, 좀 더 힘을 내어봅시다 🍻\n https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/controller/setup/ 를 참고하여 Deployment 내의 container의 args를 자신의 상황에 맞게 수정한 뒤 배포해줍니다.\n... args: - --ingress-class=alb # ingress의 annotation에 명시할 class name - --cluster-name=umi-dev # eks cluster name - --aws-region=ap-northeast-2 - --aws-api-debug=true 저는 위와 같은 식으로 설정해주었고, 잘 배포되었는지 확인해봅니다.\n$ kubectl get po -A | grep alb kube-system alb-ingress-controller-594f84b465-q4qjb 1/1 Running 0 106m 노출시킬 서비스 배포하기 간단하게 Nginx를 배포해보록하겠습니다.\napiVersion: v1 kind: Service metadata: name: ingress-test spec: selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80 nodePort: 30010 type: NodePort --- apiVersion: apps/v1 kind: Deployment metadata: name: ingress-test labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 ALB는 기본적으로 node의 Port를 AWS 상의 target group으로서 이용하기 때문에, ingress를 통해 노출시켜줄 서비스는 적어도 NodePort 타입으로 노출되어있어야 ALB ingress controller가 해당 서비스를 노출시킬 수 있습니다.(target type을 기본값인 instance가 아니라 IP로 설정하면, Pod의 IP로 트래픽이 흘러가게 할 수는 있습니다.)\n작동방식을 설명해보자면 ingress 는 service name과 service port를 설정으로 받습니다. alb-ingress-controller는 그러면 해당 service name, service port와 연결된 NodePort를 찾아서 ALB의 target group으로 등록시킵니다.\nIngress 배포하기 apiVersion: extensions/v1beta1 kind: Ingress metadata: name: \u0026#34;ingress\u0026#34; annotations: kubernetes.io/ingress.class: alb # the value we set in alb-ingress-controller alb.ingress.kubernetes.io/scheme: internet-facing spec: rules: - http: paths: - path: /* backend: serviceName: \u0026#34;ingress-test\u0026#34; servicePort: 80 https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/ingress/annotation/ 을 참고하여 Ingress를 작성해줍니다. [kubernetes.io/ingress.class는](http://kubernetes.io/ingress.class는) alb-ingress-controller에서 설정한 ingress.class를 적어주고, alb.ingress.kubernetes.io/scheme는 용도에 따라 internet-facing 혹은 internal을 적어줍니다. ingress 를 생성하기 전에 kubectl logs -f {alb-ingress-controller pod name}을 한 창에 띄워놓으면 ALB 생성 관련 로그를 쭈루룩 볼 수 있습니다.\n elb.png \n잘 설정되었다면 위와 같이 ALB가 생성될 것 입니다.\n$ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE ingress * bf1e76be-default-ingress-e8c7-1351183883.ap-northeast-2.elb.amazonaws.com 80 30s 인증서 설정을 통해 HTTPS 까지?!  ❤️ AWS Certificate Manager와 Ingress에 대한 annotation을 이용해 간단하게 HTTPS를 이용할 수도 있습니다! 직접하려면 도메인 소유 인증과 인증서, 비밀키 등을 모두 관리해야했는데 말이지요! 🐥\n  cert.png \n이런식으로 AWS의 Certificate Manager을 통해 발급받은 인증서가 있다면 이를 alb 에서 ingress에 annotation을 설정함으로써 사용할 수 있습니다.\nIngress의 annotation에 HTTPS및 인증서 관련 설정 추가해주기\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: \u0026#34;ingress\u0026#34; annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTP\u0026#34;: 80}, {\u0026#34;HTTPS\u0026#34;: 443}]\u0026#39; alb.ingress.kubernetes.io/actions.redirect-to-https: \u0026gt; {\u0026#34;Type\u0026#34;:\u0026#34;redirect\u0026#34;,\u0026#34;RedirectConfig\u0026#34;:{\u0026#34;Port\u0026#34;:\u0026#34;443\u0026#34;,\u0026#34;Protocol\u0026#34;:\u0026#34;HTTPS\u0026#34;,\u0026#34;StatusCode\u0026#34;:\u0026#34;HTTP_302\u0026#34;}} alb.ingress.kubernetes.io/certificate-arn: {Certificate Manger의 인증서 arn} ... http redirect 및 actions에 대한 내용은 저의 애교입니다. 궁금하신 분들은 한 번 적용해보시거나 알아보시면 어렵지 않게 알아내실 수 있을 겁니다! 😆\nRoute53에 ALB 추가해주기\nEKS에서 ALB를 사용하는 등의 작업을 하시는 분은 어느 정도 aws에 대한 이해가 있으리라 생각하고, ALB를 Route53을 통해 레코드로 추가하는 작업에 대한 설명은 생략하겠습니다.\n cert2.png \nalb-ingress-controller로 AWS Certificate Manager의 인증서까지 사용한 모습.\n🌎 NLB를 사용해 서비스를 노출시키는 방법 CLB는 Deprecate 대상이라고 들었기도 하고, 굳이 써본 적이 없어 NLB로만 설명합니다. NLB는 ALB에 비해 사용이 간단합니다.\nIngress와 alb-ingress-controller를 사용했던 ALB와 달리 NLB는 서비스를 직접 노출시킵니다. 주로 Nginx Ingress Controller을 NLB에 연결해서 사용했던 기억이 납니다. NLB는 L4 LB로, Nginx를 주로 L7 LB로 사용하는 경우 이렇게 NLB를 사용합니다. ALB와 Nginx 모두 L7 LB로서 역할을 하기때문에 굳이 ALB를 사용할 필요가 없는 경우가 많았습니다.\nNLB를 통해 서비스를 노출시키기 위해선 annotaion중에서도 [service.beta.kubernetes.io](http://service.beta.kubernetes.io/)...형태의 annotation을 이용합니다. 사실 실제로 실무해서 사용해본 annotation은 거의 [service.beta.kubernetes.io/aws-load-balancer-type:](http://service.beta.kubernetes.io/aws-load-balancer-type:) \u0026quot;nlb\u0026quot; 뿐입니다. (이를 사용하지 않을 경우 디폴트가 CLB이기 때문에\u0026hellip;)\nNLB가 사용할 subnet에 적절한 태그 달기 ALB를 사용했을 때와 마찬가지로 NLB가 사용할 서브넷에 필수 태그를 달아줍니다. 기억이 안난 다면 글의 상단 ALB 파트를 참고!\nNLB로 노출할 서비스 생성하기 apiVersion: v1 kind: Service metadata: name: nginx-nlb annotations: service.beta.kubernetes.io/aws-load-balancer-type: \u0026#34;nlb\u0026#34; spec: selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80 nodePort: 30011 type: LoadBalancer  terminal.png \n zzal.png \n간단히 type을 LoadBalancer로 바꾸어주고, annotation에 어떤 ELB를 사용할지( NLB/CLB )만 적어주면 아래 그림처럼 손쉽게 NLB로 서비스를 노출 시킬 수 있습니다.\n인증서 설정을 통해 HTTPS까지?!  🌋: \u0026ldquo;그만\u0026hellip;. 아무도 안 궁금해\u0026hellip;\u0026rdquo; - 하지만 마지막까지 힘을 내서 EKS에서의 ELB를 정복해봅시다!\n apiVersion: v1 kind: Service metadata: name: nginx-nlb annotations: service.beta.kubernetes.io/aws-load-balancer-type: \u0026#34;nlb\u0026#34; service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp service.beta.kubernetes.io/aws-load-balancer-ssl-ports: \u0026#34;443\u0026#34; service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:ap-northeast-2:{{root}}:certificate/{{arn}} spec: selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80 nodePort: 30011 - protocol: TCP port: 443 targetPort: 80 nodePort: 30012 type: LoadBalancer backend-protocol은 tcp|tls 혹은 https|http로 설정이 돠는 듯합니다. 예를 들어 backend-protocol 로 tcp를 설정한 뒤 ssl-port로 443을 설정, 서비스의 spec에서의 포트로는 80과 443을 설정하면, 자동적으로 80은 tcp, 443은 tls를 이용하는 NLB listener로 설정되게 되는데 http,https도 마찬가지로 tcp,tls로 적절히 설정이 됩니다. 다만 http,https를 설정할 경우 X-Forwarded-For 헤더가 삽입된다고 합니다. (정확하지는 않아요\u0026hellip; 딱히 NLB의 backend protocol을 L7으로 설정하는 것이 NLB의 원래 스펙이 아니었던 점도 있고, L7을 이용하고 싶으면, ALB를 이용하는 것이 더 편하다고 생각이 들어서 따로 검증해본 적이 없기 때문에\u0026hellip; )\n⚠️단점이 하나 있다면 아직 http⇒https redirect가 불가능하다는 것인데, 이는 애초에 L4 LB를 이용하는 것과 L7 LB를 이용하는 쓰임에 대한 차이라고 생각을 하기 때문에 감수를 해야할 것 같습니다. 예를 들어 L4 NLB에 L7 nginx-ingress-controller을 연결하여 redirect는 nginx가 담당하도록하는 방식을 많이 이용하는 것 같습니다. NLB에서는 L4 의 뭔가 SSL/TLS한 작업을 하기 위함이고, L7의 https 작업이 주가 되는 것은 아니므로\u0026hellip;? 사실 이 부분은 잘 모르겠습니다\u0026hellip;💦💦 잘 아시는 분이 계시다면 알려주시면 감사하겠습니다. ㅜㅜㅜ( NLB에서 HTTP, HTTPS redirect가 안되는 이유 참고 - https://aws.amazon.com/premiumsupport/knowledge-center/redirect-http-https-elb/ )\n⭕️ NLB를 통한 HTTPS 서버 구축 결과 어쨌든 위의 annotation을 통해 올바르게 svc를 설정한다면\n$ kubectl get svc nginx-nlb LoadBalancer 10.100.180.174 ae27784521c4f4bcd96b22f2cca2358b-4bffb166fafa47f2.elb.ap-northeast-2.amazonaws.com 443:30014/TCP 37m  Route53 설정은 추가로 해주어야함. - 예시 nlb.umidev.net - ALIAS ae2778452xxxxxxxxxxxxxx.elb.ap-northeast-2.amazonaws.com  이렇게 service가 생성될 것이고, (service의 port로서 사용할 80,443 등은 service.spec에서 명시) 생성 후 A Record Alias로서 NLB의 DNS name을 넣어주면 사진과 같이 HTTPS 접속이 가능합니다!\n cert3.png \n🐳 마치며 어차피 한 번의 검색이면 정보를 얻을 수 있는 모든 annotation이나 기타 설정에 대한 내용을 다루기 보단 나름 제가 실제로 쿠버네티스를 관리하는 데브옵스 인턴로서 일을 하면서 헷갈렸던 내용과 EKS에서의 ELB 관리에 대한 흐름을 위주로 설명하려 노력했고, 저의 삽질이 깃든 내용들입니다 ㅎㅎㅎ\n아는 범위 + 좀 더 조사하여 열심히 정리해보았지만, 부족한 부분이 있을 수도 있고 틀린 부분이 있을 수도 있을텐데, 보완해주실 내용이 있다면 말씀해주시면 열심히 검토해보겠습니다~! 감사합니다.\n🧙‍♂️글쓴이  박진수 - 👨‍👩‍👧‍👧AUSG (AWS University Student Group) 3기로 활동 중 관심사  Docker, Kubernetes 등의 컨테이너 기술 Argo, Spinnaker, Github action 등의 CI/CD 툴 Terraform, AWS를 통한 클라우드 인프라 구축   Blog - https://senticoding.tistory.com Email - bo314@naver.com  📚 참고 (References)  EKS의 Required subnet tags  https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/load-balancing.html https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/alb-ingress.html   Kubernetes Cloud provider aws - https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#aws ALB ingress controller install - https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/controller/setup/ ALB ingress Annotation https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/ingress/annotation/ NLB의 HTTPS redirect가 불가능한 이유 -https://aws.amazon.com/premiumsupport/knowledge-center/redirect-http-https-elb/ NLB service Annotations - https://kubernetes.io/ko/docs/concepts/services-networking/service/#aws-nlb-support Ingress의 class에 대해 - https://kubernetes.io/ko/docs/concepts/services-networking/ingress/#인그레스-클래스  ","date":"2020-09-06T19:11:07+09:00","image":"https://umi0410.github.io/blog/aws/aws_eks_elb/preview_huc38b74e19f60c2848e88837c4c6920fa_1709468_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/aws/aws_eks_elb/","title":"EKS K8s에서 ELB(ALB, NLB) 제대로 설정하며 사용하기"}]