[{"content":"VBoxManage(VirtualBox CLI)로 VM 생성하기 시작하며 홈서버를 직접 구성해보면서 실제 기기 개수가 많지 않아 좀 아쉬웠는데 최근에는 이를 VM으로 충당해볼까 싶은 생각이 들기도 했다. 갖고 있는 랩탑 하나에 여러 대의 VM을 띄우면 홈서버에서 좀 더 많은 수의 노드가 있는 것처럼 이용할 수 있지 않을까 싶어 VM을 띄워보기로 했다. 하지만 이번에는 단순히 GUI로 VM을 띄우는 것이 아니라 CLI로 VM을 띄우면서 LV(Logical Volume)을 이용해봤다. 해당 내용을 까먹을까봐 이 글을 통해 한 번 정리해본다.\n실습 환경 호스트 환경은 아래와 같다.\nCPU amd64, 8 Core RAM 16GB Disk 256GB OS Ubuntu server 22.04 구성하고자하는 VM은 다음과 같다.\nCPU amd64, 2 Core RAM 4GB Disk 16GB OS Ubuntu server 22.04 우선 내 랩탑의 OS는 Ubuntu server 22.04이기에 기본적으로 GUI 프로그램은 아예 실행이 안된다. 따라서 VirtualBox GUI는 이용할 수 없었고 VBoxManage라는 CLI 프로그램을 사용했다.\n이번에 만들어볼 것은 Host machine의 Linux LV(Logical Volume)을 본인(VM)의 디스크 장치로 이용하는 VM이다.\n굳이 LV를 이용하고자 했던 이유는 호스트 머신의 OS를 완전히 다시 설치하더라도 VM의 데이터는 보존되길 바랬기 때문이다. 사실 이렇게까지 할 필요는 딱히 없었는데 요즘 볼륨이나 마운트쪽이 궁금해져 이렇게 작업해봤다.\nLV 생성하기 $ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS ...(생략) sda 8:0 0 238.5G 0 disk ├─sda1 8:1 0 1G 0 part /boot/efi ├─sda2 8:2 0 2G 0 part /boot └─sda3 8:3 0 235.4G 0 part └─ubuntu--vg-ubuntu--lv 253:0 0 100G 0 lvm / $ sudo pvs PV VG Fmt Attr PSize PFree /dev/sda3 ubuntu-vg lvm2 a-- 235.42g 135.42g $ sudo vgs VG #PV #LV #SN Attr VSize VFree ubuntu-vg 1 1 0 wz--n- 235.42g 135.42g $ sudo lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert ubuntu-lv ubuntu-vg -wi-ao---- 100.00g sda라는 디스크 장치에 sda1, sda2, sda3라는 Physical Volume이 존재함. ubuntu-vg라는 Volume Group은 sda3라는 Physical Volume으로 구성됨. ubuntu-vg라는 Volume Group 아래에는 ubuntu-lv라는 Logical Volume이 존재함. lsblk 로 랩탑의 디바이스 정보를 출력해본 내용은 위와 같다.\n앞으로 만들 vm1이라는 VM에게 ubuntu-vg(sda3)에 남는 135.4G 중 16GB를 Logical Volume으로 할당해줄 것이다. 따라서 LV를 만들어주자.\n$ sudo lvcreate -n vm1-disk -L 16G ubuntu-vg Logical volume \u0026#34;vm1-disk\u0026#34; created. $ sudo lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert ubuntu-lv ubuntu-vg -wi-ao---- 100.00g vm1-disk ubuntu-vg -wi-a----- 16.00g vm1이 Disk로 사용할 LV가 잘 만들어진 모습이다. 포맷하며 파일시스템을 설정해줘야하려나 싶었는데 딱히 그렇지는 않고 그냥 바로 VirtualBox가 사용하도록 해줄 수 있는 듯하다.\n이미지 준비하기 sudo apt update sudo apt install -y virtualbox virtualbox-ext-pack 우선 이제 슬슬 virtualbox가 필요할 것이기에 virtualbox를 설치해준다. ext-pack은 extension인데 학습할 때에는 웬만하면 있는 게 좋을 것이다. 아마 잠시 후 RDP(Remote Desktop Protocol)라는 프로토콜을 이용할 때에도 ext-pack이 있어야할 것이다.\nsudo su alias vb=VBoxManage 이제부터는 많은 명령어들이 root 권한을 필요로 할 것이다. 따라서 대부분의 작업을 root로 수행할 것이다.\n그리고 vboxmanage라는 커맨드가 상당히 마음에 들지 않아서 나는 vb 라는 alias를 이용했다.\n# vb internalcommands createrawvmdk \\ -filename vm1-disk.vmdk \\ -rawdisk /dev/ubuntu-vg/vm1-disk RAW host disk access VMDK file vm1-disk.vmdk created successfully. internal command를 이용해 호스트의 디스크 전체를 사용하는 VMDK 이미지를 vm1-disk.vmdk 생성했다. 생성되는 .vmdk 파일의 크기는 몇 백 바이트 정도로 상당히 작은데, 이 파일 자체가 데이터를 담는 것은 아니고 그냥 어떻게 호스트의 디스크(현재의 경우 vm1-disk)를 이용할 지에 대한 설정만 존재할 뿐이라서 그런 듯하다. 참고로 cat 을 통해 내용을 확인해볼 수도 있다.\ncurl -LO https://mirror.kakao.com/ubuntu-releases/22.04/ubuntu-22.04.1-live-server-amd64.iso 그리고 이제 Ubuntu Server 22.04에 대한 .iso 이미지 파일을 다운로드 해준다. 카카오의 미러링 페이지를 이용하면 와이파이로도 빠른 경우 약 30초만에 다운로드가 가능하다. 이 이미지는 설치용 이미지이다.\n이제 이미지 준비는 완료 됐다 🙂\nVBoxManage(VirtualBox CLI)를 통해 VM 생성하기 mkdir -p vbox 현재의 로컬 경로에 virtualbox vm이 base folder로 사용할 디렉토리를 만들어준다. 사실 글 초반에 적은 취지에 맞추려면 이 경로도 별도의 볼륨쪽이면 좋을 듯하긴한데 일단은 그냥 /root/vbox 로 이용하려한다.\n# vb createvm --name vm1 \\ --basefolder=/root/vbox \\ --ostype=Ubuntu_64 \\ --register Virtual machine \u0026#39;vm1\u0026#39; is created and registered. UUID: 8ad1ab2c-1e7d-4e22-b65c-51ff7cacfc25 Settings file: \u0026#39;/root/vbox/vm1/vm1.vbox\u0026#39; VM이 생성됐다고 한다! 하지만, VM을 실행하기 전에 몇 가지 설정을 더 해줘야한다.\n아직 우리는 앞서 만든 image들을 mount하지도 않았기 때문이다.\n# vb storagectl vm1 \\ --name \u0026#34;SATA Controller\u0026#34; \\ --add sata --bootable on # vb storageattach vm1 \\ --storagectl \u0026#34;SATA Controller\u0026#34; \\ --port 0 --device 0 \\ --type dvddrive \\ --medium ubuntu-22.04.1-live-server-amd64.iso # vb storageattach vm1 \\ --storagectl \u0026#34;SATA Controller\u0026#34; \\ --port 1 --device 0 --type hdd \\ --medium /root/vm1-disk.vmdk SATA라는 Hard Disk Controller을 추가해준 뒤 생성해뒀던 vm1-disk.vmdk (16GB짜리 LV 관련) 이미지를 HDD로서 추가해준다.\n그리고 bridge network를 통해 host와 같은 네트워크를 이용할 수 있게끔할 것이다.\n# ip addr show wlp2s0 2: wlp2s0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 20:16:b9:9b:55:1e brd ff:ff:ff:ff:ff:ff inet 192.168.219.180/24 metric 600 brd 192.168.219.255 scope global dynamic wlp2s0 valid_lft 13790sec preferred_lft 13790sec inet6 fe80::2216:b9ff:fe9b:551e/64 scope link valid_lft forever preferred_lft forever # vb modifyvm vm1 --nic1 bridged --bridgeadapter1 wlp2s0 내 경우에는 wlp2s0 라는 와이파이 네트워크 인터페이스에 bridge 할 수 있도록 설정했다.\n# vb modifyvm vm1 --memory 4092 \\ --cpus 2 앞서 언급한 대로 Memory는 4GB, CPU는 2 Core로 설정했다.\n이제 거의 다 왔다.\n하지만 일반적인 GUI에서 VirtualBox를 통해 VM을 실행했을 때와 달리 나의 호스트 머신의 OS는 Ubuntu Server이기 때문에 나는 설치 이미지로 VM을 시작했을 때 VM의 화면을 볼 수 없다.\n만약 Cloud image를 이용하거나 Auto installation 설정을 잘 해뒀다면 화면 없이 어떻게 어떻게 알아서 OS를 설치하고 설정한 Bridge adapter을 이용해 DHCP해서 네트워크에 조인하고 SSH 서버가 켜질 수 있을 것이다. 이렇게 되면 GUI 없이 Host machine 및 같은 네트워크 장비에서 SSH로 접속할 수 있는 게 맞긴하다.\n하지만 지금 이용 중인 Ubuntu server 이미지는 Cloud image도 아니고 우리는 별도로 Auto installation 설정을 한 것도 아니기에 Display 장치를 통해 화면을 보면서 키보드로 설치를 진행해야할 것이다.\n이때 사용할 수 있는 것이 RDP(Remote Desktop Protocol)이라는 기술이다. 자세한 내용은 따로 찾아보는 걸로하고 여기선 간단히 정리하자면 그냥 Remote에서 VM의 화면을 보면서 입출력을 할 수 있는 기술을 의미한다. 나의 경우 같은 공유기 아래의 맥북에서 RDP 접속을 통해 Ubuntu Server 호스트의 Ubuntu Server Guest(VM)의 설치를 진행할 수 있었다.\n위의 그림과 같이 맥북을 통해 호스트 머신에 RDP로 접속하면 Guset에 연결될 수 있을 것이다.\nRDP를 이용하기 위해선 클라이언트 프로그램이 필요할 것이다.\nOSX에서는 remote desktop 클라이언트로 Microsoft Remote Desktop이 가장 좋은 듯했다. 나머지는 유료거나 자기들이 개발한 자체 프로토콜 or http 등을 이용하는 것 같았다.\nRDP 프로토콜에서 인증이 필수는 아닐 것 같은데 아쉽게도 이 훌륭한 클라이언트의 단점이 하나 있다면 인증이 강제되는 것 같다는 것이다.\n따라서 VM에 RDP 설정을 켜고, 인증을 활성화 해야한다.\n# export VRDE_USERNAME=foo # export VRDE_PASSWORD=bar 위와 같이 RDP 인증에 사용할 username, password를 설정해준다. 이후 커맨드에서 사용하게 될 환경변수이다. 그냥 VRDE는 VirtualBox에서 RDP를 부르는 말 정도로만 알고 넘어가도 될 듯하다.\n# vb setproperty vrdeauthlibrary VBoxAuthSimple # vb modifyvm vm1 --vrde on \\ --vrdeport 3389 \\ --vrdeauthtype external # export PASSWORD_HASHED=$(vb internalcommands passwordhash $VRDE_PASSWORD | \\ sed \u0026#39;s/^Password hash: //\u0026#39;) # vb setextradata vm1 \\ VBoxAuthSimple/users/$VRDE_USERNAME $PASSWORD_HASHED # vb getextradata vm1 enumerate | \\ grep \u0026#34;VBoxAuthSimple/users/$VRDE_USERNAME\u0026#34; 좀 복잡하긴한데 쉽게 요약하면 그냥 RDP 인증을 VBoxAuthSimple이라는 방식을 통해 진행하겠다는 거고 그를 위한 username, password를 설정하는 작업이다.\n좀 더 자세히 설명하자면 다음과 같다.\n# vb internalcommands passwordhash $VRDE_PASSWORD Password hash: fcde2b2edba56bf408601fb721fe9b5c338d10ee429ea04fae5511b68fbf8fb9 password를 설정하기 위해 password를 해시하는 커맨드이다. plain password를 그대로 설정하지 않고 한 번 미리 해시한 뒤에 그 값을 전달해줘야한다.\n# vb getextradata vm1 enumerate | \\ grep \u0026#34;VBoxAuthSimple/users/$VRDE_USERNAME\u0026#34; Key: VBoxAuthSimple/users/foo, Value: fcde2b2edba56bf408601fb721fe9b5c338d10ee429ea04fae5511b68fbf8fb9 extradata를 조회해봄으로써 foo 라는 유저의 해쉬된 패스워드 값을 확인해볼 수 있다.\n이 값이 조회되지 않으면 인증 설정이 제대로 적용되지 않은 것이다.\n# vb startvm vm1 --type headless 드디어! vm을 시작할 때가 됐다. --type headless 인자를 주게 되면 VM의 화면을 바로 GUI로 띄우지 않고 headless mode로 시작한다. GUI로 띄우기 위한 도구들이 호스트에 제대로 설치되어있지 않다면 에러가 날 것이고 나의 랩탑 또한 거의 순정 상태의 Ubuntu server였기 때문에 headless mode로 시작해야만 했다.\nRDP Client(e.g. Microsoft Remote Desktop)에서 위와 같이 설정해주면 될 것이다.\n🎉 RDP를 통해 성공적으로 연결되면 이렇게 설치 화면을 볼 수 있을 것이다!\n와우.. 드디어 VBoxManage로 VM을 띄우는 데에 성공했다. 호스트의 공유기에 붙은 인터페이스를 브릿지 어댑터로 설정하니 호스트와 같이 공유기에 DHCP로 IP를 할당받는 것도 성공한 모습이다.\n설치가 완료된 뒤 RDP로 연결된 화면에서 몇 가지 명령어를 사용해봤다. 🙂 설치가 완료되면 이제 RDP는 굳이 사용하지 않아도 된다. 기본적으로 같은 네트워크의 어떤 장비에서든 설치 시에 입력한 username, password로 SSH 접속이 가능할 것이다. 예를 들면 다음과 같이 말이다.\n👆 같은 네트워크의 맥북에서 VM에 SSH 접속을 해서 아무 명령어나 실행해본 예시\n# vb controlvm vm1 poweroff # vb storageattach vm1 \\ --storagectl \u0026#34;SATA Controller\u0026#34; \\ --port 0 --device 0 \\ --medium none # vb storageattach vm1 \\ --storagectl \u0026#34;SATA Controller\u0026#34; \\ --port 1 --device 0 \\ --medium none # vb storageattach vm1 \\ --storagectl \u0026#34;SATA Controller\u0026#34; \\ --port 0 --device 0 --type hdd \\ --medium /root/vm1-disk.vmdk # vb startvm vm1 --type headless 그리고 이제 설치가 완료됐으니 더 이상 설치용 이미지인 ubuntu-22.04.1-live-server-amd64.iso 를 dvddrive로 붙여줄 필요도 없다. 꼭 해야하는 작업은 아니긴하지만 --medium none 을 통해 장치들을 모두 연결 해제한 뒤 그냥 .vmdk 를 0번 포트에 연결해줄 수 있다.\n배운 것, 얻은 것 PV, VG, LV 등 Linux의 LVM(Logical Volume Manager)의 개념을 접해볼 수 있었다. ‘VirtualBox의 모든 기능을 VBoxManage로 사용할 수 있구나’싶었다. RDP, VRDE라는 개념을 접해볼 수 있었다. Bridge Adapter, NAT, NAT Network, Host only 등 VirtualBox에서 사용하는 네트워크 형태를 한 번 더 경험해볼 수 있었다. 글에서 다룬 내용 외에 더 해볼 만한 것 cloud-init 볼륨을 통한 auto installation - cloud-init이라는 방식을 이용하면 직접 설치 과정을 하나 하나 진행할 필요 없이 설치와 초기 설정을 자동화 할 수 있다. Live 이미지가 아닌 Cloud image 이용하기 - Cloud image를 이용하면 auto install하는 게 기본 설정인듯 함. 좀 더 손쉽게 auto install할 수 있는 느낌이랄까. PXE 네트워크 부팅 qemu, lima 등 다른 가상화 도구를 통해 VM을 이용해보기 원하는 다양한 네트워크 토폴로지 구성해보기 마치며 며칠 전에 VirtualBox로 VM을 만들 때 Ubuntu server에서 CLI인 VBoxManage로 만들어보겠다고 꽤나 많은 삽질을 했었다. 뿐만 아니라 요즘 pxe, qemu, bridge interface, kubespray 등등으로 인해 많은 삽질이 계속되고 있었다. 본업은 본업대로 하고 퇴근 후에 취미로 공부하다보니 시간이 많지 않다보니 계속 삽질만 하고 배운 내용을 복습/정리하지 못한 것 같아서 한 번 정리해봤다.\nLVM, IDE와 SATA의 차이, RDP라는 새로운 프로토콜, img와 iso, 등등… 많은 새로운 개념 혹은 들어만 봤던 개념들을 직접 이용해보려하니 GUI에서는 정말 쉬웠던 작업을 진행하는 데에 많은 삽질이 수반됐다.\n다음에는 qemu로 VM 띄우는 내용을 작성해볼까하는데 시간이 될지는 모르겠다 화이팅..! 💪\n","date":"2022-12-06T01:35:00+09:00","image":"https://umi0410.github.io/blog/companion-server/create-vm-by-vboxmanage/preview_huf4f0c872aab790cb588eb64454baa8fb_10871749_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/companion-server/create-vm-by-vboxmanage/","title":"VBoxManage(VirtualBox CLI)로 VM 생성하기"},{"content":"시작하며 “반려 서버 키우기”라는 건전한 취미 생활을 본격적으로 시작하기 전에 그 사전 작업 중 하나로 Core DNS를 찍먹해보려한다.\n현재 나의 홈랩 상황은 다음과 같다.\n(서버 및 장난감) 라즈베리파이 4대 (서버 및 장난감) 랩탑 1대 (서버 관리용) PC 2대 핸드폰 2개 즉 나의 반려 서버는 5대, 반려 서버에 접속하는 장비는 4대(PC + 핸드폰) 정도라고 볼 수 있다.\n기존에는 매번 사용하는 PC의 /etc/hosts 를 편집해서 도메인 네임을 이용하곤 했는데 한 대의 PC로만 접속하는 게 아니라 여러 장비로 서버에 접속하게 될 수 있다보니 매번 /etc/hosts를 편집하는 것이 그닥 유쾌하진 않았고, 심지어 모바일에서는 /etc/hosts 를 제어할 수 없어 IP로 직접 접근해야했다.\n이에 대해 고민하던 차에 지인으로부터 Core DNS가 그렇게 사용하기 편하다는 얘기를 듣고 한 번 Core DNS를 통해 홈랩에서 사용할 프라이빗한 DNS 서버를 구축해보려한다.\nCore DNS가 그렇게 편한지 Getting Started Core DNS는 간단히 컨테이너로 띄울 수 있었다.\n아무런 Corefile 설정을 정의하지 않는 경우 기본적으로는 whoami 플러그인을 통해 아주 간단한 fake dns server처럼 동작하는 듯하다. 다음과 같이 정말 간단하게 coredns를 띄울 수 있고, dig를 통해 직접 띄운 core dns가 fake dns server로라도 동작은 하는지 확인해볼 수 있다.\nfake dns server는 DNS 질의에 대해 단순히 자기 IP로만 응답하는 그런 dns server라는 의미로 사용한 명칭이다.\ndocker run --rm --name coredns -p 53:53/udp coredns/coredns $ dig @127.0.0.1 -p 53 jinsu.com | grep -C 2 \u0026#39;jinsu\\.com\u0026#39; ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.18.1-1ubuntu1.2-Ubuntu \u0026lt;\u0026lt;\u0026gt;\u0026gt; @127.0.0.1 -p 53 jinsu.com ; (1 server found) ;; global options: +cmd -- ; COOKIE: c79167ab937cb4d2 (echoed) ;; QUESTION SECTION: ;jinsu.com.\tIN\tA ;; ADDITIONAL SECTION: jinsu.com.\t0\tIN\tA\t172.17.0.1 _udp.jinsu.com.\t0\tIN\tSRV\t0 0 46738 . ;; Query time: 4 msec docker을 통해 기본 옵션으로 컨테이너를 실행하면 아무 hostname(e.g. jinsu.com)으로든 쿼리하면 위와 같이 172.17.0.1 이 조회된다. jinsu.com 을 조회하든 jinsu.me를 조회하든 foo.bar을 조회하든 결과는 똑같다. whoami plugin만 활성화된 fake dns server이기 때문이다.\n(그냥 추가적으로 적어보는 내용) 172.17.0.1 라는 IP가 낯이 익을 수 있다. 이는 Docker의 bridge network interface의 IP이다.\n$ ifconfig docker0 | grep inet inet 172.17.0.1 netmask 255.255.0.0 broadcast 172.17.255.255 inet6 fe80::42:b9ff:feb4:d995 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; $ docker inspect coredns | jq \u0026#39;.[0][\u0026#34;NetworkSettings\u0026#34;][\u0026#34;Networks\u0026#34;]\u0026#39; { \u0026#34;bridge\u0026#34;: { \u0026#34;Gateway\u0026#34;: \u0026#34;172.17.0.1\u0026#34;, \u0026#34;IPAddress\u0026#34;: \u0026#34;172.17.0.3\u0026#34;, ...(생략) } } 위와 같이 172.17.0.1 은 docker의 brige network interface의 IP임을 확인해볼 수 있다. 그리고 $ docker run 명령어에 --net host 인자를 전달해 bridge network가 아닌 host network을 이용하면 DNS 질의 시 172.17.0.1 이 아닌 127.0.0.1 를 응답으로 받을 수 있을 것이다. (OSX에서는 동일하게 브릿지 IP인듯 함.)\nconfiguration을 정의해보기 우선 나의 홈랩 상황과 목표는 다음과 같다.\n보유한 공유기: LG U+에서 임대해준 평범한 공유기. 192.168.219.0/24 네트워크를 이용. me. Zone에 Raspberry Pi 4대, DNS 서버로 사용할 Laptop 1대에 대한 A 레코드를 생성할 것임. 공유기에 연결된 장비로 laptop.me 혹은 pi20[0-3].me (regex) 에 접속했을 때 올바르게 해당하는 장비에 접속할 수 있도록하기. laptop.me - 192.168.219.180 pi200.me - 192.168.219.201 … pi203.me - 192.168.219.203 코드를 통해 직관적이고 수월하고 멱등적이게 작업할 수 있도록 하기. 추후에 장비가 추가될 때에도 손쉽게 레코드를 추가할 수 있기를 바람. 그럼. Core DNS 설정을 한 번 시작해보겠다.\nmkdir -p ${HOME}/coredns-config ${HOME}/coredns-config 에 core dns에 대한 설정을 담을 directory를 생성해준다.\ncat \u0026lt;\u0026lt; EOF \u0026gt; ${HOME}/coredns-config/Corefile .:53 { hosts /etc/coredns/config/me me. { fallthrough } forward . 8.8.8.8 log } EOF Core DNS은 Corefile 을 통해 설정해줄 수 있다.\nme. Zone은 hosts plugin을 통해 /etc/coredns/config/me 에 /etc/hosts 포맷으로 정의된 레코드들을 참고하고 거기에서 원하는 레코드를 찾지 못해 질의 결과가 NXDOMAIN인 경우 fallthrough 하여 다음 plugin인 forward 를 이용하도록한다.원하는 찾는 도메인 네임이 정의되어있지 않다면 8.8.8.8로 포워드하도록 했다. 완전히 프라이빗한 나만의 TLD를 이용하는 것이 아니라 널리 쓰이는 TLD 중 하나인 .me 를 이용하는 이유는 그냥 브라우저 주소창에서 http:// 혹은 https:// 를 사용하기 귀찮아서이다. (널리 쓰이는 TLD는 http:// 혹은 https://를 적어주지 않아도 알아서 도메인으로 접속을 시도하는데 나만의 TLD는 구글 검색을 해버림)\nme. Zone이 아닌 다른 Zone에 대한 질의는 모두 기본적으로 8.8.8.8로 forward하도록했다.\ncat \u0026lt;\u0026lt;EOF \u0026gt; ${HOME}/coredns-config/me 192.168.219.180 laptop.me # Laptop EOF me. Zone에 대한 파일을 ${HOME}/coredns-config/me 경로에 기본적인 내용과 함께 생성했다.\nme_zone_file=${HOME}/coredns-config/me for id in {200..203}; do cat ${me_zone_file} | grep \u0026#34;pi${id}\u0026#34; if [[ $? -eq 0 ]]; then echo \u0026#34;Record about pi${id} has already exists.\u0026#34; else echo \u0026#34;192.168.219.${id} pi${id}.me # Raspberry Pi (id=${id})\u0026#34; \u0026gt;\u0026gt; ${me_zone_file} \u0026amp;\u0026amp; \\ echo \u0026#34;Added a record about pi${id}.\u0026#34; fi done; 200~203번 raspberry Pi에 대한 레코드도 zone file에 추가해줬다.\ndocker run --rm --name coredns -p 53:53/udp \\ -v ${HOME}/coredns-config:/etc/coredns/config coredns/coredns \\ -conf /etc/coredns/config/Corefile 위에서 작성한 설정들을 bind mount 해서 coredns를 실행시켜보자.\necho -n \u0026#34;laptop.me: \u0026#34; dig +short @127.0.0.1 -p 53 laptop.me for id in {200..203}; do echo -n \u0026#34;pi${id}.me: \u0026#34; dig +short @127.0.0.1 -p 53 pi${id}.me done; # Expected output: laptop.me: 192.168.219.180 pi200.me: 192.168.219.200 pi201.me: 192.168.219.201 pi202.me: 192.168.219.202 pi203.me: 192.168.219.203 dig를 통해 조회했을 때 잘 조회되는지 확인해봤다.\n위와 같이 응답이 온다면 결과적으로 coredns 서버가 제대로 뜬 것이다!\n오호라. 녀석 쓸만하다~ 편하다~!\n그럼 이제 우리 공유기를 이용하는 경우 기본적으로 랩탑에 띄운 DNS 서버를 이용하도록 설정해보도록 하겠다.\ndocker run --name coredns -p 53:53/udp \\ --restart always -d \\ -v ${HOME}/coredns-config:/etc/coredns/config coredns/coredns \\ -conf /etc/coredns/config/Corefile 이제는 계속 이 컨테이너를 홈랩의 DNS 서버로 이용할 것이므로 --rm 인자를 지우고 --restart always -d 인자를 추가적으로 전달해줬다.\n이제 공유기의 설정에서 DNS 서버를 설정해주자. 주 DNS 서버는 랩탑의 core dns 서버로, 부 DNS 서버는 기존에 통신사가 제공해주던 DNS 서버를 이용하려한다.\n나는 LG U+가 제공해준 공유기를 그대로 사용중이기 때문에 공유기의 관리자 페이지인 http://192.168.219.1 에 들어가서 설정을 해줘야한다. 앞서 말한대로 랩탑의 주소인 192.168.219.180 과 U+의 보조 DNS 주소인 164.124.101.2 를 적어줬다. 이상하게도 “유선 네트워크 설정” 탭에만 DNS 설정이 있던데 이 설정이 무선 네트워크에도 동일하게 적용되는 듯하다. (다행..)\n끝으로 PC에서는 자신의 DNS 설정이 공유기가 전달해주는 값으로 잘 사용 중인지 확인해본다. 이전에 내가 내 맥북에게 8.8.8.8 을 강제한 적이 있었는지 8.8.8.8으로 설정이 고정되어있었고 /etc/hosts 에서 레코드 설정을 지웠을 때 도메인 네임을 원활히 이용할 수 없었다. 위와 같이 고정값을 제거해주니 도메인 네임을 잘 이용할 수 있었다.\nfor domain in \u0026#34;laptop.me\u0026#34; \u0026#34;pi\u0026#34;{200..204}\u0026#34;.me\u0026#34;; do ping $domain -c 1 echo \u0026#39;\u0026#39; done; # Expected output: PING laptop.me (192.168.219.180): 56 data bytes 64 bytes from 192.168.219.180: icmp_seq=0 ttl=64 time=6.245 ms --- laptop.me ping statistics --- 1 packets transmitted, 1 packets received, 0.0% packet loss round-trip min/avg/max/stddev = 6.245/6.245/6.245/0.000 ms PING pi200.me (192.168.219.200): 56 data bytes 64 bytes from 192.168.219.200: icmp_seq=0 ttl=64 time=5.037 ms --- pi200.me ping statistics --- 1 packets transmitted, 1 packets received, 0.0% packet loss round-trip min/avg/max/stddev = 5.037/5.037/5.037/0.000 ms ...(생략) PING pi203.me (192.168.219.203): 56 data bytes 64 bytes from 192.168.219.203: icmp_seq=0 ttl=64 time=4.532 ms --- pi203.me ping statistics --- 1 packets transmitted, 1 packets received, 0.0% packet loss round-trip min/avg/max/stddev = 4.532/4.532/4.236/0.000 ms 끝으로 실제로 접속이 가능한지도 ping을 통해 확인해봤다. 잘 되는 듯하다.\n혹시.. 다른 dns server에게 질의하고서 다른 엔드포인트에게 ping을 날린 것은 아닐까 걱정됐다. 그럴 땐 로그를 확인하면 된다.\n$ docker logs coredns --tail 100 -f # Exptected output ...(생략) [INFO] 192.168.219.103:60949 - 45524 \u0026#34;A IN laptop.me. udp 27 false 512\u0026#34; NOERROR qr,aa,rd 52 0.000251775s [INFO] 192.168.219.103:63828 - 32218 \u0026#34;A IN pi200.me. udp 26 false 512\u0026#34; NOERROR qr,aa,rd 50 0.000233695s [INFO] 192.168.219.103:52154 - 19774 \u0026#34;A IN pi201.me. udp 26 false 512\u0026#34; NOERROR qr,aa,rd 50 0.000224735s [INFO] 192.168.219.103:49555 - 29706 \u0026#34;A IN pi202.me. udp 26 false 512\u0026#34; NOERROR qr,aa,rd 50 0.000230596s [INFO] 192.168.219.103:53338 - 47975 \u0026#34;A IN pi203.me. udp 26 false 512\u0026#34; NOERROR qr,aa,rd 50 0.000225897s 로그도 나의 dns server가 잘 동작한다는 것을 보여준다. 🙂 얏호~!\nTODO 앞으로 반려 서버 키우기를 진행하게 되면 몇 가지 수정사항들이 있을 수 있을 것 같고, 그에 맞춰 이 글에도 수정사항들이 반영될 수 있을 것 같다. 예상되는 TODO는 다음과 같다.\nRecord 관리 개선 - 현재는 bash script로 레코드를 관리하는 셈인데 Git을 이용하고 자동으로 sync를 맞추든 다른 방안을 찾아보든 하면 좋겠다. 공유기 구비 - U+의 기본 제공 공유기로는 기능이 좀 부족한 면이 있다. DNS 서버 뿐만 아니라 공유기 자체도 코딩이 가능한 나만의 장비를 쓰면 좋을 듯한데 그 이유는 각 장치에게 DHCP static lease를 좀 더 잘 해주고싶기 때문이다. 현재는 공유기 관리 페이지에서 장치별로 MAC address에 따라 static lease해주고 있다. 그리고 공유기 설정 뭐만 바꾸면 몇 십초 동안 중단되어버림… DNS 서버용 장치 구비 - 현재 CoreDNS가 laptop에 떠있는데 laptop이 뻗는다거나하면 U+의 보조 DNS 서버를 이용하게 된다. 따라서 이건 저렴한 장치(e.g. Raspberry Pi 3)를 DNS server dedicated로 사용하면 어떨까싶다. Change log [rev 2] 2022-11-29 - file plugin이 아닌 hosts plugin을 이용하도록 개선함 file plugin에는 fallthrough 기능이 없었음. 따라서 CoreDNS선에서 NXDOMAIN이 발생하는 경우 다음 plugin인 forward로 8.8.8.8에 질의할 수 없었다. 이를 바로 깨닫지는 못했는데 그 이유는 초반에는 OS와 브라우저의 캐시 때문에 잘 동작했었기 때문이다. [rev 1] 2022-11-29 - file plugin으로 me. Zone을 이용 참고 https://kimmj.github.io/coredns/configure-dns-server/ - 완전 처음에 coredns 띄우는 게 얼마나 쉬운지 맛보기용\nhttps://coredns.io/manual/toc/#configuration - core dns 공식 문서\nhttps://coredns.io/plugins/file/ - coredns의 file plugin\nhttps://coredns.io/2017/03/01/how-to-add-plugins-to-coredns/ - whoami 플러그인 개발 관련\nhttps://easydmarc.com/blog/what-is-soa-record-in-dns/ - SOA 를 어떻게 설정해야할지\nhttps://blog.naver.com/techtrip/222154620404 - zone 설정에서 IN 이 있어야하는지 없어도 되는지\nhttps://github.com/carlpett/tfz53/issues/2 - zone file에서 주석 쓰는 방법\n","date":"2022-11-30T01:35:00+09:00","image":"https://umi0410.github.io/blog/companion-server/core-dns/preview_hudf745d9e6218ded522d0b9ff98b2db64_1519649_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/companion-server/core-dns/","title":"[반려 서버 키우기 사전 작업] Core DNS를 홈랩의 커스텀 DNS 서버로 이용하기"},{"content":" 시작하며 Istio in Action을 읽고난 뒤에 리눅스나 커널에 대한 책을 읽어보겠다는 계획을 갖게 되었습니다. 정확히는 DevOps와 SE를 위한 리눅스 커널 이야기라는 책을 읽고싶었습니다. 기존에는 그 책이 사무실에 비치되어있었지만 이번에 사무실을 이사하면서 그 책이 어디갔는지 보이지가 않더라구요. 그래서 그냥 다른 책장에 비치되어있던 이 책을 읽어봤습니다.\n다루고 있는 내용은 주로 리눅스 구조라기보다는 전반적인 컴퓨터 구조, 운영체제 같았습니다.\n책에 담긴 내용 간단히 정리 커널의 역할, 운영체제의 역할이 무엇인지. 커널과 운영체제 및 드라이버들이 없다면 어떤 불편이 있을지 설명합니다. syscall, interrupt에 대해 이해하기 쉽게 설명하고 어떤 플로우로 동작하는지 설명합니다. CPU, 코어, 하이퍼 스레드에 대해 설명합니다. 메모리와 Virtual Memory, Cache, HDD, SSD의 특징과 어떻게 동작하는지를 설명합니다. fork \u0026amp; execve가 어떻게 동작하는지와 계층적인 리눅스의 프로세스 관계에 대해 설명합니다. 파일 시스템이 무엇인지, 파일 시스템이 없다면 어떠한 불편이 있는지 설명합니다. 장단점 장점 실제 코드와 그것에 대한 수행 결과, 개념을 나타낸 그림 등을 통해 이해를 돕습니다. 단순히 \u0026ldquo;이런 역할을 하는 녀석입니다.\u0026ldquo;로 설명이 끝나는 게 아니라 그 녀석이 없으면 어떤 불편이 있는지 예시를 들어줍니다. e.g. 디바이스 드라이버가 없다면 유저들은 작성하는 모든 프로그램에서 디바이스를 어떻게 제어할지를 구현해야할 것이고, 여러 프로세스가 같은 디바이스를 사용하려는 경우 어떻게 해야할 지에 대한 문제가 생길 것이다. e.g. 가상 메모리라는 개념 없이 물리 메모리를 직접 이용한다면 프로세스들이 실제로 자신의 물리메모리 주소를 관리해야하는데 이런 경우 다양한 어려움이 존재할 것이다. (너무 다양해서 이 글에선 생략.) 평소에 궁금했던 개념들, 내가 잘 알고 있는 건지 의문이 들었던 개념들, 이것까지 알아야하나싶은 개념들에 대해 한 번 가볍게 정리해볼 수 있습니다. 예를 들면 Page cache, Hyper thread 이런 개념들은 들어보기만 많이 들어보고 가볍게 알아볼 기회가 없었는데 이번에 가볍게 \u0026lsquo;아~ 이런 거구나~\u0026rsquo; 해볼 수 있었습니다. 교수님께 배웠던 Virtual Memory 역할과 흔히 인터넷에서 Virtual memory를 설명하던 내용이 조금 달라서 내가 잘 알고있는 건지 의문이 들었는데 이런 의문도 해소가 됐습니다. 흔히 인터넷에서 봤던 Virtual memory의 역할은 Page라는 개념을 바탕으로 물리 메모리를 프로세스가 안전하고 편리하게 사용할 수 있도록 해주는 것이었습니다. 교수님께 배웠던 Virtual memory는 이뿐만 아니라 메모리를 demading page, swap 같은 응용된 내용도 있었습니다. 그래서 제가 인터넷에서 봤던 내용들이 맞는 건지 교수님께 배웠던 내용이 맞는 건지 궁금했는데 이 책 덕분에 \u0026lsquo;뭐 모두 맞는 내용 같다.\u0026lsquo;는 결론을 낼 수 있었습니다. 단점 깊이가 깊진 않습니다. 깊이만 놓고 보면 학부생 수준의 컴퓨터구조, 운영체제 정도인 듯합니다. 번역이 조금 어색한 부분들이 종종 보였습니다. 마치며 2022년 독서 현황 \u0026ldquo;북적북적\u0026quot;이라는 앱을 통해 캡쳐한 저의 2022년 독서 상태입니다. 올해에 3권을 읽는 게 목표였는데 아무래도 올해가 가기전에 한 권 정도는 더 읽어볼 수 있을 것 같습니다! 책의 권수보다는 제가 어떤 내용을 어떤 깊이로 배워나가고 있는지가 더 중요하겠지만 일단은 뿌듯하네요! ㅎㅎ 한국어로 된 책인데다가 친숙한 내용, 깊지 않은 깊이였던 터라 제 기준에선 꽤나 빨리 읽을 수 있었던 것 같습니다.\n다음에 기회가 된다면 이 책처럼 컴퓨터 구조, 운영체제의 내용을 주로 다루는 책보다는 커널에 대해 다루는 책을 읽어보고 싶습니다.\n","date":"2022-10-09T19:00:00+09:00","image":"https://umi0410.github.io/blog/computer-science/linux-architecture-with-practice-and-images-review/preview_hu1424090fd25681f8121c16eecd9c85e0_232071_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/computer-science/linux-architecture-with-practice-and-images-review/","title":"[책] 실습과 그림으로 배우는 리눅스 구조 리뷰"},{"content":"시작하며 저는 데브옵스 엔지니어로 현재(2022년 9월)의 회사에 입사한 후, 기존에 막연하게 궁금증을 가져왔던 Istio라는 제품을 주로 담당하게 됐습니다.\n공식 문서와 릴리즈 노트 위주로 Istio를 공부하던 중 팀원분께 Istio in Action이라는 책을 추천받게 됐고, 이 책을 통해 조금 두루뭉실했던 이해와 삽질을 바탕으로 했던 판단들이 좀 더 분명해질 수 있었던 것 같습니다.\n그 동안은 지루함으로 인해 책을 다소 멀리 하고 짧은 아티클들을 위주로 학습을 해온 반면 올해 제 목표 중 하나는 책을 통해 좀 더 개념을 잘 정리하고 깊이 있게 공부를 해나가는 것이었습니다. 이 책은 그런 저의 목표에 다행히 잘 부합해줬던 것 같고 정말 간단히 리뷰를 해보려합니다.\n책에 담긴 내용 간단히 정리 책에 담겨있던 내용들과 함께 제 개인적인 견해, 선호를 담아 간단히 정리해보겠습니다 :)\n서비스 메쉬란 무엇인지 Istio는 무엇인지에 대한 이해를 돕습니다. 사실 Istio 관련 작업을 진행해오면서도 서비스 메쉬라는 개념이 도대체 무엇인가 의문이었습니다. 이 책에 서비스 메쉬라는 이름의 유래나 그 정의가 잘 소개되어있는지는 모르겠는데 Istio가 뭘 해주는 녀석인지는 상세히 설명된 듯 합니다. 궁금했던 서비스 메쉬에 대해 따로 알아본 내용을 정리해보자면 우선 그 이름의 유래가 궁금했는데 네트워크 형태가 그물망인 메쉬 네트워크에서 유래되어 서비스 간의 통신이 메쉬 네트워크 형태를 띄기 떄문에 서비스 메쉬라고 이름이 붙은 듯 합니다. 네트워크가 그물망 형태를 갖는다는 이유는 프록시와 같은 애들에게 각 서비스들의 요청, 응답이 다 전달되는데 이떄 하나의 Gateway에 의존하기보다는 그물망처럼 프록시들 스스로가 자신이 어떤 로직을 수행한 후 어디로 요청을 전달해야하는지를 인지하기 때문입니다. 프록시가 모든 서비스들에게 붙다보니 공통된 기능을 인프라 레벨에서 수행할 수 있습니다. 서비스마다 공통적으로 필요한 기능이면서 각 서비스들이 해당 기능을 개별적으로 개발할 필요 없이 인프라 레벨에서 공통으로 지원할 수 있는 기능은 무엇이 있는지 알려줍니다. 인증, 인가, timeout, retry, circuit breaker, rate limiting 등이 있습니다. 해당 기능들을 애플리케이션 내부에서 각각 구현하면 어떤 단점들이 있는지, 인프라 레벨에서 지원하면 어떤 장점들이 있는지 잘 소개되어 있습니다. (개인적 의견) 그렇다고해서 Istio로 그런 것을 지원하는 것만이 옳은 선택지다라고는 생각하지 않습니다\u0026hellip; 현실적으로 개발 단계에서 해당 기능들을 구현하는 게 더 간단할 수도 있고, 사소하거나 간단한 기능일 수도 있는 부분들을 릴리즈가 빠른 Istio에게 맡기는 것이 옳을지는 갸우뚱입니다. Dev와 Ops가 어우러질 수 있는 시대라고는하나 Istio의 기능에 기대게 되는 순간 그 기능을 테스트해보기 위해서는 Istio가 지원되는 테스트 환경에서 테스트를 진행해봐야할 것이고, 이 경우 보통의 개발자들에게는 번거로움이 될 수 있다고 생각합니다. Ingress나 k8s의 기본 Service 리소스만으로는 왜 부족한지, 왜 Istio의 Gateway, VirtualService, DestinationRule 등의 리소스가 필요한지 설명해줍니다. VirtualService와 DestinationRule을 이용하 카나리 배포를 손 쉽게 이용할 수도 있고, 앞서 언급한 timeout, retry, circuit breaker의 기능도 인프라 레벨에서 지원이 가능합니다. Envoy의 역할과 xDS API에 대해 설명해줍니다. 사실 Istio는 Envoy를 추상적이고 선언적으로 관리하게 해주는 도구일 뿐이지 않나 싶습니다. 그래서 Envoy라는 애를 잘 이해해야하는데 Envoy가 뭘 하는 애인지, 어떻게 설정할 수 있고 어떻게 설정을 확인할 수 있는지 잘 설명해줍니다. 트러블 슈팅 팁들을 알려줍니다! 이 부분은 정말 어디서도 찾아보기 힘든 내용이었던 것 같고 실제 트러블슈팅 시에 필요한 개념을 잡는 데에 도움이 많이 됐던 것 같습니다. 디버깅 로그를 켜는 법, 그것을 해석하는 법, SNI 이름을 해석하는 법, 과부하의 원인이 될 수 있는 점, 컨트롤 플레인이 동작하지 못할 때 발생할 수 있는 현상 등을 알려줍니다. (제가 개인적으로 트러블 슈팅을 좋아하기도 합니다.) VM을 우리 Service Registry에 포함시켜 Istio를 구성하는 방법, 다양한 모델의 Multi Cluster을 이용하는 법 등을 설명해줍니다. VM 관련된 내용은 그닥 해당 사항이 없을 듯하여 슥슥 넘겼고, 추후에 제가 Istio Multi Cluster을 이용하게 될 수도 있어 흥미로웠습니다. Istio를 설치하는 다양한 방법에 대해 소개하고, 주관적이면서도 현실적인 추천안을 제시해줍니다. Helm으로 Istio를 설치하는 경우는 어떠한지, 아예 수동으로 따로 설치하는 것은 어떤지, Istioctl을 이용하면 어떤지, control plane과 data plane을 따로 관리하는 방식은 어떤지 등을 설명해줍니다. 이런 내용을 잘 정리해서 설명하는 자료는 거의 찾아보기 힘들었던 것 같습니다. 기본적으로 Istio가 제공하는 선언적인 API들 외에 Lua Script나 Wasm을 통해 Envoy의 기능을 확장하는 방법을 알려줍니다. 마치며 2022년 독서 현황 \u0026ldquo;북적북적\u0026quot;이라는 앱을 통해 캡쳐한 저의 2022년 독서 상태입니다. 앞으로 약 2달이 남았는데 남은 2달 동안 한 권을 또 독파해 올해에 총 3권을 읽어보는 게 소박한 목표입니다. 소박하지만 이렇게 점차 늘려가다보면 나중엔 속도도 빨라지고 지식의 양도 복리처럼 늘어나있기를 기대해봅니다.\n미래에 누군가 제게 \u0026ldquo;신입 엔지니어 시절에 주로 뭐 했냐\u0026rdquo; 묻는다면 이제는 Istio를 빼놓을 수 없을 듯합니다. 😊\nIstio의 개념이 궁금은 하지만 직접 다뤄보긴 부담스러운 분들께서는 한 번 진득히 읽어보시면 좋을 듯하고, Istio를 살짝 사용은 해보고있는데 개념이 잘 정리되어있지 않아 한 번 정리를 원하시는 분들은 슥슥 읽어보시면 좋을 듯합니다. 그럼 이로서 Istio in Action의 리뷰를 마치겠습니다.\n","date":"2022-09-30T22:00:00+09:00","image":"https://umi0410.github.io/blog/devops/istio-in-action-review/preview_hu30ad32e4e2271bd82baaf769e10d7d06_259079_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/devops/istio-in-action-review/","title":"[책] Istio in Action 리뷰"},{"content":"시작하며 시간은 정말 나이를 먹을 수록 빠르게 지나가는 것일까? 매번 지난 회고를 돌이켜보면 시간이 빠르게 지나갔다는 얘기로 시작하게 된 것 같다. 올해에도 역시나다. 시간이 참 빠른 것 같다.\n이전 회고글과 같이 기술적으로 매력적인 신입이 되고자하는 회고라기보다는 이번 글은 회고라고 적기는 했으나 취준 과정 그리고 입사 이후에 대한 일기라고 볼 수도 있을 것 같다.\n되돌아본 2022 크게 올 한해를 정리해보자면 다음과 같다.\n2022.01~2022.02 | 당근마켓 플랫폼 서버 Golang 개발자 인턴으로 근무했다. 2022.03~2022.04 | 짧다면 짧은 시간이겠지만 취준생으로서 노력했다. 2022.03 | 고독한 취준 시기이고 이후에도 고독한 직장인이 되겠지만 그를 위해 취미 하나쯤은 갖고자 기타 레슨을 시작했다. 2022.04 | 취준을 마치고 홀로 🇺🇸미국여행을 다녀왔다. 향후 N년간은 겪어보지 못할 것 같다. 2022.05 | 행운이게도 고맙고 좋은 사람을 만났다. 2022.05 | 데브시스터즈에 데브옵스 엔지니어로 입사했다. 또 인생 첫 자취를 시작했다. 2022.07 | 회사에 적응 후 관심있던 istio 관련 업무를 주로 보고 있다. 2022.09 | 새로운 k8s와 클라우드 인프라 관리에 대한 큰 그림을 그려나가야할 때이다. 동료들과 종종 “진수님은 요즘은 잘 지내고 계신가요?” 이런 얘기를 나누게 될 때가 있다. 올해의 나는 항상 이렇게 대답하고 있다. “26년 인생 중 가장 행복한 한해를 보내고있다\u0026quot;고.\n그렇게 싫어했던 대학 졸업반이 되며 암기형, 족보형 대학 시험들로 부터 해방될 수 있었고, 대학 시험과 달리 채용 절차에서는 내 흥미와 개성을 살려 해왔던 공부와 작업들, 내가 가진 태도가 생각보다 매력적으로 평가받았음에 감사하고 뿌듯했다.\n입사 후에도 마음 한 켠에 인간적인 외로움이 크게 자리 잡을 수 있었을텐데 운 좋게도 고맙고 좋은 사람을 만날 수 있었다.\n지금의 회사에 입사하기까지의 과정들이 순탄하지만은 않았을 수 있지만 그런 조금은 아프고 힘들었던 과정들이 감사히도 나를 더 좋은 사람으로 만들어줄 수 있었던 것은 아닐까 싶기도한다.\n요즘은 데브옵스 엔지니어로서 관심있던 istio를 주로 담당해나가며 책임감이 커져가고 있다.\n2022년에 목표했던 것 이전에 작성했던 “2021년 회고”를 보면 올해 2022년에는 이런 3가지 계획을 세웠었다.\n지식의 깊이를 더 깊게 해야겠다 분야별 책 읽기 동영상 강의 꾸준히 시청하기 선택과 집중을 하자 너무 다양한 분야보다는 내가 우선순위에 둔 기술들을 위주로 개수는 적더라도 깊이있게 공부해나가자 운동을 소홀히 하지말자 먼저 1번에 대해 피드백 해보자.\n작년에는 MSA나 CQRS, Spring들을 주로 공부했었고, 웹에 나와있는 아티클들을 복붙해가며 단순한 개발을 해나가는 데에 많은 시간을 할애했던 것 같다. 당시에 이런 식으로 작업해나갔던 것이 잘못됐다고 후회하지는 않는다. 당시는 그런 다양한 개발 경험이 필요했고 그를 통해 다양한 지식도 습득할 수 있었던 것도 사실이었다.\n하지만 너무 두서 없이 이 글 저 글 짬뽕해서 적용하고 끝. 이러다보니 실제로 머릿 속에서 체계가 잘 잡히진 않는 느낌이었다. 따라서 좀 더 깊이를 깊게 하고자 했다.\n올해 초에는 NoSQL과 관련된 “NoSQL 철저 입문”이라는 책을 읽었다. 당근마켓에서 인턴을 하면서 DynamoDB나 Redis와 같은 데이터베이스를 적극적으로 사용하는 것을 볼 수 있었고, 항상 궁금했던 NoSQL에 대해 좀 더 알아보고 싶어 이 책을 읽게 됐었다. 사실 지금은 이 책을 완독한 지 반년이 지나자 그닥 내용이 잘 기억나지는 않지만 필요할 때 다시 찾아보면 빠른 시간에 슥슥 습득할 수 있을 것 같다. 그리고 이렇게 책을 완독한 것이 언제인지 모르겠는데 오랜만에 완독을 했다는 것 또한 꽤나 뿌듯했다.\n입사 후에는 네트워크에 관련된 내용들을 많이 다루고 있고, 그 중에서도 istio를 주로 다루고 있다. 프로덕션의 istio를 담당하기 위해서는 꽤나 알아야하는 내용들이 많고 istio는 섬세하고 신중하게 다뤄져야하는 녀석이다. 게다가 이 녀석은 한국어로 된 자료가 많지도 않으며 버전별 변경사항이 많아 블로그 글 짜깁기 방식으로는 공부를 하기가 어렵다.\n따라서 다음 세 가지를 바탕으로 공부하고 있다.\nIstio의 공식 문서 도서 - Istio in Action Istio release notes istio를 공부하고 책임지고 업무를 진행하면서 참 많은 걸 배울 수 있었던 것 같다. 또한 istio 뿐만 아니라 kafka, zookeeper등과도 관련된 내용이기도 하다.\n버전을 왜 주의하며 작업해야하는지 서로 다른 버전간의 변경 동안 서비스의 무중단을 위해 어떤 것을 신경써야할지 경우에 따라 코드 단까지 파헤쳐서 원인을 파헤치는 것 뛰어난 동료가 곁에 있다는 것이 얼마나 든든한지 대부분의 제품들에는 log level을 조절할 수 있고 뭔가 안된다싶으면 log level을 낮춰보면 큰 도움이 될 수 있다는 것 envoy proxy가 양측에 존재하는 경우, 서버쪽에만 존재하는 경우, 클라이언트 측에만 존재하는 경우 각각 어떻게 동작하는지. ← 이 부분은 주로 공식 문서에서는 제대로 소개된 글이 없는 것이 보통은 envoy proxy가 양쪽에 존재하는 경우에 대해서만 다루기 때문이다. replica가 개수에 따라 동작이 다를 수 있다는 점. 따라서 여유가 된다면 가능한 경우를 모두 사전에 테스트해봐야할 수 있다. replica가 stand alone으로 한 대만 존재하는 경우 replica가 2대가 존재하는 경우 replica가 3대가 존재하는 경우 replica가 4대(짝수)가 존재하는 경우 … 어쨌든 목표대로 Istio in Action 이라는 책도 열심히 읽고 있고 점점 책 읽는 것에 익숙해져 가고 있는 것 같아서 뿌듯하다. 다음에는 아마 Linux 커널 관련한 책을 한 권 더 읽어보지 않을까싶다. 현실적으로 2022년의 남은 3개월 동안 한 권만 더 제대로 읽어도 나는 만족한다.\n따라서 \u0026ldquo;1. 지식의 깊이를 더 깊게 해야겠다\u0026ldquo;는 잘 지켜진 것 같다.\n2번 \u0026ldquo;선택과 집중\u0026rdquo; 관련해서는 잘까진 아니어도 준수하게는 지켜지고 있는 듯하다.\n솔직히 알아야하는게 너무 너무 많은데… 나름 내 페이스대로 일상도 챙기면서 집중하기로 선택한 분야부터 쳐내가다보면 시간이 흘렀을 때 넓으면서도 깊은 사람이 될 수있으리라 생각한다.\n따라서 올해에 선택한 분야는 Linux와 Istio 및 네트워크이다. 왜 이 분야를 선택하게 되었는가는 아무래도 팀원들의 영향이 좀 있었는데 팀에 Linux와 네트워크 고수들이 정말 많기 때문이다. 나도 Linux를 좋아했고 Network도 신기해했던 분야였는데 우리 팀원분들과 함께라면 좀 더 수월하게 공부해나갈 수 있을 것 같다. 주워 듣는 키워드들만으로도 많은 도움이 되는 듯하다.\nIstio는 내가 입사 전부터 관심은 있지만 경험해볼 일은 없던 제품이었는데 우연히 팀원분의 Istio 작업을 참관하게 됐었고 흥미가 가게 되었다. 더불어 아이러니하게도 팀내의 Linux와 네트워크 고수는 많았지만 이 분들이 Istio에 대해서는 아직 큰 관심을 안 갖고 계신 상태인 경향이 있어 그나마 부족한 내가 빠르게 학습해서 기여해볼 수 있을만한 분야라는 생각이 들어 이 부분을 열심히 공부해나가고 있다.\n선택하지 않은 것들에 대해 나열해보자면 정말 많은데 다음과 같다고 볼 수 있다… 이렇게 많은 것들 중에서 Linux와 Istio 두 개만 선택해 집중해보고있다! (사실 Linux도 거의 못 보고 있긴하다.)\nk8s의 기반지식들 - k8s를 참 잘 쓰고 있지만 그 기반지식은 좀 약한 편이다. 이런 것들도 알아가면 좋긴하겠지만 아직 얘네를 챙길 여유가 없다 ㅜㅜ Kafka, Zookeeper, 합의 알고리즘 - kafka를 운영하고 있고, 대략적인 개념은 알지만 내가 직접 써본 적은 없는 게 아쉽다. 또한 kafka를 사용하지 않는 경우에도 zookeeper을 써볼 수 있는 곳이 많을 것 같은데아직 잘은 모르는 상태이다. datadog, prometheus, grafana, … 와 같은 모니터링, 시계열 DB들 - 잘 돌고는 있는데 다른 분들이 주로 맡아주시고 계신다… CouchBase, CockroachDB - 위와 마찬가지. Spring, Golang을 비롯한 개발 - 유의미한 개발을 해볼 기회가 없다. 정말 좋아하는 걸 개인적으로 사용해보려고 만드는 게 아니면 딱히 시간을 내기가 쉽지 않다. 알고리즘 공부 - 딱히 업무에 필요 없는 것 같고, 이직 생각도 없어서 안 하고 있다. … 끝으로 세 번째, \u0026ldquo;운동을 소홀히 하지 말자.\u0026rdquo;\n솔직히 요즘은 취미 생활이 운동 → 기타로 조금 가중치가 이동된 상태라 운동을 꾸준히 하는 게 쉽지가 않은 듯하다. 회사 다니기 전에는 일주일에 5번정도 했던 것 같은데 솔직히 그건 무리인듯하고 3~4번이라도 하려했는데 아무래도 1~2회로 줄여서라도 꾸준히 해나가야겠다. 기타를 거의 주 5일 치다보니 기타까지 치고 운동까지 하긴 무리가 있다. 그래도 기타 실력이 많이 늘었으니 만족~ 🙂\n요즘 나는 어떤가.. 사실 요즘은 마냥 행복하지만은 않다. (행복하긴 합니다. ㅎㅎ) 팀원이 맘에 안든다거나 철 없는 상상과 다른 사회 생활에 낙담을 했다거나 그런 것은 아니다. 단지 생각보다 내 어깨가 무겁고 앞으로의 일들을 잘 헤쳐나갈 수 있을지 조금은 자신이 없고 막막하기도 해서 이렇게 글을 적어보게 됐다…\n입사 초기에는 “저 이거 했어요! 괜찮은지 봐주시고 approve해주시면 적용하겠습니다!” 이런 가벼운 자세로 업무를 했던 것 같고 그닥 크리티컬하지 않은 부분들을 작업했으며 리뷰어분께서 많은 부분을 바로 잡아주실 수 있었기에 부담이 적었고 따라서 마냥 재미있었던 것 같다.\n하지만 그 후로 몇 달이 채 되지 않았음에도 때로는 나도 리뷰어가 되어 누군가의 작업을 면밀히 검토해야하기도하고 팀을 대표해 면접에 들어가 면접자분의 합불 여부를 판단해야하기도 하고 크리티컬한 작업을 담당하게 되기도 하고 내 판단을 모든 팀원들이 믿어주고 함께 한 배를 타고 가게 되기도한다.\n생각보다 어깨의 무게감이 빠르게 무거워진 감은 있지만 그래도 뭐 누리고 있는 것에 비하면 감사한 마음으로 이고 가야하지 않을까 싶다. 또한 어디가서는 신입이 못해볼 값진 경험이기도 할 것이다. 실제로 나의 이런 업무를 부러워하는 지인들이 분야를 막론하고 많이 존재하기도 한다! ㅎㅎ 이 글을 계기로 좀 더 힘내고 계속해서 꾸준하고 성실한 능력있는 팀원이 되어야겠다. 😆\n남은 2022년에 대한 계획 일단 슬로건은 “열정을 잃지 않고 열심히 즐기면서 공부하는 엔지니어가 되기”로 해야겠다. 솔직히 게임회사에 다니다보니 더 게임이 하고싶기는 한데… 여유가 많지 않다보니 게임은 이번 추석 연휴까지만 하고 남은 한해 동안은 게임은 끊고 공부를 좀 열심히 해야할 시기인듯 하다.\n기술 관련 전문 서적 읽기 Istio in Action 책 완독하기 선배님이 추천해주신 운영체제 책 완독하기 취미 생활 꾸준히 유지하기. 게임은 줄이더라도 취미 생활은 꾸준히 유지해야겠다. 평생가는 교양이자 자신만의 스트레스 해소 방법 및 체력이라고 생각한다. 기타 운동 컨퍼런스 영상 보고 후기 남기기 이제는 단순히 기술 자체를 설명하는 영상보다는 발표자가 겪은 문제 상황, 고민, 시도, 해결책, 느낀 점 등이 잘 들어난 컨퍼런스 영상을 보고 후기를 남겨볼까한다.\n내후년쯤엔 나도 연사자로서 컨퍼런스에 참여해볼 수 있으면 좋을텐데.. 그 밑거름이 되어주지 않을지..!\n마치며 원래는 칭찬만 하는 회고보다는 잘 못한 점을 짚고 넘어가고 다음엔 잘해보자 식의 회고를 적어보려 노력하는 편인데 이번 해는 너무나 행복한 해였어서인지 잘했다 잘했다!만 하다가 끝난 회고가 아닌가 싶기도하다.\n취준하면서 많은 회사를 넣었었고 많은 회사 면접을 보면서 내가 행복하게 일할 수 있는 회사는 어디일지 꽤나 고민했고 열심히 알아봤던 것 같다. 결과적으로는 현재의 회사에 오게 됐고 훌륭한 리더들과 훌륭한 동료들이 있어 든든하고 늘 곁에서 많이 배우고 있다. 좋은 보상과 대우로 인해 업무에 몰두하기도 좋은 환경이며 맡고 있는 업무도 재미있고 도전적이어서 올해는 참 행복한 나날을 보내고 있는 것 같다.\n중간에 조금은 힘들다 책임감이 크다 뭐 그런 내용을 적기는 했지만 아주 행복한 와중에 그런 부분도 있다는 것일 뿐이고, 조금은 내가 오바한 걸 수도 있어서 혹여 우리 팀원분이 이 글을 읽으신다면 우리가 진수님한테 그렇게 책임을 넘겨드렸나 싶어 걱정할 수도 있을텐데 그냥 잘 해내고 싶어서 괜히 나 혼자 책임을 느끼고 있다고 보는 게 맞을 것 같다. ㅎㅎ\n7월은 Istio와 씨름한다고, 8월은 OAuth 2.0, OIDC와 씨름한다고, 9월은 장염 이슈 + 새로운 아키텍쳐 설계로 인해 조금 계속해서 다다다 달려온 느낌인데 지치지 않고 뽜이팅하기!!!\n","date":"2022-09-10T15:46:54+09:00","image":"https://umi0410.github.io/blog/thought/2022-09-review/preview_hu29fb6a1fc6668bac997912987ebc8bfc_15975_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/thought/2022-09-review/","title":"2022년 9월에 적어보는 조금은 뜬금없는 9개월간의 2022년 회고"},{"content":"시작하며 데브옵스 엔지니어로 입사한 뒤 주로 맡고 있는 작업은 Istio 관련 작업이다. 평소 참 관심 있었던 분야이기도 하고 istio 뿐만 아니라 네트워크에 대해 개인적으로 정말 공부해보고싶었는데 덕분에 재미있게 공부하고 성장하며 근무하고 있는 것 같다. 😊\n근데 요즘 들어 점점 단순히 ‘어떻게 저떻게 하니까 돌아는가네~ 오.. 나 istio 좀 파악한듯?ㅋㅋ’ 수준의 자세로는 트러블슈팅을 하거나 올바르게 설계하기가 쉽지 않은 경우들이 잦아졌다.\n따라서 평소 궁금했던 내용 중 하나를 살짝 파헤쳐볼까한다.\n나는 주로 istio ingress gateway와 관련된 작업을 많이 했었고 이 경우 Virtual Service는 항상 ingress gateway를 참조하도록 설정해왔다. 근데 istio를 처음 배울 때는 분명 ‘client 측에서 outbound handler 역할을 하는 Envoy 사이드카을 통해 알아서 우리가 의도하는 목적지를 찾아서 요청을 보낸다.’ 이런 식으로 배웠던 것 같은데 ‘왜 나는 Virtual Service를 이용할 때 항상 Gateway를 설정해줘야하는 것이었을까?’하는 궁금증이 생겼다.\nVirtual Service는 Gateway에 ingress gateway를 설정해주지 않아도 되는 것일까? Virtual Service에 설정한 내용은 ingress gateway로 들어온 요청뿐만 아니라 mesh 내부에서의 요청에도 적용될 수 있는 것인가? Destination Rule은 뭐하는 녀석일까? 주로 얘는 Virtual Service와 함께 쓰는 예시가 많이 보이던데 꼭 Virtual Service와 함께 써야하는 걸까? 따라서 이번 글에서는 위와 같은 궁금증들에 대해 삽질해본 내용을 정리해보려한다. 작업해본 내용은 아래와 같다.\ningress gateway를 통해 들어오는 요청이 아니라 mesh 내부에서 보내는 요청에 대해 적용될 수 있도록 Virtual Service를 정의한 뒤 mesh 내부에서 요청을 보내보기 Virtual Service 없이 Destination 이용해보기 (2022.09.30 - 지금 와서 돌이켜보니 이때 했던 내용들은 사실 Istio의 동작 방식이나 원리를 알면 당연한 내용일 수 있었을 것 같다.)\nVirtual Service와 Destination Rule에 대해 짚고 넘어가보자. virtual services as how you route your traffic to a given destination, and then you use destination rules to configure what happens to traffic for that destination. Destination rules are applied after virtual service routing rules are evaluated, so they apply to the traffic’s “real” destination.\nhttps://istio.io/latest/docs/concepts/traffic-management/#destination-rules\nistio를 처음 배울 때에는 (물론 이 글을 쓰기 전까지도.. ㅎㅎ;;) Virtual Service와 Destination Rule이 참 헷갈렸다.\nVirtual Service는 Gateway를 통해 hosts 설정한 호스트를 이용해 들어온 요청을 어떤 기준으로 어떤 (약간은 가상인) 목적지로 보내줄 것인가를 의미한다. 이 말도 좀 헷갈릴 수 있는데 예를 들면 다음과 같다.\ningress gateway 를 통해 Virtual Service의 hosts인 [”foo.jinsu.me”] 중 하나인 foo.jinsu.me를 이용해 들어온 요청을 subset이나 weight에 따라 foo v1 이나 foo v2 로 보낸다. 위와 같은 설정을 해주는 것이 Virtual Service이다.\n그럼 foo v1과 foo v2는 누가 정의해줄까? 귀신 같이 알아서 v1에 대한 endpoint와 v2에 대한 endpoint를 분류할 수는 없을텐데 말이다. 바로 이때 Destination Rule이 등장한다.\nDestination Rule은 어떤 기준으로 특정 Service(쿠버의 기본 리소스 타입)에 대한 subset을 나눌지 정의할 수 있다. 혹은 그 외에도 기본적으로 Service가 제공하지 않는 유용한 방식들을 통해 Endpoint가 관리될 수 있도록 해준다. 이때 말하는 Endpoint는 쿠버의 기본 리소스 타입 중 하나인 Endpoint가 아니라 Destination Rule을 통해 실제로 트래픽이 흘려보내진다는 의미의 Endpoint를 의미한다.\n근데 뭐,,, 나름대로 두 개념을 정리해보려했는데 시원한 정리는 아닌 것 같다… 쓰다보면 감이오는데 아직 딱 깔끔하게 정리할 수 있는 수준의 이해는 아닌가보다.\nmesh 내부의 통신에서도 virtual service나 destination rule이 동작하는가? 앞서 Virtual Service에 대해 설명할 때 “Gateway를 통해 들어온 요청이 …(생략)” 이라며 설명을 했는데 그렇다면 Virtual Service는 ingress gateway를 통해 들어온 요청에 대한 설정만 정의가 가능하냐라고하면 그건 당연히 아니다. 애초에 istio를 통해 우리가 얻고 싶은 것은 ingress에 대한 기능뿐만 아니라 mesh 내부 통신에 대한 기능도 큰 부분일 것이니 말이다.\nThe reserved word mesh is used to imply all the sidecars in the mesh. When this field is omitted, the default gateway (mesh) will be used, which would apply the rule to all sidecars in the mesh.\nhttps://istio.io/latest/docs/reference/config/networking/virtual-service/#VirtualService\nVirtual Service가 mesh 라는 예약된 gateway 이름을 이용하면 그것은 mesh 내의 sidecar들에게 모두 적용되는 rule이 된다. Virtual Service의 gatewaymesh 내부 통신에 있어 Virtual Service의 역할이 큼에도 불구하고 아이러니하게도 위와 같은 내용을 찾아볼 수 있는 문서가 많지는 않았다.\n정리하자면 “mesh 내부의 통신에서도 virtual service나 destination rule이 동작하는가?”에 대한 대답은 아래와 같다.\nVirtual Service의 gateways에 mesh 가 포함되어있거나 gateways 가 생략되었고, client 측 envoy sidecar가 outbound handler 역할을 수행한다면 → YES Virtual Service의 gateways를 정의했는데 mesh 가 포함되어있지 않다면 → NO client 측 envoy sidecar가 존재하지 않거나 outbound handler 역할을 제대로 수행하지 못한다면 → NO 그럼 한 번 실습을 통해 확인해보자.\nmesh 내부 통신에서 Virtual Service + Destination Rule을 통해 Service subset과 weight 이용해보기 httpping-without-sidecar라는 이름의 서비스를 배포해두었고, 이는 golang으로 만들어진 요청이 들어올 때 “pong” 로그를 남기는 단순한 웹 애플리케이션이다. 이 서비스의 Pod들은 Envoy 사이드카를 주입받지 않는다.\n$ kubectl get pod -l app=httpping-without-sidecar NAME READY STATUS RESTARTS AGE httpping-without-sidecar-v1-7fd6bfdb57-bcpdh 1/1 Running 0 2m54s httpping-without-sidecar-v1-7fd6bfdb57-s4smn 1/1 Running 0 2m54s httpping-without-sidecar-v1-7fd6bfdb57-zg8td 1/1 Running 0 2m54s httpping-without-sidecar-v2-5df8b7f656-n4cvh 1/1 Running 0 2m53s httpping-without-sidecar-v2-5df8b7f656-nfm6q 1/1 Running 0 2m53s httpping-without-sidecar-v2-5df8b7f656-vbgpr 1/1 Running 0 2m53s v1 과 v2에 대한 Deployment 각각 배포해두었다.\n각 버전별로 3개의 Pod가 배포되어있다.\n버전 별로 Pod들은 다음과 같이 라벨이 붙어있다.\n# v1 Pod의 경우 app: httpping-without-sidecar version: v1 # v2 Pod의 경우 app: httpping-without-sidecar version: v2 Service, Destination Rule, Virtual Service는 다음과 같다.\napiVersion: v1 kind: Service metadata: namespace: infra-jinsu-lab name: httpping-without-sidecar spec: selector: app: httpping-without-sidecar ports: - protocol: TCP port: 8080 targetPort: 80 Service는 version을 명시하지 않은 채 app만을 명시하는 수퍼셋으로서 정의했다.\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: namespace: infra-jinsu-lab name: httpping-without-sidecar-destination spec: host: httpping-without-sidecar subsets: - name: v1 labels: app: httpping-without-sidecar version: v1 - name: v2 labels: app: httpping-without-sidecar version: v2 Destination Rule은 Service에 해당하는 Endpoint들의 Subset처럼 정의된다.\nversion 라벨이 추가되었다.\nhost는 Service를 가리킬 수 있는 이름이어야한다. 같은 네임스페이스일 경우 Short name으로 표현할 수도 있고, 언제는 FQDN(e.g. httpping-with-sidecar.infra-jinsu.lab.svc.cluster.local)으로 표현할 수도 있다.\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: namespace: infra-jinsu-lab name: httpping-without-sidecar spec: hosts: - \u0026#34;httpping-without-sidecar\u0026#34; gateways: - mesh http: - match: - uri: prefix: / route: - destination: host: httpping-without-sidecar subset: v1 port: number: 8080 weight: 10 - destination: host: httpping-without-sidecar subset: v2 port: number: 8080 weight: 90 앞서 말한대로 클러스터 내부 통신에 대해 Virtual Service를 적용하려면 gateways 를 생략하거나 mesh 라는 예약된 이름의 게이트웨이를 포함시켜준다. 또한 mesh 내부 통신에 대해서는 hosts에 Destination Rule처럼 Service를 참조할 수 있는 이름을 적어준다.\n그리고서는 Destination Rule에서 정의한 v1 subset은 weight를 10으로, v2 subset은 weight를 90으로 설정해줬다. 이렇게 되면 누군가가 서비스를 향해 요청을 보냈을 때 v1과 v2가 1대9의 비율로 로드밸런스가 될 것이다 🙂\ncurl -H \u0026#39;foo: bar\u0026#39; http://httpping-without-sidecar.infra-jinsu-lab:8080/ping 좌측은 weight가 10인 v1 Pod 3개 | 우측은 weight가 90인 v2 Pod 3개 Envoy 사이드카가 삽입된 어떤 Pod에서 위의 커맨드를 통해 httpping-without-sidecar 서비스로 요청을 보낸 경우의 모습이다. 좌측 로그는 v1 Pod 3개의 로그이고, 우측 로그는 v2 Pod 3개의 로그이다.\nVirtual Service에서 정의한대로 대부분의 트래픽은 v2로 흘러갔고 Destination Rule에서 정의한 대로 v1, v2에 대한 subset이 잘 분류된 듯하다!\n참고로 당연히도 클라이언트 측에서 Enovy가 outbound handling을 해주지 못한다면(혹은 sidecar가 삽입되지 않았다면) weight 조절도 되지 않고 그냥 라운드로빈으로 부하가 분산될 것이다.\nVirtual Service 없이 Destination Rule만으로 원하는 방식을 통해 부하 분산시키기 이번에는 Virtual Service 없이 Destination Rule의 기능 중 하나인 consistentHash를 통한 로드밸런스를 해볼 것이다. 이를 이용하면 특정 키에 대한 값을 해싱해서 해당 해시 값을 담당하는 Pod에게 요청을 보낼 것이다!\n$ kubectl delete virtualservice httpping-without-sidecar virtualservice.networking.istio.io \u0026#34;httpping-without-sidecar\u0026#34; deleted Virtual Service 없이 동작함을 분명히 하기 위해 삭제해준다.\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: namespace: infra-jinsu-lab name: httpping-without-sidecar-destination spec: host: httpping-without-sidecar trafficPolicy: loadBalancer: consistentHash: httpHeaderName: foo 이번에는 Destination을 위와 같이 작성해줌으로써 헤더 중 foo라는 키에 대한 값에 따라 부하가 분산되도록 할 것이다. 같은 값으로 요청을 보내면 담당하는 해시 값이 변경되지 않는 한 같은 Pod로만 요청이 갈 것이다.\ncurl -H \u0026#39;foo: bar\u0026#39; http://httpping-with-sidecar.infra-jinsu-lab:8080/ping Envoy 사이드카가 삽입되어 outbound handler가 잘 동작 중인 다른 Pod에서 위와 같이 요청을 여러 차례 보내봤다.\nConsistent Hash 설정 후 foo: bar라는 동일한 헤더로 요청을 보낸 경우의 로그 위와 같이 하나의 Pod로만 요청이 간 것을 확인할 수 있었다!\n마치며 이번 주에 업무가 정말 많았는데 이래 저래 오갔던 대화 중에 헷갈리는 부분이 있어서 갑자기 야밤에 이렇게 내용을 정리해봤다. 공식 문서를 다시 한 번 정독한 뒤 ‘아~ 얘의 역할은 이거니까 이렇게 하면 이렇게 되겠군!’이라며 살짝쿵 이해를 한 뒤에 실습을 해봤고 결과가 잘 나와서 뿌듯하다. 맘 편히 잘 수 있을 듯.\n다만 문서 내용과 컨셉들의 “역할\u0026quot;이나 “의미\u0026quot;에 대한 이해 뿐만 아니라 실제 동작 플로우까지도 알고 싶은 마음은 있는데 너무 조급해하는 것보다는 빠르진 않더라도 꾸준하게 이렇게 하나 하나 공부해나가면 될 것이라고 믿는다!\n참고 각종 istio 문서들 Traffic Management 컨셉 https://istio.io/latest/docs/concepts/traffic-management/ Traffic Management, Networking config 관련 문서 https://istio.io/latest/docs/reference/config/networking/ Using Istio VirtualService from inside of the cluster - https://stackoverflow.com/questions/65626962/using-istio-virtualservice-from-inside-of-the-cluster ","date":"2022-08-11T03:20:00+09:00","image":"https://umi0410.github.io/blog/devops/virtual-service-destination-rule-mesh-internal-traffic/preview_hu479968be15567657f18e71d11f31e69a_283588_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/devops/virtual-service-destination-rule-mesh-internal-traffic/","title":"Virtual Service와 Destination Rule을 이용해 mesh 내부 트래픽을 원하는 대로 라우팅해보기"},{"content":"시작하며 요즘 istio를 공부하던 중 istio가 변화해온 과정에 대해서도 흥미가 생겨 지난 버전들의 릴리즈 노트들도 읽어보고 있습니다. 그러던 중 현 시점(2022년 7월)에는 이미 공식적인 End of Life가 지난 2021년 3월 처음 릴리즈된 istio 1.10의 릴리즈 노트를 보다가 재미있는 점을 하나 발견할 수 있었는데요.\n바로 envoy proxy가 eth0에 대한 요청을 넘겨주는 container의 network interface가 lo 에서 eth0 로 변경되었다는 점입니다. 네트워크에 대해, 그 중에서도 특히 네트워크 인터페이스에 대해 잘 몰랐던 때에는 이 변경사항에 그닥 관심이 안 갔을 것 같은데 최근 네트워크 인터페이스에 대해 공부를 해서인지 이 변경사항에 관심이 갔습니다.\n그래서 이번 글에서는 네트워크 인터페이스와 관련된 istio 1.9 → 1.10의 변경사항을 실제 작업을 통해 확인해보려합니다. 실제 작업은 다음과 같습니다.\nistio 1.9 - 서버를 lo (루프백 인터페이스)의 IP(127.0.0.1)로 띄운 경우 Service에 요청을 보내 성공적인 응답을 받을 수 있음을 확인 istio 1.9 - 서버를 eth0 의 IP로 띄운 경우 Service에 요청을 보내도 요청이 제대로 전달되지 않음을 확인 istio 1.10 - 서버를 lo (루프백 인터페이스)의 IP(127.0.0.1)로 띄운 경우 Service에 요청을 보내도 요청이 제대로 전달되지 않음을 확인 istio 1.10 - 서버를 eth0 의 IP로 띄운 경우 Service에 요청을 보내 성공적인 응답을 받을 수 있음을 확인 istio \u0026lt;-\u0026gt; k8s 버전 호환성 k8s 버전은 istio 1.9와 istio 1.10 모두와 호환되는 k8s 1.20 을 이용할 것이고 k8s는 minikube를 통해 로컬에서 구성할 것입니다.\n⚠️ 참고로 M1 맥북의 minikube에선 envoy proxy가 동작하지 않아서인지 istio를 제대로 이용할 수 없습니다. 따라서 저는 ubuntu에서 minikube를 통해 작업했습니다.\nistio 1.9 → 1.10 변경사항 중 재미있는 점 1.10 릴리즈 노트 (참고: https://istio.io/latest/news/releases/1.10.x/announcing-1.10/)\n1.10 버전의 변경사항에 대해 좀 더 자세히 알아보겠습니다. 이전 버전에서는 Sidecar Envoy Proxy가 eth0으로 들어온 요청을 lo로 넘겨줘왔는데 이번 버전(1.10)에서부터는 eth0로 들어온 요청을 eth0로 넘겨준다는 것입니다.\n대부분 서버를 띄울 때 0.0.0.0을 통해 loopback interface와 ethernet interface 모두에 서버를 띄우는 경우가 많아 해당 내용이 큰 변화를 가져오진 않을 수 있습니다.\n하지만 만약 기존에 특정 interface의 IP만을 지정해서 서버를 띄우는 경우가 있었다면 이 변경사항으로 인해 통신 장애가 발생할 수 있습니다.\nloopback interface의 IP로만 서버를 띄운 경우 → eth0로 보낸 요청을 sidecar를 거쳐 lo가 받을 수 있던 컨테이너가 요청을 받을 수 없어짐. eth0 interface의 IP로만 서버를 띠운 경우 → eth0로 보낸 요청을 sidecar를 거쳐 여전히 eth0가 요청을 받을 수 있어짐. 이러한 변경 사항이 추가된 이유는?\nOn-premise나 non-k8s 환경에서는 보통 lo 에 서버를 띄우는 경우는 외부에서의 접속을 막으려는 경우일 것입니다. 반면 eth0 에 서버를 띄우는 경우는 외부에서의 접속을 허용하려는 경우일 것입니다.\n하지만 이전에는 이러한 기존 환경에서 istio를 도입하게 되면 기존의 의도와 반대되게 오히려 lo 에 서버를 띄운 경우 오히려 envoy sidecar에게 요청을 전달받을 수 있어지고, eth0 에 서버를 띄운 경우는 오히려 enovy sidecar에게 요청을 전달받을 수 없었던 모순된 상황이 발생했다고 합니다.\n따라서 이러한 모순된 상황들을 방지하고자 이러한 변경사항이 추가되었다고 합니다.\nminikube를 통해 k8s 1.20.15 환경 구축하기 $ minikube start --profile k8s-1-20 --kubernetes-version 1.20.15 위의 명령어를 통해 특정 k8s 버전(1.20.15)의 환경을 로컬에 구축할 수 있습니다.\nk8s에 istio 1.9와 1.10 설치하기 istioctl을 통해 두 버전의 istio를 k8s cluster에 배포해줄 것입니다.\nistioctl의 버전에 맞게 istio가 설치되기 때문에 그때 그때 편리하게 istioctl의 버전을 변경해야합니다. 따라서 저는 asdf라는 도구를 사용했습니다. 스타 수는 적지만 asdf의 istioctl plugin을 이용하기도 했습니다.\n미리 정의되어 있는 Profile 목록 참고: https://istio.io/latest/docs/setup/additional-setup/config-profiles/\nistioctl을 통해 istio를 클러스터에 배포할 때에는 미리 정의되어있는 profile이라는 것을 이용해 좀 더 편리하게 설정을 해줄 수 있습니다. 이번 작업에서 gateway들은 필요 없기 때문에 저는 minimal profile을 통해 설치해주겠습니다.\nasdf plugin-add istioctl https://github.com/virtualstaticvoid/asdf-istioctl.git \u0026amp;\u0026amp; \\ asdf install istioctl 1.9.9 \u0026amp;\u0026amp; \\ asdf install istioctl 1.10.6 위의 명령어를 통해 istioctl 1.9.9와 1.10.6 두 버전을 모두 설치할 수 있습니다.\n$ asdf global istioctl 1.9.9 \u0026amp;\u0026amp; istioctl version no running Istio pods in \u0026#34;istio-system\u0026#34; 1.9.9 istioctl 1.9.9를 이용하도록 설정합니다.\nistioctl install --set profile=minimal --set revision=1-9 --set values.global.proxy.holdApplicationUntilProxyStarts=true ! values.global.proxy.holdApplicationUntilProxyStarts is deprecated; use meshConfig.defaultConfig.holdApplicationUntilProxyStarts instead ! Istio is being downgraded from 1.9.0 -\u0026gt; 1.9.9.This will install the Istio 1.9.9 minimal profile with [\u0026#34;Istio core\u0026#34; \u0026#34;Istiod\u0026#34;] components into the cluster. Proceed? (y/N) y ✔ Istio core installed ✔ Istiod installed ✔ Installation complete istio 1.9.9를 1-9라는 revision으로 깔아줍니다.\nvalues.global.proxy.holdApplicationUntilProxyStarts와 meshConfig.defaultConfig.holdApplicationUntilProxyStarts는 envoy proxy가 준비된 뒤에 우리가 정의한 실제 컨테이너를 시작하는 옵션입니다. 전자는 Deprecated 되었다고 경고가 뜨지만 적용해본 결과 1.9 버전 대에서는 여전히 전자를 사용하는 것 같더라구요. 따라서 저는 1.9.9 버전에서는 전자의 인자를, 1.10.6 버전에서는 후자의 인자를 적용해주었습니다.\n만약 envoy proxy가 완전히 준비되기 전에 다른 컨테이너에서 네트워크 작업을 수행하면 해당 작업은 성공적으로 수행되지 않을 것입니다.\n$ asdf global istioctl 1.10.6 \u0026amp;\u0026amp; istioctl version client version: 1.10.6 control plane version: 1.9.9 data plane version: none istioctl 1.10.6을 이용하도록 설정합니다.\n$ istioctl install --set profile=minimal --set revision=1-10 --set meshConfig.defaultConfig.holdApplicationUntilProxyStarts=true WARNING: Istio is being upgraded from 1.9.9 -\u0026gt; 1.10.6. WARNING: Before upgrading, you may wish to use \u0026#39;istioctl analyze\u0026#39; to check forIST0002 and IST0135 deprecation warnings. This will install the Istio 1.10.6 minimal profile with [\u0026#34;Istio core\u0026#34; \u0026#34;Istiod\u0026#34;] components into the cluster. Proceed? (y/N) y ✔ Istio core installed ✔ Istiod installed ✔ Installation complete Thank you for installing Istio 1.10. Please take a few minutes to tell us about your install/upgrade experience! https://forms.gle/KjkrDnMPByq7akrYA istio 1.10.6을 1-10이라는 revision으로 깔아줍니다.\n워닝이 떠있긴 하지만 그닥 상관은 없습니다.\n$ kubectl get pod -n istio-system NAME READY STATUS RESTARTS AGE istiod-1-10-5cd4d5b44d-b98xm 1/1 Running 0 84s istiod-1-9-6b549b9d87-9f76b 1/1 Running 0 2m56s 잘 깔렸다면 위와 같이 Pod들이 성공적으로 떴을 것입니다.\nistio sidecar가 injected된 netcat server \u0026amp; client Pod 배포하기 이제 netcat server이자 client가 될 Pod 들을 배포해줄 것입니다.\n배포할 리소스들은 모두 다음과 같습니다.\nNamespace rev-1-9 - Istio 1.9의 sidecar inject를 받을 Namespace Namespace rev-1-10 - Istio 1.10의 sidecar inject를 받을 Namespace Pod netcat-client - default Namespace에서 각 Service를 통해 netcat 요청을 날려줄 client Pod netcat-lo - lo 인터페이스의 IP 에 netcat 서버를 띄우는 Pod. Pod netcat-eth0 - eth0 인터페이스의 IP에 netcat 서버를 띄우는 Pod. Service lo - lo 인터페이스의 IP에 netcat 서버를 띄운 Pod를 노출시켜줄 Service Service eth0 - eth0 인터페이스의 IP에 netcat 서버를 띄운 Pod를 노출시켜줄 Service (netcat 서버 Pod는 8080 포트에 요청이 TCP 커넥션이 형성되면 “pong”으로 응답하는 Pod입니다.)\napiVersion: v1 kind: Namespace metadata: labels: istio.io/rev: 1-9 name: rev-1-9 --- apiVersion: v1 kind: Namespace metadata: labels: istio.io/rev: 1-10 name: rev-1-10 --- apiVersion: v1 kind: Pod metadata: namespace: default name: netcat-client spec: containers: - image: ubuntu:22.04 command: - \u0026#34;/bin/bash\u0026#34; - \u0026#34;-c\u0026#34; args: - | apt update \u0026amp;\u0026amp; apt install -y ncat while true; do sleep 60; done; imagePullPolicy: IfNotPresent name: netcat-client restartPolicy: Always --- apiVersion: v1 kind: Pod metadata: namespace: rev-1-9 name: netcat-lo labels: app: rev-1-9-lo spec: containers: - image: ubuntu:22.04 command: - \u0026#34;/bin/bash\u0026#34; - \u0026#34;-c\u0026#34; args: - | apt update \u0026amp;\u0026amp; apt install -y net-tools ncat ifconfig eth0 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print $2}\u0026#39; while true do echo \u0026#39;pong\u0026#39; | nc -l 127.0.0.1 8080 sleep 1 done imagePullPolicy: IfNotPresent name: ubuntu ports: - containerPort: 8080 restartPolicy: Always --- apiVersion: v1 kind: Pod metadata: namespace: rev-1-10 name: netcat-lo labels: istio.io/rev: 1-10 app: rev-1-10-lo spec: containers: - image: ubuntu:22.04 command: - \u0026#34;/bin/bash\u0026#34; - \u0026#34;-c\u0026#34; args: - | apt update \u0026amp;\u0026amp; apt install -y net-tools ncat while true do echo \u0026#39;pong\u0026#39; | nc -l 127.0.0.1 8080 sleep 1 done imagePullPolicy: IfNotPresent name: ubuntu ports: - containerPort: 8080 restartPolicy: Always --- apiVersion: v1 kind: Pod metadata: namespace: rev-1-9 name: netcat-eth0 labels: istio.io/rev: 1-9 app: rev-1-9-eth0 spec: containers: - image: ubuntu:22.04 command: - \u0026#34;/bin/bash\u0026#34; - \u0026#34;-c\u0026#34; args: - | apt update \u0026amp;\u0026amp; apt install -y net-tools ncat ifconfig eth0 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print $2}\u0026#39; while true do echo \u0026#39;pong\u0026#39; | nc -l $(ifconfig eth0 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print $2}\u0026#39;) 8080 sleep 1 done imagePullPolicy: IfNotPresent name: ubuntu ports: - containerPort: 8080 restartPolicy: Always --- apiVersion: v1 kind: Pod metadata: namespace: rev-1-10 name: netcat-eth0 labels: istio.io/rev: 1-10 app: rev-1-10-eth0 spec: containers: - image: ubuntu:22.04 command: - \u0026#34;/bin/bash\u0026#34; - \u0026#34;-c\u0026#34; args: - | apt update \u0026amp;\u0026amp; apt install -y net-tools ncat ifconfig eth0 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print $2}\u0026#39; while true do echo \u0026#39;pong\u0026#39; | nc -l $(ifconfig eth0 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print $2}\u0026#39;) 8080 done imagePullPolicy: IfNotPresent name: ubuntu ports: - containerPort: 8080 restartPolicy: Always --- apiVersion: v1 kind: Service metadata: namespace: rev-1-9 name: lo spec: ports: - port: 8080 protocol: TCP selector: app: rev-1-9-lo --- apiVersion: v1 kind: Service metadata: namespace: rev-1-9 name: eth0 spec: ports: - port: 8080 protocol: TCP selector: app: rev-1-9-eth0 --- apiVersion: v1 kind: Service metadata: namespace: rev-1-10 name: lo spec: ports: - port: 8080 protocol: TCP selector: app: rev-1-10-lo --- apiVersion: v1 kind: Service metadata: namespace: rev-1-10 name: eth0 spec: ports: - port: 8080 protocol: TCP selector: app: rev-1-10-eth0 $ kubectl apply -f tmp.yaml namespace/rev-1-9 created namespace/rev-1-10 created pod/netcat-client created pod/netcat-lo created pod/netcat-lo created pod/netcat-eth0 created pod/netcat-eth0 created service/lo created service/eth0 created service/lo created service/eth0 created 위의 명령어들을 이용해 sidecar가 injected될 netcat 서버와 클라이언트 Pod를 배포해줍니다.\n$ kubectl get pod -A NAMESPACE NAME READY STATUS RESTARTS AGE default netcat-client 1/1 Running 0 15m istio-system istiod-1-10-5cd4d5b44d-8lv2h 1/1 Running 0 145m istio-system istiod-1-9-5ccbd888d8-scdzz 1/1 Running 0 107m ... rev-1-10 netcat-eth0 2/2 Running 0 18m rev-1-10 netcat-lo 2/2 Running 0 18m rev-1-9 netcat-eth0 2/2 Running 1 17m rev-1-9 netcat-lo 2/2 Running 1 17m $ kubectl get svc -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 3h10m istio-system istiod-1-10 ClusterIP 10.110.227.112 \u0026lt;none\u0026gt; 15010/TCP,15012/TCP,443/TCP,15014/TCP 3h3m istio-system istiod-1-9 ClusterIP 10.107.205.100 \u0026lt;none\u0026gt; 15010/TCP,15012/TCP,443/TCP,15014/TCP 145m kube-system kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 3h10m rev-1-10 eth0 ClusterIP 10.111.38.103 \u0026lt;none\u0026gt; 8080/TCP 35m rev-1-10 lo ClusterIP 10.106.100.209 \u0026lt;none\u0026gt; 8080/TCP 35m rev-1-9 eth0 ClusterIP 10.105.29.177 \u0026lt;none\u0026gt; 8080/TCP 35m rev-1-9 lo ClusterIP 10.100.228.64 \u0026lt;none\u0026gt; 8080/TCP 35m 성공적으로 배포했다면 위와 같을 것입니다.\nnetcat 서버들에게 요청 보내보기 kubectl exec와 netcat 명령어를 통해 netcat-client 의 이름으로 배포한 Pod로 각 Service들을 거쳐 각 Pod에게 요청을 보내볼 것입니다.\n$ kubectl exec -n default netcat-client -- bash -c \u0026#34;echo \u0026#39;ping\u0026#39; | nc lo.rev-1-9 8080\u0026#34; pong $ kubectl exec -n default netcat-client -- bash -c \u0026#34;echo \u0026#39;ping\u0026#39; | nc eth0.rev-1-9 8080\u0026#34; Ncat: Connection reset by peer. command terminated with exit code 1 각 Pod의 netcat server가 성공적으로 실행되면 위와 같이 netcat-client Pod를 이용해 명령을 날려 응답을 받아볼 수 있습니다.\n(초기 서버가 구동되기까지 다소 시간이 걸릴 수 있습니다. 이 경우 서버의 로그를 통해 서버가 구동됐는지를 확인해보는 것이 좋습니다.)\neth0의 IP에 서버를 띄운 Pod를 목적지로 하는 Service에게 요청을 보낼 경우 해당 목적지 Pod가 istio 1.9에 의해 sidecar injection 되었다면 Pod 내의 서버는 올바르게 요청을 전달받을 수 없습니다.\n$ kubectl exec -n default netcat-client -- bash -c \u0026#34;echo \u0026#39;ping\u0026#39; | nc lo.rev-1-10 8080\u0026#34; Ncat: Connection reset by peer. command terminated with exit code 1 $ kubectl exec -n default netcat-client -- bash -c \u0026#34;echo \u0026#39;ping\u0026#39; | nc eth0.rev-1-10 8080\u0026#34; pong istio 1.10을 이용한 경우 결과는 예상대로 istio 1.9와 반대입니다.\nlo 의 IP에 서버를 띄운 Pod를 목적지로 하는 Service에게 요청을 보낼 경우 해당 목적지 Pod가 istio 1.10에 의해 sidecar injection 되었다면 Pod 내의 서버는 올바르게 요청을 전달받을 수 없습니다.\n마치며 요즘 버전이 참 중요하다는 생각이 듭니다. 실제로 이렇게 버전에 따라 통신이 되고 안 되고가 결정될 수 있다는 것을 확인해보니 한층 더 버전이 중요하게 느껴집니다.\n따로 컨테이너 이미지를 제작하고 imagePullSecret을 기입하기 귀찮아서 기존에 존재하는 Ubuntu 이미지를 활용하려다 보니 간단하게 네트워크 통신을 하기 위해 netcat을 적극 활용해봤습니다. 오히려 너무 코드가 길어졌나 싶기도하네요.\n이 작업을 하면서 완전히 이해하지 못한 부분이 한 가지 존재하긴 합니다. 작업하며 경험한 바로는 Pod의 IP로 요청을 보내는 경우에는 istio 버전에 무관하게 eth0의 IP로 서버를 띄워야만 외부(동료 Pod들)에서는 접속이 가능하고, lo의 IP로 서버를 띄우는 경우 Pod의 IP로 접속이 불가능했는데요. 사실 이로 인해 많은 삽질을 했습니다… 왜 서비스로 접속할 때에는 예상대로 버전에 맞는 동작을 하는데, Pod의 IP로 직접 접속할 때에는 무조건 eth0의 IP로 서버를 띄운 경우에만 외부에서 접근이 가능한지는 잘 모르겠습니다.\n하지만 과거 버전의 istio를 통해 네트워크 인터페이스의 동작을 확인해보는 것은 이 정도로 충분할 것 같다고 생각이 들어 이번 글은 여기서 마무리 지으려합니다. 🙂\n","date":"2022-07-17T22:00:00+09:00","image":"https://umi0410.github.io/blog/devops/istio-1-10-network-interface/preview_hu29fb6a1fc6668bac997912987ebc8bfc_15975_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/devops/istio-1-10-network-interface/","title":"istio와 envoy proxy를 통해 경험해보는 네트워크 인터페이스 (istio 1.10)"},{"content":"시작하며 Clova AI 스피커 올해에는 많은 변화가 있었다!\n💼 데브옵스 엔지니어로 회사를 가게 됐고 아주 만족 중이다! 🏠 회사의 좋은 보상, 복지에 힘입어 생애 첫 자취를 하게 됐다. 💻 네트워크나 리눅스, 보안 등 딥한 영역에 좀 더 관심을 갖게 됐다. 🎸 기타를 꾸준히 다니고 있다. 그런 변화들 속에서 자취방에 인터넷 설치를 하게 됐는데 클로바 AI 스피커를 공짜로 주더라.\n본가에 있을 때도 쓰긴 했는데 딱히 관심 없다가 직접 클로바 앱을 깔아서 이것저것 써보니 \u0026lsquo;전보다 재밌네..? ㅎㅎ\u0026rsquo; 싶었고, \u0026lsquo;아니 이거 직접 만들지는 못하나\u0026rsquo;하는 생각이 들던 참에 \u0026ldquo;스킬 스토어”라는 게 눈에 띄었다. 스킬 스토어에는 개별 개발사들이 개발한 자기네 기능들이 소개되어있었다.\n요즘 재미있게 딥한 영역을 공부해나가고 있긴 하지만 너무 딥한 것만 하다보니 심심풀이 작업 좀 즐기고 싶은 마음에 나도 하나 스킬을 만들어보고자 했다. 마침 요즘 istio에도 관심이 많이 갔고 적용해보고 싶었는데 갖고 있던 k8s 클러스터에 istio는 배포해놓은 상태였고, Gateway, VirtualService 등의 CRD와 인증서만 잘 만져주면 스킬을 공개할 수 있을 것 같았다.\n그리하여 \u0026ldquo;우미, 미스터 디버거\u0026ldquo;라는 스킬을 개발하고자 했고 이 녀석의 기능은 사용자가 \u0026ldquo;디버거, 디버깅 팁 좀 알려줘\u0026quot;라는 식으로 요청하면 미리 정의해놓은 디버깅 팁(e.g. \u0026ldquo;환경변수가 잘 설정되어있나요?\u0026rdquo;) 중에서 랜덤으로 팁을 알려주는 것이다.\n이번 글은 그냥 일기장 같은 느낌으로 CS 지식은 일부 제외하고 적어보려합니다.\n이번 글에서 사용한 소스코드는 Github umi0410/umi-mrdebugger에서 찾아보실 수 있습니다.\n환경 및 버전 정보 이름 버전 k8s cluster 1.22 Istio 1.11.8 cert-manager 1.8.2 실제로 실무를 다루고 최신 기술들을 이용하다보니 최근 버전으로 인한 삽질이 잦았다. 따라서 혹시 참고하실 분이 계실까 싶어 내 클러스터의 환경 및 버전 정보를 정리해봤다. 버전은 참 중요하다. 각 도구들의 호환성을 잘 확인해봐야한다.\n나는 클러스터를 한동안 업그레이드 해주지 않아 2022년 7월 기준으로 그냥 저냥 최신은 아닌 1.22 버전의 k8s 클러스터를 갖고 있었고 istio는 k8s 1.22와 호환되는 버전 중 가장 오래된 minor 버전의 1.11.8을 사용했다. istio를 최대한 구버전으로 이용해본 이유는 istio를 업그레이드해나가면서 istio의 변경사항, 역사등을 공부하면 좋을 것 같기 때문이었다. 때로는 업그레이드 작업을 실제로 진행해야할 수도 있고 말이다.\ncert-manager는 가끔 CRD api version이 변경되는 경우가 있어서 그냥 기록해놨다.\nk8s는 GKE를 이용 중이고 istio는 istioctl로, cert-manager는 Helm3로 관리 중이다.\n서버 개발하기 서버 개발은 별거 없다. 그냥 HTTP Post 요청 분석 후 응답을 주면 된다. 실제로 정말 그 정도 수준이라 퇴근 후 몇 시간 만에 개발 → 배포까지 완료할 수 있었다. 생각보다 금방이었다.\n직상된 지 몇 년은 된 듯하면서 그닥 친절하진 않은 공식 문서와 예시 프로젝트(코인헬퍼)를 참고하면 쉽게 만들어볼 수 있다. 나는 예시 프로젝트인 코인헬퍼 레포지토리를 포크 떠서 개발했다.\n\u0026ldquo;우미, 미스터 디버거\u0026rdquo; 레포지토리: https://github.com/umi0410/umi-mrdebugger\n(일기장스러운 글이므로 코드를 해석하진 않겠습니다.)\n많은 예시 프로젝트 중 코인헬퍼를 골라 개발해나간 이유는 유일하게 Go언어로 개발된 예시 프로젝트였기 때문일 뿐이었다. 실제로는 그냥 타입 선언이랑 유틸 함수 정도만 가져오고 나머진 다 재구현하게 되긴 했다. 기존엔 내장 http package를 사용했는데 fiber라는 웹프레임워크 이용하도록 변경하는 등..\n쿠버네티스에 배포하기 쿠버네티스에 배포된 istio를 통해 외부에서 서버 Pod로 접속 가능하게끔 해줄 것이고, cert-manager를 통해 인증서 발급 후 해당 인증서와 관련된 시크릿을 istio가 가져가서 tls를 제공한다.\n(k8s manifest file 또한 앞서 언급했듯 https://github.com/umi0410/umi-mrdebugger 에서 찾아보실 수 있습니다.)\numi-mrdebugger/k8s $ tree 4s 00:38:42 . ├── certificate.yaml ├── cluster-issuer.yaml ├── deployment.yaml ├── gateway.yaml ├── svc.yaml └── virtual-svc.yaml 필요한 항목들은 k8s 리소스들은 위와 같다\n# cluster-issuer.yaml apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: clova-issuer namespace: cert-manager spec: acme: # The ACME server URL server: https://acme-v02.api.letsencrypt.org/directory email: dev.umijs@gmail.com # Name of a secret used to store the ACME account private key privateKeySecretRef: name: clova-issuer-secret solvers: - http01: ingress: class: istio cert-manager가 Let\u0026rsquo;s Encrypt로 인증서를 발급받을 수 있게 해주는 ClusterIssuer를 작성한다. 아마 Certificate와 동일 네임스페이스를 이용한다면 ClusterIssuer가 아닌 Issuer여도 괜찮을 것이다.\n그리고 기존에는 Challenge 방식을 DNS01로 했었는데 DNS01 방식이 약간 가물 가물해져서 우선은 HTTP01 방식으로 도메인을 인증하기로 했다.\n참고로 HTTP01 방식을 이용하는 경우 인증서를 발급받기 전부터 미리 HTTP → HTTPS 를 설정해두면 Challenge가 성공적으로 이루어지지 않는 듯하다. 아마 HTTPS로 리다이렉트 되는데 인증서가 없다보니 HTTPS 통신이 원활하지 않아서겠다.\n# certificate.yaml apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: mrdebugger-cert namespace: istio-system spec: secretName: mrdebugger-cert-secret commonName: mrdebugger.clova.jinsu.me issuerRef: name: clova-issuer kind: ClusterIssuer group: cert-manager.io dnsNames: - \u0026#34;mrdebugger.clova.jinsu.me\u0026#34; HTTP01 방식으로 Challenge를 진행하기 때문에 와일드카드(*, asterisk)는 사용하지 못해 그냥 사용할 도메인 네임인 mrdebugger.clova.jinsu.me 를 적어주었다.\n# deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: mrdebugger labels: app: mrdebugger spec: replicas: 1 revisionHistoryLimit: 3 selector: matchLabels: app: mrdebugger template: metadata: labels: app: mrdebugger spec: terminationGracePeriodSeconds: 1 containers: - name: mrdebugger image: umi0410/mrdebugger imagePullPolicy: Always ports: - name: http containerPort: 8080 readinessProbe: httpGet: port: 8080 path: /health initialDelaySeconds: 5 timeoutSeconds: 2 successThreshold: 1 failureThreshold: 3 periodSeconds: 10 # svc.yaml apiVersion: v1 kind: Service metadata: namespace: mrdebugger labels: app: mrdebugger name: mrdebugger spec: selector: app: mrdebugger ports: - name: mrdebugger port: 80 protocol: TCP targetPort: http type: ClusterIP 이번엔 그저 그런 평범한 서버 Pod를 배포해줄 Deployment와 그것을 노출시켜주는 Service를 작성해줬다. 이건 뭐 설명할 건 없다.\n# gateway.yaml apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: mrdebugger spec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;mrdebugger.clova.jinsu.me\u0026#34; #tls: # httpsRedirect: true - port: number: 443 name: https protocol: HTTPS hosts: - \u0026#34;mrdebugger.clova.jinsu.me\u0026#34; tls: mode: SIMPLE credentialName: mrdebugger-cert-secret istio의 Gateway CRD를 정의하자. 배포되어있는 istio ingress gateway를 selector로 선택해주고 아까 정의한 인증서가 담길 secret을 TLS에서 사용하도록 한다.\n# virtual-svc.yaml apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: mrdebugger namespace: mrdebugger spec: hosts: - \u0026#34;*\u0026#34; gateways: - mrdebugger http: - match: - uri: prefix: / route: - destination: host: mrdebugger port: number: 80 마지막으로 istio의 VirtualService CRD를 정의해주면 된다. 그냥 모든 요청을 mrdebugger 서비스로 넘겨준다고 보면 된다.\n💡 몇 가지 istio CRD에 대해 헷갈리는 부분들을 정의해보자면 다음과 같다.\nGateway는 istio ingress gateway와 다르다. Gateway는 CRD이고, ingress gateway는 자신을 selector로 설정한 Gateway를 보고 해당 설정을 적용한다. 마치 Ingress와 Nginx Ingress Controller 같은 느낌이랄까 Istio ingress gateway는 보통 istio-system 네임스페이스에 존재한다. Gateway는 아무 네임스페이스에 만들어도 된다. VirtualSevice 또한 아무 네임스페이스에 만들어도 된다. 단, VirtualService의 destination은 접근 가능한 mesh 내의 호스트여야하고 네임스페이스가 다른 경우 적절한 도메인 네임을 써야할 수 있다. 같은 네임스페이스의 경우 service name만으로 이용 가능하다. Service, Deployment, Pod, … 또한 아무 네임스페이스에 만들어도 된다. umi-mrdebugger $ kubectl create ns mrdebugger namespace/mrdebugger created umi-mrdebugger $ kubectl label namespace mrdebugger istio-injection=enabled namespace/mrdebugger labeled umi-mrdebugger $ kubectl apply -n mrdebugger -f k8s/![clova-extension.png](clova-extension.png) certificate.cert-manager.io/mrdebugger-cert created clusterissuer.cert-manager.io/clova-issuer created deployment.apps/mrdebugger created gateway.networking.istio.io/mrdebugger created service/mrdebugger created virtualservice.networking.istio.io/mrdebugger created 한 번에 리소스가 빵 떠서 잘 연결되면 좋겠지만,,, 당연히도 현실실은 그렇진 않다. 🥲\n자잘한 이슈들을 해결해주고 나면 다음과 같이 서버에게 성공적으로 응답을 받을 수 있다.\n# 인증서가 READY 되었다! $ kubectl get cert -n istio-system mrdebugger-cert NAME READY SECRET AGE mrdebugger-cert True mrdebugger-cert-secret 28m $ curl https://mrdebugger.clova.jinsu.me/health OK 🎉 굿. 좋당!\nClova AI Console에서 Custom Extension 등록하기 Clova AI Console Clova AI Console에 가서 자신의 Extension을 생성하면 된다. 만드는 과정은 별건 없다.\n호출 이름 설정 이것만 잘 설정해주면 되는데 보통 \u0026ldquo;{{호출 이름}} 시작해줘\u0026quot;로 스킬이 시작되는 것 같다. 그 이후는 세션이 끊어지지 않으면 세션의 컨텍스트를 살려 대화할 수는 있는듯 하다.\nInteraction 모델 빌드 그리고 나선 Interaction 모델 설정에 들어가 빌드를 해줘야한다. 자신이 Custom Intent를 사용하지 않는다해도 무조건 한 번은 빌드를 해줘야한다. 문서 상으로는 3~5분 정도 소요된다고 하는데 체감상 한 10분은 걸린듯하다… 최초 1번이 오래 걸리는 것 같고 그 이후로는 금방 되는 듯? (사이트 UI가 그닥 친절하진 않다. 약간은 버려진 느낌…?)\n그러고 나면 테스터 ID를 등록해줘야한다. 보통은 심사, 실제 퍼블리시까지는 귀찮기 때문에… 혼자만 쓸 거라던가 그냥 재미삼아 만들어본 경우에는 뭐 테스터만으로 사용하는 것도 충분할 것 같다. 네이버 API 등등은 주로 이렇게 출시 심사 전까지 테스트 ID를 사용할 수 있게 해주더라.\n테스터 아이디를 등록한 뒤 네이버 클로바 앱에 자신의 네이버 아이디로 로그인하면 스킬 스토어 페이지에서 위와 같이 자신이 등록한 스킬을 볼 수 있다. 문서상으론 시간이 꽤 걸릴 수도 있다고 했는데 아마 바로 보였던 것 같다.\n그럼 아마 아까 콘솔에서 설정한 \u0026ldquo;{{호출 이름}} 시작해줘\u0026quot;로 스킬을 시작할 수 있을 것이다. 나는 끝에 \u0026ldquo;시작해줘\u0026quot;를 안 붙여도 되는 줄 알고 호출 이름을 \u0026ldquo;디버깅 도와줘\u0026quot;로 했는데 고생을 좀 했다. 호출 이름이 \u0026ldquo;디버깅 도와줘\u0026quot;인 경우 실제로는 \u0026ldquo;디버깅 도와줘 시작해줘\u0026rdquo; 이런 식으로 말해야 실행이 되기 때문이다… ㅜㅜ\n그럼 결과를 확인해보자.\n🎉 귀엽게 디버깅을 도와줘는 \u0026ldquo;미스터 디버거 우미\u0026ldquo;를 만나볼 수 있었다. 🥳\n마치며 오랜만에 딥한 기술을 탐구하는 것에서 벗어나 유흥스러운 개발을 해보았다. 사이드 프로젝트를 시작으로 가꾸게 됐던 GKE 클러스터에 istio를 구축만 해놓고 제대로 도입은 못해본 상태였는데 이번 서버 배포를 진행하면서 한 번 데모처럼 istio를 사용해볼 수 있었어서 좋았다. istio를 도입해본 걸 생각하면 그닥 유흥스럽다기 보단 기술 탐구스러웠을지도..? ㅎㅎ\n새로 회사를 들어가고나서는 궁금한 것, 공부하고 싶은 것이 넘쳐나고 재미있게 공부해나가고 있지만 다소 블로그에 정리하긴 어려운 내용들이 많은 것 같다. 어려운 내용들이다보니 공부는 했더라도 1번의 공부만으로 의미있는 글을 담아내긴 어렵기도 했다.\n하지만 바쁘기도 한데다가 뭔가 깊이 있는 글을 쓰려다보니 요즘 블로그에 글을 오히려 안 쓰게 되고 \u0026lsquo;아 이것도 써보고 싶은데 저것도 써보고 싶은데…\u0026rsquo; 생각만 하다 벌써 마지막으로 글을 쓴지도 엄청 오래된 것 같다.\n그래서 앞으로는 CS 지식이나 정확한 정보는 아니더라도 가끔 이렇게 개발 일기스러운 글도 적어볼까한다. 그런 글은 남에게 정보를 전달하려는 목적보단 나를 기록하는 용도기 때문에 \u0026lsquo;…입니다.\u0026rsquo; 체보다는 \u0026lsquo;…이다\u0026rsquo; 체로 그냥 가볍게 적어나가려한다.\n그럼, 다음에 또 만나용.\n","date":"2022-07-13T23:00:00+09:00","image":"https://umi0410.github.io/blog/clova-ai-on-k8s/preview_hu0de35698344aa28c3f5141b2d5dcaef6_812038_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/clova-ai-on-k8s/","title":"쿠버네티스로 Clova AI Custom Extension 배포하기 (feat. Istio, Cert Manager)"},{"content":"시작하며 저번 글(\u0026ldquo;ArgoCD 선언적으로 이용해나가기 - Helm, App of App\u0026rdquo;)에서는 Helm과 App of App 패턴을 이용해 ArgoCD를 선언적으로 이용해나가는 방법을 다뤘습니다.\n바~로 ArgoCD를 좀 더 선언적으로 이용해나가는 것과 관련된 팁부터 글을 적어나가기는 좀 무리가 있을 것 같아 해당 글에서는\n배경지식들에 대한 간략한 정리와 핸즈온 같은 느낌의 내용들도 많이 포함하게 됐던 것 같아요.\n이번 글에서는 ArgoCD를 실제로 이용하기 위해 필요할만한 이런 저런 설정들을 어떻게 선언적으로 정의해볼 수 있을지를 소개해보려해요!\n그리고 그 예시로 RBAC 설정을 해보겠습니다. 이번 글에서 수행해볼 작업은 Github 계정을 ArgoCD와 연동시켜 역할(Role) 기반으로 ArgoCD의 권한을 제어하는 것이에요.\n\u0026lsquo;왜 RBAC을 ArgoCD 커스터마이징의 예시로 했는가?\u0026lsquo;는 제가 사이드 프로젝트에서 ArgoCD를 이용하면서 Local user(ArgoCD의 자체 계정)을 이용하는 데에 불편이 있었기 때문이에요. 저는 ArgoCD와 Github 계정을 연동해 RBAC을 이용함으로써 해결했습니다. 이제는 Admin 계정의 패스워드를 암기할 필요도 새로운 팀원에게 새 계정을 생성해 전달할 일도 필요가 없어졌습니다! 또한 Github 계정 별로 권한을 갖도록 (e.g. readonly) 설정할 수도 있어졌습니다.\n그리고 글의 내용은 \u0026lsquo;제 글만 무조건 따라하면 됩니다\u0026lsquo;보다는 \u0026lsquo;저는 이런 식으로 자료를 찾았고 이렇게 적용해봤어요.\u0026lsquo;식의 내용들을 다뤄보겠습니다.\n본 글에서 사용하는 환경 핸즈온 과정 참고 레포지토리 - https://github.com/umi0410/declarative-argocd/tree/master/configuring-rbac Minikube를 이용한 K8s Cluster ArgoCD Helm Chart를 이용해 argocd namespace에 배포한 ArgoCD RBAC이란? RBAC은 Role-Based Access Contol의 약자로 쉽게 말하자면 그냥 역할 기반 권한 제어입니다.\n속성 기반으로 나름 복잡한 로직에 의해 어떤 유저가 어떤 리소스에 어떤 작업을 수행할 수 있는지를 정의하는 ABAC(Attribute-Base Access Control)에 비해 그저 간단히 역할에 권한들을 부여하고 유저들은 그 역할을 부여받음으로써 권한을 체크하게 됩니다.\n개인적으로는 많은 경우에 간단히 RBAC만으로도 권한 제어가 충분한 경우가 많은 것 같습니다. 그리고 많은 솔루션들이 RBAC 기능을 지원하는 것 같았어요. ArgoCD 또한 마찬가지로 RBAC을 지원하고 있습니다.\nGithub organization + team으로 ArgoCD RBAC 이용하기 자, 그럼 Github의 Organization, Team을 통해 ArgoCD에서 RBAC을 이용해보는 과정을 진행해봅시다!\n1. ArgoCD에 RBAC 설정하기 요즘 인기있는 솔루션들은 RBAC 및 기타 권한 제어 기능이 대부분 존재합니다. 보안과 권한 관리는 아주 중요하기 때문입니다. 그리고 그런 권한 제어의 방법 중 가장 대중적인 방법은 바로 RBAC 입니다.\n공식 문서 속 RBAC 설명 공식 문서는 언제나 가장 좋은 교과서입니다. 자세한 방법은 공식 문서에서 찾아보실 수 있습니다.\nArgoCD Helm Chart 속 Configuration default values.yaml 또한 제가 사용한 ArgoCD Helm Chart에서 제공하는 README.md 속 Configuration 부분과 default values.yaml 보면\nHelm Chart를 통해 배포한 ArgoCD의 경우 어떻게 values.yaml을 통해 rbac을 설정할 수 있는지 나와있습니다.\n기본 원리는 다음과 같습니다.\np, \u0026lt;role/user/group\u0026gt;, \u0026lt;resource\u0026gt;, \u0026lt;action\u0026gt;, \u0026lt;appproject\u0026gt;/\u0026lt;object\u0026gt; - AppProject에 속한 Object(e.g. Application)에 대한 권한을 정의한다. p, \u0026lt;role/user/group\u0026gt;, \u0026lt;resource\u0026gt;, \u0026lt;action\u0026gt;, \u0026lt;object\u0026gt; - AppProject에 속하지 않는 Object(e.g. Clusters, AppProject, Repositories, \u0026hellip;)에 대한 권한을 정의한다. 그리고 위에서 말하는 Resource나 Action은 다음과 같습니다.\nResources: clusters, projects, applications, repositories, certificates, accounts, gpgkeys Actions: get, create, update, delete, sync, override, action 공식 문서의 설명이 좀 부족하긴하지만 Github SSO를 이용해 인증하는 경우 RBAC은 다음과 같이 설정할 수 있다고 합니다.\np, role:org-admin, applications, *, */*, allow p, role:org-admin, clusters, get, *, allow p, role:org-admin, repositories, get, *, allow p, role:org-admin, repositories, create, *, allow p, role:org-admin, repositories, update, *, allow p, role:org-admin, repositories, delete, *, allow g, your-github-org:your-team, role:org-admin 의미: org-admin이라는 Role이 갖는 권한들을 정의하고 해당 :에 속하는 Github user로 로그인한 경우 org-admin Role에 허용된 권한들을 갖는다. 그리고 저는 다음과 같은 요구사항을 가정하려합니다.\nRBAC 요구사항\n나의 Github Organization khu-dev의 khumu-devops 팀의 유저들은 admin 권한을 갖는다. 나의 Github Organization khu-dev의 khumu-devops 팀의 유저들은 readonly 권한을 갖는다. 그 경우 저는 다음과 같이 values.yaml을 작성한 뒤 helm release를 update 해주면 됩니다.\n# values.yaml server: rbacConfig: policy.csv: | g, khu-dev:khumu-developer, role:readonly g, khu-dev:khumu-devops, role:admin ... readonly와 admin이라는 Role은 공식문서에도 나와있듯 builtin-policy로서 이곳에 작성되어 있습니다.\n$ helm upgrade -n argocd argocd \\ argo/argo-cd -f values.yaml 그럼 위의 커맨드를 통해 RBAC 설정을 적용시켜주겠습니다.\n2. ArgoCD에 Github을 통한 SSO 설정하기 ArgoCD는 Dex라는 인증 관련 각종 기능을 담당하는 컴포넌트를 함께 제공하고 있습니다. Github의 OAuth2를 이용해 SSO(Single Sign On)을 이용하는 과정은 공식문서 Dex 부분에도 소개되어있고, 사실 이대로 따라만하면 됩니다. ㅎㅎ\nGithub OAuth app 생성하기 Organization - Settings - Developer Settings - OAuth Apps 에서 위와 같이 OAuth App을 만들어주고 이후 생성되는 Client ID와 Client Secret을 다음과 같이 dex.config에 전달하면 됩니다. (사실 Organization이 아니라 개인 계정에서 App을 생성해도 동작합니다.)\n단, dex.config는 argocd-cm이라는 Configmap의 .data의 필드로 존재하기 때문에 우리는 Helm chart의 values.yaml에 dex.config를 올바르게 전달해야합니다. 이를 위해선 server.config.dex.config를 다음과 같이 작성해주면 됩니다.\nserver: config: dex.config: | connectors: # Github OAuth를 통해 SSO하려는 경우 - type: github id: github # 상관 없을 듯 뭘로 하든 name: GitHub # 상관 없을 듯 뭘로 하든 config: clientID: \u0026lt;your github oauth app client id\u0026gt; clientSecret: \u0026lt;your github oauth app client secret\u0026gt; orgs: - name: \u0026lt;your github organization name\u0026gt; # e.g. khu-dev ... server.config.dex.config에 작성해야한다는 것은 어떻게 알 수 있는가?\nArgoCD의 공식 문서에서는 argocd-cm.yaml을 \u0026lsquo;General Argo CD configuration\u0026rsquo;로 소개하고 있습니다. ArgoCD Helm Chart Github에서는 values.yaml 속 server.config를 동일하게 \u0026lsquo;General Argo CD configuration\u0026rsquo;로 소개하고 있습니다. 따라서 values.yaml 속 server.config에 정의하는 내용이 곧 argocd-cm.yaml의 .data가 됨을 유추할 수 있습니다. $ helm upgrade -n argocd argocd \\ argo/argo-cd -f values.yaml 한 번 더 위의 커맨드를 통해 RBAC 설정을 적용시켜주겠습니다.\nLOG IN VIA GITHUB 버튼이 추가된 모습 성공적으로 Github을 통한 SSO가 활성화되면 ArgoCD에 접속할 때 위와 같이 LOG IN VIA GITHUB 버튼이 추가된 것을 확인할 수 있습니다.\n그럼 요구사항대로 khu-dev라는 Github Organization의 khumu-developer Team에 속한 계정으로 ArgoCD에 로그인해보겠습니다.\nUser info 성공적으로 로그인한 뒤 \u0026lt;your argocd url\u0026gt;/user-info로 이동하면 위와 같이 : 이라는 이름의 그룹에 속한 모습을 볼 수 있습니다.\nPermission denied 이후 만약 어떤 Application을 Sync하려해도 현재 속한 그룹인 khu-dev:khumu-developer는 readonly Role을 부여받았기에 어떠한 App도 Sync할 권한이 없어 위와 같이 Permission Denied 라는 에러를 맞이하게 됩니다.\nUser info 🎉🎉🎉 하지만 위와 같이 khu-dev:khumu-devops에 속한 계정으로 로그인한 뒤 동일하게 어떤 App을 Sync해보면 이번에는 성공적으로 App이 Sync되는 모습을 볼 수 있습니다. khu-dev:khumu-devops 그룹은 admin Role을 부여받았기 때문입니다!\n자, 그럼 이제는 admin 계정을 비롯한 username, password 기반의 계정을 이용할 필요 없이 Github을 이용해 ArgoCD의 인증/인가를 수행할 수 있습니다 ㅎㅎ\n마치며 새로운 회사에 입사하고 행복하게 적응해나가면서 운동, 취미, 업무까지 챙기다보니 글을 작성해놓고 마무리를 오랜 시간 동안 짓지 못하다가 급히 지방선거일을 기회 삼아 글을 마무리 지어봤습니다. 우선은 ArgoCD를 Helm chart와 함께 선언적으로 이용해나가는 내용은 이 글을 끝으로 2편으로 마무리 지어보려합니다. 사이드 프로젝트에서 ArgoCD를 이용하면서 ArgoCD는 선언적으로 관리가 가능하다는 장점을 많이 써먹지 못하고 좀 더러운 형태로 억지로 사용하다보니 번거롭게 느껴졌던 경우가 많았는데 저 스스로도 이를 선언적인 형태로 개선하면서 많이 편해졌고, 그 과정들을 이렇게 글로 기록하고나니 나름 뿌듯합니다.\nArgoCD 관련한 또 다른 내용으로는 Applpicaitonset에 관해 추후에 기회가 되면 다뤄볼까 생각 중입니다.\n몇 년 전에 처음 데브옵스 엔지니어 인턴으로서 이쪽 일을 시작했을 때 제가 알던 ArgoCD는 \u0026lsquo;ArgoCD라고 요즘 쿠버네티스 환경에서 쓸만한 CD 툴이 있다더라\u0026rsquo; 정도의 도구였던 것 같은데 이제는 자체 CD툴만을 사용하는 기업을 제외하면 쿠버네티스를 사용하는 경우는 대부분이 ArgoCD를 이용할 정도로 대중적인 기술이 된 것 같고, 그 시절보다 많은 기능들을 편리하게 제공하는 기술이 된 것 같아 새삼 기특하단 생각이 들었습니다 ㅎㅎ.\n데브옵스로 입사하면서 거대한 규모의 클라우드 인프라를 다루다보니 네트워크 관련된 내용을 자주 접하게 되고, 이론적으로는 제가 약한 부분이라고 생각이 들어서 앞으로는 DevOps 서비스들보다는 네트워크 공부를 우선적으로 해볼까합니다! 곧 자취를 시작할텐데 라즈베리 파이로 자취방에서 재밌는 무언가를 해볼 수 있을까 싶은 생각도 있구요 ㅎㅎ. 시간이 되면 관련 내용을 틈틈이 기록해보겠습니다! 감사합니다.\n참고자료 RBAC vs ABAC - Okta ArgoCD 공식 문서 ArgoCD Helm Chart ","date":"2022-05-07T22:00:00+09:00","image":"https://umi0410.github.io/blog/devops/declarative-argocd-helm-rbac/logo_huc6aa4a846d689b08bb4f93a6ab036ab8_63751_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/devops/declarative-argocd-helm-rbac/","title":"ArgoCD 선언적으로 이용해나가기 - Github을 통한 SSO 및 RBAC"},{"content":"시작하며 취준생 기간을 마무리하고 데브옵스 엔지니어로서 커리어를 시작하게 됐습니다! 🎉\n근무를 시작하기 전까지 기간이 좀 남아있어서 그 전까지 뭘 해보면 재밌을까 고민하던 중에 새로 생긴 클라우드 동아리인 Cloud Club에서 클라우드와 인프라에 관한 주제로 발표할 기회가 주어져 ArgoCD를 토픽으로 발표를 해보려합니다. 그리고 관련된 내용을 한 번 더 블로그에 정리해보려해요!\n주제는 \u0026ldquo;ArgoCD 선언적으로 이용해나가기\u0026rdquo; 이고 주로 다룰 내용은 어떻게 ArgoCD를 좀 더 선언적으로 관리하고 이용해나갈 수 있을지입니다.\nArgoCD가 뭔지, 어떻게 Getting Started할지, GitOps가 뭔지 등을 다루지 않으려는 이유는 이미 그 내용으로는 충분한 양질의 자료가 존재한다고 생각하기 때문입니다. ArgoCD나 GitOps의 개념 자체 같은 내용보다는 새로운 기술(ArgoCD)을 접할 때 어떻게 입맛대로 커스터마이징해나가는지 그리고 그 작업을 어떻게 좀 더 유지보수하기 쉽도록 즉 선언적으로 해나갈 수 있을지에 대한 저의 작업 과정을 소개해보려합니다 ☺️\n이번 글에서는 크게 다음과 같은 작업을 다루게 될 거에요.\nminikube를 통해 로컬에 kubernetes 환경을 구축합니다. helm을 통해 ArgoCD를 install 합니다. ArgoCD를 이용해보기 위해 CRD인 AppProject와 Application을 생성합니다. 실제로 GitOps의 형태로 CD가 잘 동작하는지 확인~ 1~4번 과정을 helm을 통해 좀 더 선언적으로 수행할 수 있도록 개선합니다. 해당 과정들을 실습해보고 싶으신 분들은 제가 작성해둔 Repository(https://github.com/umi0410/declarative-argocd)를 포크한 뒤 작업하시면 편하실 것 같습니다.\nGlossary (용어 사전) 간단하게 이번 글에서 다루게 될 용어들을 정리해보겠습니다. 자세한 내용이 궁금하시다면 따로 찾아보시는 것을 추천드리겠습니다. 이번 글에서 전부를 다루긴 힘들기 때문이에요 ㅜㅅㅜ\nGlossary - 용어 사전 minikube - \u0026ldquo;minikube is local Kubernetes, focusing on making it easy to learn and develop for Kubernetes.\u0026rdquo; from Minikube Docs ArgoCD - \u0026ldquo;Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes.\u0026rdquo; - from ArgoCD Documents CRD(Custom Resource Definition) - K8s의 표준 Resource는 아니지만 개별적으로 정의하여 사용할 수 있는 리소스 kind들을 말함. Application - ArgoCD가 사용하는 CRD 중 하나. 어떤 K8s 리소스들을 어떤 Repository에서 가져와서 어떤 식으로 배포할 지를 정의함. AppProject 혹은 Project - Application이 속하는 그룹. GitOps - DevOps의 실현 형태 중 하나. 인프라나 애플리케이션 운영 정보에 대한 Single Source of Truth로서 Git Repository를 활용하는 형태 Helm - K8s application을 패키징하여 편리하게 관리할 수 있도록 해주는 도구 Chart - Helm을 통해 배포되는 것. Chart는 다양한 k8s 리소스들을 패키징한 형태이다. 선언적(Declarative) - 절차적(Procedural) 혹은 명령적(Imperative) 방식과 달리 과정을 생략하고 간결하게 Desired State를 선언적으로 정의하는 것. Requirements minikube brew install minikube \u0026amp;\u0026amp; minikube start kubectl brew install kubectl \u0026amp;\u0026amp; kubectl version helm brew install helm \u0026amp;\u0026amp; helm version Github account 예시 코드 레포(https://github.com/umi0410/declarative-argocd)를 포크뜬 뒤 자신의 레포를 통해 Continuouse Deploy하기 위함 ArgoCD를 구축하고 관리하는 방법 쿠버네티스에서는 어떤 서비스를 배포하고 관리해나가는 데에는 다음과 같은 방법들이 있습니다.\nPlain k8s manifest yaml Kustomize Helm Chart \u0026hellip; 이외에도 다양한 방법들이 있을 수 있지만 일단 제가 사용해본 방법들은 위의 3개와 같아요.\n저는 복잡한 형태를 가진 오픈소스를 배포할 때에는 Helm Chart를, 우리 팀의 서비스를 각각 배포할 때에는 가볍게 인자를 수정하는 등의 용도로 Kustomize를 사용하고 있습니다. Plain k8s manifest yaml은 실제 업무나 프로젝트에서는 거의 사용하지 않고, 쿠버네티스 관련 tutorial 등에서 주로 사용하고 있습니다.\n복잡한 형태의 오픈소스를 배포할 때에는 Helm Chart를 사용하는 이유는 Helm Chart는 values.yaml과 같은 설정 파일을 통해 필요한 대부분의 것을 편리하게 설정하게 하고 관리할 수 있기 때문입니다. 오픈소스 프로젝트를 직접 Plain k8s manifest 파일들로 관리하기에는 너무나도 벅차기 때문입니다. Deployment만 해도 한 손가락으로 세기 힘든 경우도 있고, Configmap에 전달해야하는 값도 변경될 수 있는데, 또 새로운 버전이 출시되면 그 manifest 파일들을 하나 하나 비교하는 것도 너무 힘들죠. 그래서 저는 보통 Nginx Ingress Controller, Cert manager, ArgoCD 와 같은 오픈소스들은 Helm으로 관리하는 것을 선호합니다.\n반면 우리 팀 서비스를 배포하는 경우 대부분 Image tag만 변경하면 되는데 이런 케이스에는 가볍게 Kustomize를 사용하는 것이 적절하다고 생각됩니다.\n그래서! 이번 글에서는 ArgoCD를 Helm으로 배포해보려 합니다.\nHelm으로 간단하게 구축해보기 ArgoCD Helm Chart 설치 Helm 으로 ArgoCD를 설치하는 방법은 ArgoCD Helm Chart Github에서 확인하실 수 있습니다. 참 간단합니다.\n대신 저는 namespace를 argocd로 고정하여 설치할게요! 그렇지 않으면 default namespace에 설치더라구요. Release name은 argocd-demo로 하겠습니다.\n$ minikube start 😄 minikube v1.25.2 on Darwin 12.2 (arm64) ... $ kubectl create ns argocd $ helm repo add argo https://argoproj.github.io/argo-helm \u0026#34;argo\u0026#34; has been added to your repositories $ helm install -n argocd argocd-demo argo/argo-cd NAME: argocd-demo LAST DEPLOYED: Sat Apr 23 05:37:32 2022 NAMESPACE: argocd STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: In order to access the server UI you have the following options: 1. kubectl port-forward service/argocd-demo-server -n argocd 8080:443 and then open the browser on http://localhost:8080 and accept the certificate 2. enable ingress in the values file `server.ingress.enabled` and either - Add the annotation for ssl passthrough: https://github.com/argoproj/argo-cd/blob/master/docs/operator-manual/ingress.md#option-1-ssl-passthrough - Add the `--insecure` flag to `server.extraArgs` in the values file and terminate SSL at your ingress: https://github.com/argoproj/argo-cd/blob/master/docs/operator-manual/ingress.md#option-2-multiple-ingress-objects-and-hosts After reaching the UI the first time you can login with username: admin and the random password generated during the installation. You can find the password by running: kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d (You should delete the initial secret afterwards as suggested by the Getting Started Guide: https://github.com/argoproj/argo-cd/blob/master/docs/getting_started.md#4-login-using-the-cli) 대부분의 경우 helm chart를 install한 뒤 표시되는 안내문이 많은 도움이 됩니다.\n1번 - 브라우저에서 ArgoCD를 열기 위한 Port forwarding 설명이에요.\n2번 - Ingress나 TLS Termination 관련 설명이에요.\n나머지 - admin과 랜덤 패스워드에 대한 설명이에요.\n브라우저로 로그인해보기 자동으로 생성된 admin 비밀번호를 얻은 뒤 콘솔에 접속하기 위해 Port forward한 뒤 접속해볼게요.\n$ kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d 5VYjI5YcfOo6XMHx $ kubectl port-forward service/argocd-demo-server -n argocd 8080:443 Forwarding from 127.0.0.1:8080 -\u0026gt; 8080 Forwarding from [::1]:8080 -\u0026gt; 8080 로그인 화면 TLS 설정을 올바르게 해주지 않았기 떄문에 브라우저에서 인증서나 HTTPS 문제가 경고될 수 있어요. 일단은 \u0026lsquo;무시하고 페이지 진행하기\u0026lsquo;와 같은 버튼들을 통해 진행해줍시다. 로컬에서 데모해보는 것이니까요. 그럼 벌써 손쉽게 로그인 창이 뜬 것을 볼 수 있고, 아까 조회한 admin에 대한 패스워드를 통해 admin / {{YOUR_PASSWORD}}로 로그인해줍니다.\n콘솔 화면 🎉 짜잔~! ArgoCD 콘솔 화면이 등장했어요!!!\nApplication을 통해 CD 동작을 테스트해보기 ArgoCD에는 Repository, AppProject, Application 과 같은 개념들이 존재합니다.\nRepository - Private Repository 및 다양한 Repository에 대한 설정들을 담당해요. AppProject - Application이 속할 수 있는 그룹으로 해당 Project내의 Application들이 수행할 수 있는, 혹은 Project Token이 수행할 수 있는 동작을 제한할 수 있어요. Application - 원하는 k8s 리소스들을 어떤 식으로 CD할 지에 대한 설정이에요. Application이 가장 중요해요~! Private repository를 이용하기 전까지는 아마 Repository가 필요하진 않을 거고, default project 외의 프로젝트를 생성해서 사용할 게 아니면 AppProject도 필요하진 않을 거에요.\n다만 Application은 필수로 새로 생성해야 CD 동작을 확인해볼 수 있어요!\nArgoCD에서 쓰이는 대부분의 리소스는 쿠버네티스 CRD(e.g. Repository, AppProject, Application)로 관리되고 그 외의 것들도 configmap에 설정을 기입함으로써 설정할 수 있습니다!\n따라서 우리는 ArgoCD CRD 중 하나인 Application을 만들어볼 거에요. 공식 문서의 Getting Started에서는 직접 ArgoCD CLI로 Application을 만들지만 ArgoCD CLI나 콘솔 화면을 통해 Application을 만들게 되면 추후에 Application을 yaml 코드로 나타내어 선언적으로 이용하려는 경우 기존의 Application을 코드로 나타내려면 어떻게 해야하는지 혼란스러우실 수도 있습니다. 따라서 저는 직접 Application을 yaml로 작성해서 적용해주겠습니다.\n하지만 만약 아직 ArgoCD가 낯서시다면 단순히 콘솔에서 직접 Application을 생성하는 방법도 추천드립니다. 이후 어떤 설정들이 있는지 감을 잡아보시고 그런 설정들을 어떻게 CRD(yaml)로 나타내는지를 찾아나가며 IaC로 관리하는 것도 하나의 방법이라고 생각합니다.\n제가 본 게시글을 위해 만들어놓은 Repository(https://github.com/umi0410/declarative-argocd)를 Fork하셔서 사용하시면 되겠습니다. 그리고 아래 항목들을 잘 수행해주세요.\n⚠️ 아래의 kubectl apply 수행 시 URL을 본인의 Github username 혹은 Organization name으로 변경하기 ⚠️ fork 뜬 Repository의 getting-started/argocd/application.yml 파일에서 spec.source.repoURL 값을 자신의 fork 뜬 Repository URL로 올바르게 수정하기 fork를 뜨는 이유는 이후에 GitOps 방식으로 CD하기 위해 image tag를 변경해서 푸시한 뒤 업데이트 되는 모습을 보기 위함이에요!\n저희는 샘플 프로젝트로 Guestbook(방명록) 프로젝트를 배포하고 확인해볼거에요.\n$ kubectl apply -n argocd -f https://raw.githubusercontent.com/umi0410/declarative-argocd/master/getting-started/argocd/application.yml guestbook 앱이 잘 등록된 모습 그럼 위와 같이 guestbook 앱이 등록되어 sync된 모습을 볼 수 있습니다.\n$ kubectl port-forward service/guestbook-ui -n default 8888:80 Forwarding from 127.0.0.1:8888 -\u0026gt; 80 Forwarding from [::1]:8888 -\u0026gt; 80 마찬가지로 방금 배포한 guestbook이 잘 동작하는지 확인해보기 위해 guestbook-ui 서비스를 port forward해주겠습니다.\nks-guestbook-demo:0,1 0.1 버전으로 설치했다면 위와 같이 검은 타이틀의 Guestbook UI를 확인하실 수 있을 것입니다!\n그럼 과연 다른 버전의 이미지로 업데이트하려면 어떻게 해볼 수 있을까요?!\n지원되는 이미지 태그 목록.png 지원되는 이미지 태그 목록은 이곳에서 확인하실 수 있습니다.\n우리는 GitOps를 위한 도구인 ArgoCD를 이용하고 있고 guestbook application은 Auto Sync 설정을 해놓았기 때문에 단순히 image tag를 0.2로 변경한 뒤 푸시해주기만 하면 됩니다!\n그럼 fork 뜨신 레포지토리에서 getting-started/k8s/deployment.yml의 spec.template.spec.containers[0].image tag 값을 0.1 -\u0026gt; 0.2 로 변경해주시고 푸시해주세요.\nks-guestbook-demo:0.2로 배포된 Pod 위와 같이 Pod의 컨테이너의 이미지 태그가 0.2로 변경된 것을 확인할 수 있고\nks-guestbook-demo:0.2 기존에는 검정색 타이틀의 \u0026ldquo;Guestbook\u0026rdquo; 이었지만 이제는 알록달록한 \u0026ldquo;Fancy Guestbook\u0026rdquo; 이 된 것을 확인할 수 있어요~!\n이런 식의 작업 방식에서 불편한 점들 성공적으로 CD 작업을 수행할 수 있었습니다. 하지만 실제로 ArgoCD를 도입하고 꾸준히 사용하기 위해서는 방금과 같은 방식으로 작업하게 되면 몇 가지 불편 사항들이 존재할 수 있어요.\nhelm으로 ArgoCD를 설치한 뒤 추가적으로 수동으로 Application을 등록해줘야 합니다. 이렇게 되면 또 다른 클러스터에 동일한 설정으로 ArgoCD를 설치하거나 재구축할 때 많이 번거로울 수 있어요. 이후에도 Application을 새로 생성할 때나 변경할 때마다 kubectl을 직접 수행해야합니다. 우리는 GitOps의 형태로 ArgoCD를 통해 Deployment, Service, Configmap 등을 관리하고 있지만 Application은 직접 kubectl로 관리하고 있어요. App of App 패턴을 통해 좀 더 선언적으로 관리하기 따라서 위와 같은 불편 사항들을 해소하기 위해서 ArgoCD에는 App of App이라는 패턴이 존재합니다. App of App 패턴이란 ArgoCD로 관리할 Application들을 관리하는 우두머리 Application을 두는 패턴을 말해요.\n공식문서에서도 관련 내용을 찾아볼 수 있어요. 예를 들어 App of App 패턴을 이용하면 다음과 같이 다른 앱들을 관리할 수 있게 돼요. 그럼 위에서 2번으로 언급한 문제는 어느 정도 해결할 수 있지만, 그럼 그 우두머리 App은 어떻게 생성/관리할 것인가(1번 문제)가 여전히 문제에요.\n저는 이 문제를 우리가 직접 App을 생성/관리하는 것이 아니라 ArgoCD helm chart의 추가적인 설정을 통해 선언적으로 관리함으로써 해결하려합니다.\nArgoCD Helm Chart README.md에 소개된 부분 설정값 예시는 이곳에서 확인하실 수 있습니다.\n즉, ArgoCD Helm Chart를 통해 관리할 Application을 values.yaml에 작성할 수 있다는 것입니다.\n그럼 getting-started/values.yaml의 server.additionalApplications[0].source.repoURL을 마찬가지로 본인이 fork 뜨신 레포지토리의 URL로 변경해주시고 아래의 명령어로 helm chart를 업그레이드해주세요.\n$ helm upgrade -n argocd argocd-demo argo/argo-cd -f getting-started/values.yaml ... 이후에는 아래와 같이 app-of-app이라는 guestbook Applicaiton을 관리하는 Application이 생성된 것을 보실 수 있을 거에요 :)\napp-of-app 패턴이 적용된 모습 app-of-app은 guestbook application을 관리하고 있습니다. 만약 추가적으로 Application을 생성하거나 삭제하고 싶다면 직접 kubectl을 통해 관리할 것이 아니라 변경하고싶은 사항을 getting-started/argocd에 작성한 뒤 Git push하면 ArgoCD가 알아서 적용해주게 됩니다! 대단하죠?!\n그럼 위의 작업을 통해 우리가 앞서 언급했던 불편들이 해소되었는지 확인해볼게요. 제가 바라는 건 백지 상태의 클러스터에서도 제가 선언해놓은 설정 파일들만을 통해 helm install 명령어 한 줄만으로 현재와 동일한 ArgoCD 상태, guestbook 상태를 갖는 것이에요! 아주 선언적이죠?!\n$ minikube delete Deleting \u0026#34;minikube\u0026#34; in docker ... 🔥 Deleting container \u0026#34;minikube\u0026#34; ... ... $ minikube start 😄 minikube v1.25.2 on Darwin 12.2 (arm64) ✨ Automatically selected the docker driver ... # ArgoCD나 guestbook 관련 Pod나 리소스가 없음을 확인해볼게요 $ kubectl get pod --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-64897985d-pd57n 1/1 Running 0 36s ... $ kubectl create ns argocd namespace/argocd created $ helm install -n argocd argocd-demo argo/argo-cd -f getting-started/values.yaml NAME: argocd-demo LAST DEPLOYED: Sat Apr 23 07:27:22 2022 NAMESPACE: argocd ... 그럼 시원하게 minikube를 통해 구축한 쿠버네티스 환경을 날려버리고 다시 생성한 뒤 ArgoCD를 helm chart로 깔아볼게요. 이번에는 helm chart 설치 시 getting-started/values.yaml을 설정파일로 이용해야합니다.\nhelm install이 완료되면 동일한 방법으로 새로운 Admin 패스워드를 조회한 뒤 다시 port forward해주시고 콘솔에 로그인해주세요.\n선언적으로 관리된 ArgoCD. guestbook은 Sync 중이에요 :) 선언적으로 관리된 guestbook 🎉 와우.. 새로운 클러스터에서도 우리가 선언적으로 작성한 코드들만 있다면 위와 같이 기존과 동일한 상태로 구축이 가능하네요! 좋습니다 좋아요~\n마치며 ArgoCD를 Helm으로 install 해보기 Application을 생성하고 guestbook 프로젝트를 통해 CD 테스트해보기 그 과정에서의 불편이나 문제사항을 발견해내고 선언적으로 해결해나가기 App of App 패턴 이용하기 ArgoCD Helm Chart가 지원하는 additionalApplications 설정을 이용하기 이번 글에서 위의 과정들을 통해 ArgoCD를 어떻게 좀 더 선언적으로 관리할 수 있을지 또 그런 작업을 수행할 때 자료들을 어디서 찾아볼 수 있는지에 대해 알아봤습니다.\n좀 더 제가 어떻게 자료를 찾아나가는 지 그런 요령이나 과정들을 적어보려했는데 글이 길어지면서 기능 자체의 내용에 집중하게 된 감이 있네요 🥲..\n다음 편에서는 ArgoCD에서 RBAC 설정을 해보면서 \u0026lsquo;어떻게 새로운 기능들을 찾고 적용해나가는지\u0026lsquo;에 좀 더 포커스를 맞춰보려해요. 또한 이런 과정들은 ArgoCD 뿐만 아니라 Kafka, ELK, Jenkins 등의 오픈소스들을 사용할 때에도 도움이 될 수 있을 것이라 생각합니다. \u0026lsquo;아~ 이런 식으로 자료를 찾아볼 수 있구나\u0026rsquo; 하고 말이죠 ㅎㅎ\n이번 글이 ArgoCD를 처음 이용해보시는 분들, 이용해오면서 조금 불편하셨던 분들, 새로운 오픈소스를 어떻게 학습하고 적용해나갈지 잘 모르겠는 분들께 도움이 될 수 있는 글이었기를 바랍니다! 감사합니다~ 😆\n","date":"2022-04-22T14:11:07+09:00","image":"https://umi0410.github.io/blog/devops/declarative-argocd-helm-app-of-app/logo_huc6aa4a846d689b08bb4f93a6ab036ab8_63751_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/devops/declarative-argocd-helm-app-of-app/","title":"ArgoCD 선언적으로 이용해나가기 - Helm, App of App"},{"content":"시작하며 저번 편에서는 Key-Value DB와 Document DB를 정리해봤고 이번 편에서는 Column Family DB와 GraphDB에 대해 정리해보려한다.\nColumn Family DB나 GraphDB는 실제로 사용해본 적이 한 번도 없어서 어떤 녀석들인지 자세히는 모르지만 책 내용 위주로 정리해보도록 하겠다.\nColumn-Family DB Column-Family DB는 한 Row에 무수히 많은 Column을 Schema가 강제되지 않은 형태로 이용할 수 있는 NoSQL의 한 종류이다. Wide column DB, Wide column store, Columnar DB 등의 다른 다양한 명칭들도 존재는 하지만 책에서는 좀 더 여러 컬럼을 묶어서 생각할 수 있는 점이 강조되는 Colum-Family DB라는 용어를 이용했다고 한다.\nColumn-Family DB는 다른 일반적인 NoSQL과 관계형 데이터베이스의 사이의 그 무언가 같은 느낌인데 이는 NoSQL임에도 관계형 데이터베이스와 비슷한 몇 가지 특징들이 있기 때문이다.\nSQL과 유사한 쿼리 언어를 제공하기도 한다. Row와 Column 등의 개념이 존재한다. 그 외의 특징은 다음과 같다.\n자주 함께 사용되는 Column들을 묶어 Column family로 이용할 수 있다. Column family는 약간 RDB에서의 하나의 Table 같은 느낌으로 볼 수도 있다. 개인적으로는 Column family DB에서 하나의 Row에 해당하는 데이터들은 이미 JOIN이 수행된 데이터들의 모임 같다. 사실 2차원적인 테이블이 아닌 중첩된 맵 구조이다. SortedMap\u0026lt;Row Key, SortedMap\u0026lt;Column Key, Column Value\u0026gt;\u0026gt; 같은 느낌으로 볼 수 있다. 단일 Row에 대한 전체 데이터들은 디스크에 저장될 때 서로 인접하지 않을 수도 있다. 하지만 하나의 Column-Family에 대한 데이터들은 인접해서 저장된다. Schema가 강제되진 않음. JOIN을 지원하지 않는 것 같다. 엄청나게 많은 데이터, 높은 확장성, 높은 가용성에 특화된 듯하다. 컬럼값들의 버전 관리가 가능하다. 확장과 가용성에 특화된 DB이다보니 마스터가 SPOF (단일 실패 지점)이 될 수 있는 형태를 지양한 듯하고, Column-Family DB는 P2P 형태로 어느 누구 하나가 마스터이지 않고 평등하다.\n따라서 클러스터 내 서버들의 상태 공유, 데이터 최신화 및 복제 등 기존에 마스터가 수행하던 작업을 동료 Node들끼리 알아서 수행해야하고 그를 위한 커뮤니케이션이 필요한데 이때 동료 Node들이 많아질 수록 커뮤니케이션이 기하급수적으로 늘어날 수 있다. 이런 커뮤니케이션을 효율적으로 하기 위한 프로토콜들도 생겨나게 됐다. 간략히만 정리해보자면 그 중 하나가 “가십 프로토콜”이며, “각 노드가 다른 모든 노드들의 상태를 직접 체크하는 게 아니라 각자가 몇 몇 노드의 상태를 직접 체크하고 나머지 노드의 상태는 서로 주고 받은 정보를 참고한다” 정도로 정리해볼 수 있을 것 같다.\nColumn-Family DB는 주로 빅데이터 분야에서 많이 쓰이는 것 같고, 빅데이터를 다룰 때에는 다음과 같은 작업들이 필요하다.\nETL (Extract, Transfrom, Load) - 데이터의 추출, 적절한 형태로 가공, 적재 데이터 분석 DB 성능 모니터링 Column-Family DB는 위와 같은 기능들을 잘 제공하고 있다. 반면 주로 제품(비즈니스) 로직을 위한 데이터 저장은 관계형 데이터베이스, Key-Value DB, 문서 DB를 많이 이용하는 것 같기는 하다.\n하지만 간단히 정리하려다가 다시 궁금증이 생겨서 자료를 좀 찾아보던 중 컬럼 패밀리를 제품 로직에서 사용하는 케이스에 관해 eBay의 상품 좋아요 기능을 Cassandra를 기준으로 데이터 모델링하는 것 관련 좋은 글을 발견하게 되어 이 글을 참고해보면 좋을 것 같다. 요약은 다음과 같다.\n요구되는 조회 패턴을 근거로 올바른 수준의 비정규화를 하는 것이 중요하다. 너무 정규화 되어있으면 필요한 정보를 찾기 위해 한 번 더 조회가 필요할 수 있고 비효율적이다. 너무 비정규화 되어있으면 불필요하게 수많은 데이터가 중복되게 되고, 수정 사항 반영이 쉽지 않을 수 있다. Graph DB GraphDB는 로우, 컬럼 등을 바탕으로 데이터를 모델링하는 대신에 노드(혹은 정점, Vertex)와 관계(Relation 혹은 Edge)를 통해 데이터를 모델링한다. 이때 Edge는 방향이 있을 수도 있고, 없을 수도 있다.\n서로 많은 관계를 맺는 케이스에 사용하기 좋은 듯한데, 예를 들면 SNS의 유저 간의 친밀도, 네트워크에서 도시 간의 소요 시간 등을 나타내기 좋다.\nGraph DB에서 모델링할 수 있는 그래프와 그 예시는 다음과 같다.\n방향 그래프 - 조직 구성원의 상하 관계 무방향 그래프 - 조직 구성원의 협력 관계 유동 네트워크 (Flow network) - 도로 시스템, 교통 네트워크. (잘 모르겠습니다.) 이분 그래프 - 각 연결 관계들을 따라 가면 두 개의 집합으로 번갈아 표현되는 그래프 e.g. 유저 - 게시글의 좋아요 관계 유저 - 게시글 - 유저 - 게시글 - \u0026hellip; e.g. 선생님과 학생의 관계 선생님 - 학생 - 선생님 - 학생 - \u0026hellip; 다중 그래프 - vertex ↔ vertex 간에 여러 edge가 존재할 수 있는 경우 e.g. 네비게이션 가중 그래프 - edge에 가중치가 존재하는 경우 e.g. 최단 경로 찾기 그래프 DB를 언제 사용할 수 있을까?\n항상 어떤 DB를 사용할 지, 어떻게 모델링할지는 사용하려는 패턴, 특히 조회 패턴에 근거해서 고려되어야한다.\n예를 들어 다음과 같은 조회 패턴을 가진 경우 그래프 DB를 사용하면 좋다.\n정점 A에서 정점 B로 가는 엣지는 몇 개인가? 정점 A에서 정점 B로 가는 엣지 중 비용이 100 미만인 엣지는 몇 개인가? 정점 A에서 정점 B로 가는 가장 효율적인 경로는 무엇인가? 좀 더 실생활과 밀접한 예시는 다음과 같다.\n유저 A와 친밀도가 높은 유저들은 누구인가? 유저 A와 유저 B가 함께 아는 친구가 존재하는가? 지역 A에서 지역 B까지 이동할 수 있는 경로가 존재하는가? 이렇게 특정 엔티티를 기준으로 해당 엔티티와 엔티티와 다른 엔티티들의 연결 관계를 바탕으로 조회를 하는 경우 그래프 DB를 사용하면 좋다.\n반면, 다음과 같이 엔티티나 테이블 전체에 걸친 질의는 적합하지 않다.\n오늘 받은 좋아요가 가장 많은 게시글은 어떤 게 있는가? 가장 연결된 경로가 많은 지역은 어디인가? 실제로는 Graph DB를 이용하는 유즈케이스는 다음과 같은 운송 회사가 있을 수 있다.\n“제한 시간 내에 운송 가능한 최소 비용 경로를 찾기”\n운송 소요 시간을 가중치로 한 출발 시설 ↔ 도착 시설 간의 연결 관계가 존재 경로를 거쳐가던 중 제한 시간을 초과하게 되면 그 경로는 제외됨. 그 외의 가능한 경로 중 가장 저렴한 경로를 채택! 참고해보면 좋을 것 같은 링크 카산드라 데이터 모델링 part.2 - eBay의 좋아요 모델링 - http://hochul.net/blog/cassandra-data-modeling-best-practices-part-1/ Data modeling 관련 카산드라 문서 - https://cassandra.apache.org/doc/latest/cassandra/data_modeling/index.html 시간 나면 읽어보면 좋을 것 같다. 마치며 간단하게 Column-Family DB와 Graph DB에 대해 정리해봤다. 책 내용 외에도 위에 첨부한 eBay의 Cassandra 모델링 이야기가 꽤 재미있고, 평소 궁금했던 내용을 잘 알려준 것 같다. 책을 읽으면서 알게 됐지만 무심코 지나친 내용들이나 까먹은 내용들이 꽤 있었던 것 같은데 리뷰를 기회 삼아 한 번씩 다시 되돌아 볼 수 있는 것 같다.\n다음 책도 리뷰해보면 좋을 것 같다!\n이걸로 2022년 첫 책 리뷰 끝~!\n","date":"2022-03-17T03:30:00+09:00","image":"https://umi0410.github.io/blog/computer-science/book-review-nosql-introduction-3/cassandra-modeling_hu4c562377df99e1314a765d985ca6b3ef_61429_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/computer-science/book-review-nosql-introduction-3/","title":"(도서) \"NoSQL 철저 입문\" - Column-Family DB, Graph DB"},{"content":"(도서) “NoSQL 철저 입문” - Key-Value DB, Document DB 시작하며 앞선 글에서는 “NoSQL 철저 입문”이라는 도서에 대한 총평과 NoSQL 데이터베이스들이 공통적으로 갖는 특징에 대해 알아봤다.\n이번 글에서는 NoSQL들 중에서도 Key-Value DB와 Document DB이 특히나 서로 유사한 형태를 띄는 것 같아 묶어서 정리해보려한다.\n추가적으로 각자의 고유한 특징은 아니더라도 공통적 특징일 수 있는 Master - Slave(read replica) 구조나 Partitioning, Sharding 등등에 대해서도 정리할 예정이다.\nKey-Value Database 키와 값 뿐인 가장 간단한 형태의 NoSQL 종류이다. 아주 단순한 조회만이 지원된다. 키를 통해서만 조회가 가능하다. 키를 알고 있다면 데이터를 바로 찾을 수 있지만 키가 없다면 풀서치를 해야할 수 있다. Key-Value Database는 간단한 형태의 DB이기 때문에 테이블이나 그래프로 복잡한 관계를 나타내려는 경우에는 부적합하다. 단순 저장과 키를 바탕으로한 조회로 충분한 경우에 가볍게 사용하기 좋다.\n아무래도 단순함은 빠른 속도와도 연관이 되기 때문에 속도가 빠른 편이고 대표적인 예시가 Redis이다. 단순한 동작에 In-Memory Database라는 특징으로 인해 Redis는 엄청난 속도를 자랑한다.\n단, 메모리 같이 작은 크기의 저장소를 이용할 때는 저장소가 가득 차지 않도록 주의해야한다! 주로 이를 위해 주로 eviction policy라는 저장소가 가득 차기전에 데이터를 적절히 삭제하는 정책을 설정한다. 주로 인-메모리 저장소는 캐시로 사용되고, 캐시는 지역성을 근거로 하기 때문에 시간 지역성을 바탕으로한 LRU 관련 정책이 많이 사용된다.\nKey-Value DB는 Key를 바탕으로 조회가 가능하기 때문에 Key에 조회 조건이 포함되어야 한다. 만약 어떤 유저의 최근 알림 내역 정보를 Key-Value DB 중 하나인 Redis에 저장한다고 해보자.\nrecent_notifications:{{username}} = [{{json string}}, {{json stirng}}] 예를 들어 유저가 umi0410이라면 아래와 같이 Redis에 해당 유저의 최근 알림 내역 정보를 저장할 수 있을 것이다. recent_notifications:umi0410 = [ \u0026#34;{\\\u0026#34;id\\\u0026#34;: 1, \\\u0026#34;title\\\u0026#34;: \\\u0026#34;1번 알림입니다.\\\u0026#34;}\u0026#34;, \u0026#34;{\\\u0026#34;id\\\u0026#34;: 2, \\\u0026#34;title\\\u0026#34;: \\\u0026#34;2번 알림입니다.\\\u0026#34;}\u0026#34; ] 위와 같이 데이터를 저장하면 읽을 때에도 특정 유저의 최근 알림 정보를 읽어올 수 있을텐데, 이때 앞서 언급했듯 Key만을 바탕으로 데이터를 조회하게 된다.\nRDB의 경우 주로 의미 없는 Auto Increment ID를 PK이자 Clustered Index로 설정하기 때문에 Write 시에 Auto Increment ID를 바탕으로 디스크에 데이터가 연속된 위치에 저장되도록 하는 편이다. RDB는 PK가 아니라도 다양한 조건으로 값을 검색할 수 있기 때문에 PK는 성능을 고려해 의미 없는 Auto Increment ID를 넣는 것이다.\n반면 Key-Value Database는 Key를 바탕으로 밖에 데이터를 조회할 수가 없기 때문에 Key (RDB의 PK와 유사하게 ID의 역할을 한다고 볼 수 있음)에 의미가 담겨야한다.\n🔵 Key가 recent_notifications:umi0410 처럼 조회 목적에 부합하는 정보를 포함하는 경우 → umi0410 유저에 대한 데이터가 저장되는 key 값을 알 수 있기 때문에 직접 Key 값을 찍어서 데이터를 조회할 수 있음 ❌ Key가 recent_notifications:1 처럼 의미 없는 숫자값인 경우 → umi0410 유저에 대한 데이터를 찾으려면 Full scan을 해야할 수 있음. Key-Value database는 언제 사용하면 좋을까? 책에 나온 Key-Value database 사용 예시를 다시 보니 별로 마음에 안 들어서 내 생각대로 적어보려한다. 기본적으로 Key-Value database는 Key만으로 조회 요구사항을 만족할 수 있어야한다.\nRedis와 같은 In-memory 데이터베이스의 경우에는 낮은 레이턴시가 중요하다거나 수명이 짧은 데이터에 주로 사용한다. 많이 봐왔던 케이스는 캐시나 장바구니 정보, 세션 정보 등이 있는 것 같다.\n유저가 처음 앱에 접속할 때 필요할 만한 알림 개수나 피드 정보등을 캐시해두면 좀 더 빨리 유저에게 정보를 전달할 수 있을 것이다.\n장바구니나 세션 정보 같은 것들은 영구적으로 필요한 데이터가 아니면서 정합성이 엄청 중요하다거나 관계가 복잡한 데이터가 아니기 때문에 간단하게 Key-Value DB를 이용하면 좋을 것 같다.\n“Redis 장바구니”나 “Redis shopping cart”로 구글링하면 자료가 꽤 나와서 나중에 시간 되면 좀 더 찾아보고싶다.\nDocument database Document DB는 Key-Value와 꽤나 유사하지만 Value의 형태가 단순한 값이라기보단 JSON이나 YAML, XML 같은 Semi structured(반정형) 형태를 띄는 데이터베이스를 말한다.\n문서는 Key-Value 쌍의 집합이라고 볼 수 있다. 따라서 데이터 형태가 단순하면 Key-Value DB를, 그보다 좀 더 복잡하면 Document DB를 사용하는 편이라고 한다. 여기에 추가적으로 클러스터가 훨씬 커지고, 한 Key에 관련된 정보가 방대해지면 Columnar DB를 도입하게 되는 것 같다.\nDocument DB는 관계형 데이터베이스와도 유사한 개념을 갖기도 한다.\n관계형 데이터베이스에서는 Row, 문서 데이터베이스에서는 문서가 존재 관계형 데이터베이스에서는 Row의 집합인 테이블, Document DB에서는 문서의 집합인 컬렉션이 존재 단, 관계형 데이터베이스는 같은 테이블 내의 Row들이 모두 동일한 형태를 갖도록 스키마를 강제한다. 문서 데이터베이스는 같은 컬렉션 내의 문서들이 얼마든지 다른 형태를 가질 수 있다! Document DB는 Unstructured data가 아닌 그나마 조금은 구조가 존재하는 Semi structured data를 저장하기 때문에 좀 더 다양한 질의가 가능하다. 예를 들어 Key가 아닌 속성을 바탕으로도 질의를 할 수 있고, 인덱스를 걸 수도 있다.\n단 RDB와 마찬가지로 인덱스 설정을 너무 적게 하면 읽기 성능이 나빠질 수 있고, 인덱스 설정을 너무 많이 하면 쓰기 성능이 나빠질 수 있다.\nBI(Business Intelligence)나 일반적인 웹 서비스는 Write 보다 Read가 훨씬 많다. 반면 로그성 데이터들은 Write가 Read보다 훨씬 많다.\n만약 Read와 Write가 둘 다 많으면서도 짧은 Latency가 중요하다면 Write용으로는 단순 적재형 데이터베이스와 Read 용으로는 수많은 인덱스를 지원하기 용이한 데이터 웨어하우스 등을 이용하는 것이 좋다. 즉 어느 한 가지 DB만을 선택할 것이 아니라 각 DB를 각 필요한 상황에 적용할 수도 있다는 것이다. 주로 쓰기 작업에 최적화된 DB의 내용이 읽기 작업에 최적화된 DB로 전달되도록 구축한 뒤 쓰기 작업과 읽기 작업을 각각 다른 DB를 이용하는 형태가 많이 사용된다고 한다.\n문서 DB에서 관계 나타내기 RDB는 관계를 주로 테이블 간의 참조를 통해 나타낸다. 관계의 종류에는 1 대 1, 1 대 다, 다 대 다가 있다.\n1 대 1과 1 대 N 관계 1에서 완전히 상대 1이나 상대 N 을 완전히 서브 문서 형태로 정의를 할 수 있다. M 대 N 관계 한 쪽이 한 쪽을 완전히 포함할 수는 없어 결국엔 두 개의 컬렉션이 필요하다. 자신과 관련된 상대방 정보는 각자가 저장하기 때문에 양쪽에 데이터가 최신화되고 참조 무결성을 보장하도록 애플리케이션 레벨에서 잘 처리해야한다. 책에는 자신과 관련된 상대 문서들의 ID를 리스트로 관리해 참조하도록 나와있는데 이 경우에도 중첩 문서의 형태로 나타낼 수는 있을 것이라 생각한다. 간단히 Key-Value DB, Document DB에 대해 알아봤다. 이제는 NoSQL에서 노드의 역할은 어떤 것들이 있을지, 각 노드들은 어떤 데이터들을 어떤 로직에 의해 담당하게 되는지 정리해보려한다.\n문서 DB는 언제 사용하면 좋을까? 데이터 형태가 Key-Value에 저장할 정도로 간단하지는 않은 경우 Key 만으로 조회 요구사항을 충족할 수 없는 경우 ACID 트랜잭션까지는 필요하지 않은 경우 데이터의 형태가 정형화되지 않고 조금씩은 달라서 Schema를 준수하기 힘든 경우 관계형 데이터베이스로는 요청을 처리하는 한계가 있어 수평 Scale이 필요할 정도인 경우 요청이 쏟아지는 게 아니라면 사실 관계형 데이터베이스로 많은 상황들을 해결할 수 있고 그 경우 아직은 나는 관계형 데이터베이스를 사용하는 것을 선호한다. 아무래도 역사가 깊어서 레퍼런스도 많고 편리한 ORM 도구들도 많기 때문이다.\n하지만 고정된 스키마를 준수하기 힘들거나 수평 scale이 필요한 경우에는 문서 DB를 사용하는 것이 더 적절하다. 예를 들면 게시판, 댓글, 채팅 메시지등이 될 수 있을 것 같다.\n반면 주문이나 결제 내역, 재고 상황 같은 데이터들은 ACID 트랜잭션이 필요할 수 있어 관계형 데이터베이스를 사용하는 게 좋을 것 같다.\nRDB에서 많이 사용되는 마스터-슬레이브(Read Replica) 구조 쓰기 권한이 있는 Master node는 여전히 한 대, Master node를 복제해 Read 요청만 처리하는 Replica는 여러 대 둘 수 있는 구조 여러 서버를 하나의 서버처럼 사용할 수 있는 형태인 클러스터 형태 치고는 그나마 단순하다. 쓰기에 비해 읽기 요청이 월등히 많은 경우 사용할 수 있다! 노드 간 통신이 많이 필요 없고 주로 복제를 위해 마스터 ↔ 슬레이브간의 통신만 하면 되는 경우가 많다. Columnar DB의 경우 다수의 마스터가 존재할 수 있고, 이 경우 상태 체크를 위해 다수의 마스터 노드 간에 통신이 필요하고, 이를 위해 “가십 프로토콜” 같은 프로토콜이 존재하기도 한다. 나는 AWS를 주로 사용해온터라 RDS를 다룰 때에는 Read replica 얘기가 빠짐 없이 등장해서 친숙했다. RDB는 원래 수평적 Scale이 힘든 편이다. 엄격히도 일관성을 중요시하다보니 분산 환경을 지원하지 않는 경우가 많은 것 같고(확실하진 않음 ㅎㅎ) 지원한다해도 노드 수가 증가함에 따라 성능도 선형적으로 증가하기는 힘들다. 그래도 RDB에서의 수평적 Scale에 대한 니즈도 많이 존재하다보니 요즘은 AWS에서도 Aurora에 Multi master 기능을 붙이거나 RDS에도 최대 15개 갈이의 Read Replica를 지원하기도 한다.\n하지만 아무래도 RDB의 엄격한 일관성이 갖는 한계를 완전히 떨쳐낼 수는 없기 때문일지 손가락 발가락 합쳐서 셀 수 있는 개수 정도 까지만 수평 Scale이 가능한 것 같다.\n그 개수를 NoSQL 중 하나인 Redis와 비교해보자. Redis의 경우는 1000개의 마스터 노드까지도 선형적으로 성능이 증가한다고 한다. 선형적인 증가를 조건으로 했을 때 최대 1000개의 마스터인 것이지 약 16,000여개의 마스터까지도 늘릴 수는 있긴하다. 게다가 각 마스터 노드에 대한 슬레이브까지 구성할 수 있으니 얼마나 확장성에서 차이가 나는지 어느 정도 실감할 수 있다.\n파티셔닝 (Partitioning) 파티셔닝이란 무언가를 분리하는 작업을 말하는데 데이터베이스에서는 주로 데이터를 분리해서 저장하는 경우를 말한다.\n수직 파티셔닝 - 한 테이블을 여러 가상의 테이블로 분리해서 저장하는 것 수평 파티셔닝 - partition key를 바탕으로 분산하여 그에 해당하는 값을 저장하는 것 파티셔닝 예시 RDB에서는 주로 수직 파티셔닝만을 지원하고, NoSQL에서는 굳이 수직 파티셔닝을 이용하기보단 수평 파티셔닝을 이용하는 경우가 많다. (왜 NoSQL에서는 수평 파티셔닝을 잘 사용하지 않는지는 잘 모르겠다.)\nRDB에서는 수직 파티셔닝을 통해 자주 함께 접근하는 컬럼들은은 SSD 파티션에 저장하고, 잘 접근하지 않는 컬럼들은 HDD 파티션에 저장하는 것 같다.\nNoSQL에서는 주로 Partition key로 데이터가 저장될 노드가 결정되고 그 노드에 데이터를 저장한다.\n샤딩 (Sharding) 그중에서도 수평 파티셔닝은 샤딩이라는 또 다른 이름을 갖고 있다.\nAWS ElasticCache의 Shard. 출처: https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Shards.html 파티션 키(샤드 키)를 통해 그 키에 대한 데이터를 담당하는 노드 그룹을 샤드라고 하고, 한 샤드는 Primary 노드만 존재할 수도 있고 Read replica들도 존재할 수도 있다.\n그럼 파티션 키라는 놈을 바탕으로 데이터가 어디에 저장될 지를 구체적으로 어떻게 결정하는 걸까?\nRange Partitioning - 파티션 키의 구간에 따라 어떤 노드가 담당할 지를 미리 선언해놓고, 이를 참조하여 샤드를 정한다. Hash Partitioning - 파티션 키를 인자로 해싱 함수를 수행해 얻어지는 결과값을 샤드로 정한다. 주로 많이 사용되는 접근법은 Range 기반과 Hash 기반 두 가지이다.\nRange Partitioning Range partitioning은 아래와 같이 미리 파티션 키의 구간에 따라 어떤 노드가 담당할지를 미리 선언해야한다. 샤드가 4개 존재한다고 하겠다.\nid: 1 ~ id: 1000은 1번 샤드가 담당 id: 1001 ~ id: 2000은 2번 샤드가 담당 id: 2001 ~ id: 3000은 3번 샤드가 담당 id: 3001 ~ id: 4000은 4번 샤드가 담당 아주 직관적이고 간단하다는 장점이 있다.\n하지만 단점도 존재한다. 예상하지 못한 파티션 키(4000 \u0026lt; id)인 id: 4001인 데이터가 등장하게 되면 어떻게 될까? 올바르게 데이터를 저장하지 못할 수 있다.\n또한 1번, 2번 샤드는 비교적 오래된 유저에 대한 데이터이고 3, 4번 샤드는 최근 유입 유저에 대한 데이터라 각 샤드들에 대한 요청의 수가 상이하다면 어떻게 될까? 올바르게 요청이 분산되지 못하고 특정 샤드들에게만 집중될 수 있다는 문제가 있다.\n또한 결국은 어딘가에 이 Range 할당 정보를 저장하고 그것을 조회해야한다. 공통으로 필요한 데이터가 어느 한 곳에 저장되어야한다는 부분에서 요청이 증가하고 샤드도 확장되는 상황에서 이런 Range 할당 정보를 제공하는 측이 병목이 될 수도 있다.\nHash Partitioning Hash partitioning은 Range partitioning과 달리 어떤 샤드가 어떤 파티션 키를 담당하는지에 대한 매핑 테이블이 존재하지 않는다. 요청자측에서 Hash 함수를 수행한 결과값으로 자신이 요청해야하는 노드를 유추할 수 있다.\n예를 들어 Range partitioning과 같이 샤드가 4개 존재한다고 해보자\nid % 4 == 1 이면 1번 샤드가 담당 id % 4 == 2 이면 2번 샤드가 담당 id % 4 == 3 이면 3번 샤드가 담당 id % 4 == 0 이면 4번 샤드가 담당 클라이언트에 필요한 정보는 샤드 개수 뿐이다. id=4001인 데이터가 등장해도 4001 % 4는 1이므로 1번 샤드가 담당하면 된다.\n지금 예시에서는 Partition key가 id로 numeric한 값을 갖기 때문에 hash 함수를 거치지 않고 모듈러 연산만으로도 원활히 동작하는 것 같지만 실제로는 non-numeric한 값들에 대해서도 지원하고 골고루 데이터가 분산될 수 있도록 하기 위해 해싱 함수들을 사용한다.\n단 Hashing partitioning에도 단점이 존재하는데 바로 수평 Scale시에 각 샤드가 담당하는 데이터가 상이해질 수 있다. 예를 들면 노드가 2대 → 4대로 증가하는 경우를 예로 들어보겠다.\n기존 (노드 2대) id = 1 → 1번 노드 id = 2 → 2번 노드 id = 3 → 1번 노드 id = 4 → 2번 노드 수평 Scale 후 (노드 4대) id = 1 → 1번 노드 id = 2 → 2번 노드 id = 3 → 1번 노드 3번 노드 id = 4 → 2번 노드 4번 노드 위와 같이 데이터를 담당하는 샤드들이 많이 변경되어 rebalance가 필요하다는 어려움이 존재한다. 게다가 이 어려움은 Scale out 때 뿐만 아니라 Scale in 시에도 존재하기도 한다.\n이런 rebalance 과정을 좀 더 최적화하기 위해 consistent hashing이라는 개념이 존재하고 이에 대한 구현체로 virtual node들을 두는 ketama consistent hashing도 존재한다. 이에 대해서는 책에 깊게 등장하진 않고 구글링으로 많은 자료를 찾아볼 수 있기에 이번 글에서는 생략하도록 하겠다.\n마치며 Key-Value 데이터베이스와 Document 데이터베이스의 특징에 대해 적어봤다.\n책 내용을 잊어버리기 전에 한 번 정리해보면서 복습하려했었는데 살짝 까먹어가던 내용이나 대충 읽고 넘어간 부분에 대해서도 점검해볼 수 있었던 것 같다.\n( 책 내용 뿐만 아니라 나중에 복습할 용도로 제가 기존에 알고 있던 내용이나 주관적인 의견들도 조금씩 적어놨습니다. 잘못된 내용이라 생각되는 부분이 있으시면 알려주시면 감사하겠습니다! )\n","date":"2022-03-12T15:30:54+09:00","image":"https://umi0410.github.io/blog/computer-science/book-review-nosql-introduction-2/partitioning_huae423b6d1a3ea56410a819f008ee156e_828970_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/computer-science/book-review-nosql-introduction-2/","title":"(도서) \"NoSQL 철저 입문\" - Key-Value 데이터베이스, 문서 데이터베이스"},{"content":"책을 읽게 된 계기 당근마켓 플랫폼 팀의 서버 엔지니어로 지원하고 면접을 보면서 저장소에 관련한 질문을 많이 받았었다. 저장소는 관계형 데이터베이스나 다양한 NoSQL 데이터베이스들을 의미한다고 할 수 있겠다.\n많은 질문들이 나왔고, 당시의 나는 사실 데이터베이스 쪽 지식은 거의 전무했던 것 같아 제대로 대답을 못한 게 많았던 것 같다. 인턴 근무를 하면서는 내가 익숙했던 관계형 데이터베이스가 아닌 NoSQL 특히 Redis나 DynamoDB 등을 주로 사용하는 모습도 많이 보게 되어 NoSQL에 대해 관심이 가게 되었다.\n‘왜 관계형 데이터베이스는 수평 Scale이 어렵다고들 하는거지?’, ‘왜 NoSQL은 수평 Scale이 용이하지?’\n‘어떤 경우에 관계형 데이터베이스가 아닌 NoSQL을 사용하는 게 적절할까?’\n‘정말 NoSQL이어야만하는 경우들이 존재할까?’\n위와 같은 궁금증을 갖고 있었고, 그런 면에서 다양한 NoSQL 책들을 찾아보다가 하나의 NoSQL에 딥다이브하는 책보다는 다양한 NoSQL 데이터베이스들이 어떤 특징들을 갖는지 비교해주고, 실제 사용 사례는 어떤 경우가 있을지를 알려주는 책을 읽어보고 싶었다.\nNoSQL의 공통된 특징 Key-Value, Document, Column family (혹은 Columnar) 류의 NoSQL 데이터베이스들에 대해서는 추후 각 글에서 정리하는 것으로 하고 이번 글에서는 좀 더 전반적인 NoSQL의 특징에 대해 정리하려한다.\nNoSQL은 관계형 데이터베이스가 아닌 데이터베이스들을 통칭하는 명칭이기 때문에 각각이 다양한 특징을 갖지만 탄생 목적 자체가 SQL 데이터베이스로 만족할 수 없는 유즈케이스를 위해서 였기 때문에 SQL 데이터베이스의 반대되는 쪽으로는 공통적인 특징을 갖는 편이다.\n관계형 데이터베이스와 NoSQL은 절대적으로 누가 더 낫다를 비교할 존재라기 보단 각자가 적절한 목적을 갖는 상호 보완적인 존재이다!\nSchemaless ACID 특성을 갖지 않는다. 좀 억지 네이밍이긴 하지만 BASE 특성을 기반으로 한다. Eventual consistency 분산 시스템, 고가용성, 대규모 트래픽 정규화보단 비정규화와 중복 그럼 각 특징에 대해 알아보자.\nSchemaless 관계형 데이터베이스는 고정된 테이블 스키마를 갖고, 모든 레코드는 그 스키마 구조를 따라야만한다. 그렇기 때문에 데이터 형태가 계속해서 변경될 수 있는 경우에는 불편할 수 있다. 데이터가 적을 때야 그냥 alter table ... 로 스키마를 변경하면 그만이겠지만 데이터와 트래픽이 많은 실제 프로덕션에서는 이런 작업이 쉽지 않다.\n반면 거의 모든 NoSQL 데이터베이스는 스키마를 강제하지 않고 자유롭게 저장할 수 있다.\nACID 특성을 갖지 않는다. A (Atomic) - 한 트랜잭션은 원자 단위로 전체 작업이 성공하거나 전체 작업이 실패해야한다. 부분 작업만 적용되어서는 안된다. 쉽게 말하면 0과 1의 이분법적 사고라고 볼 수 있다. 한 트랜잭션 내의 일부 작업만 수행한 애매한 상태는 존재해선 안된다! C (Consistency) - 데이터베이스에 저장된 데이터는 일관성을 가져야한다. 만약 트랜잭션이 적용되었는데 다음 Read 요청 시 해당 데이터가 아직 반영이 안되어있으면 안된다. 만약 트랜잭션이 롤백되었는데 해당 데이터가 남아있는 상태가 존재하면 안된다. 구글링하다보면 Consistency를 어떤 이유에서인지 column에 대한 제약 조건과 연관지어 설명하는 한국어 자료들을 많이 찾아볼 수 있는데 나는 이는 잘못된 설명이 아닌가 싶다. 특히 NoSQL의 Eventual consistency와 대조해보면 column에 대한 제약 조건으로 consistency를 설명하는 것은 더욱 더 잘못된 설명이라고 느껴지지 않나 싶다. I (Isolation) - 트랜잭션 과정 중에는 서로 다른 트랜잭션 간에는 서로의 과정에 간섭할 수 없다. Isolation은 절대적인 것은 아니고 필요에 따라 얼마나 트랜잭션을 골비시킬 것인지를 의미하는 isolation level을 조절할 수 있다. Read uncommited, Read commited, Repeatable read, Serializable 와 같은 Isolation level들이 존재한다. D (Durability) - commit된 트랜잭션은 시스템이 비정상적으로 종료되더라도 보존되어야한다. ACID는 관계형 데이터베이스에서 제공하는 트랜잭션을 지원하기 위해 가져야할 4가지 특성을 의미한다.\nNoSQL은 보통 이런 ACID 특성을 완벽히 제공하지는 않는다. NoSQL에도 트랜잭션이 존재하기는 하지만 아마 ACID 특성을 모두 만족하는 트랜잭션은 아닐 것이다.\nACID 트랜잭션은 정말 엄격하게 일관된 데이터를 강제하기 때문에 이를 위해선 성능을 많은 부분 양보해야하고 분산 시스템에서는 특히 동작하기 힘들 수 있기 때문이다.\nBASE 특성을 기반으로 한다. BA (Basically available) - 고가용성을 가져야한다. S (Soft state) - 유저의 명시적 명령 없이 데이터가 변경될 수 있다. Eventual consistency에 의해 데이터가 덮어씌워질 수도 있고 TTL에 의해 데이터가 사라질 수도 있다. E (Eventual consistency) - 결과적으로는 일관성을 띈다. 그 과정 속에서 데이터가 일관되지 않은 상태도 존재할 수 있다. 사실 Temporary inconsistency가 좀 더 와닿는 표현일 수도.. 포인트는 *‘결과적으로는 일관성을 갖긴 하는구나~’*가 아닌 ‘일시적으로는 일관되지 않을 수도 있구나!’ 이다. 관계형 데이터베이스는 애초에 엄격하게 일관된 데이터들을 저장하기 위한 솔루션이기에 ACID를 지원하지만 NoSQL은 그런 엄격함으로 인해 확장성이나 성능을 제한받지 않고자 탄생한 솔루션이다.\n여기서 말하는 성능은 작은 데이터에 대한 성능을 의미하는 게 아니라 대규모 데이터에 대해서도 준수한 성능을 내는 것을 말한다. 데이터가 많아질 수록, 요청이 많아질 수록 범용적인 수준의 서버로는 요청을 다 감당하기 힘든데 이렇게 되면 범용적인 수준의 서버를 수평적으로 스케일 할 수 있어야 한다.\n또한 고가용성을 위해서도 수평적으로 스케일할 수 있어야한다. (관계형 데이터베이스의 경우 주로 백업 서버를 대기시켜놓는 이중화를 통해 고가용성을 지원한다.)\n하지만 단일 시스템이 아닌 분산 시스템이 되게 되면 모든 노드들이 일관된 데이터를 갖기가 힘들어지는데 이로 인해 엄격한 일관성이 아닌 Eventual consistency를 제공하게 된다.\n방금 확장성이나 성능, 고가용성에 대한 흐름으로 나온 내용들이 CAP 이론과 관련된 내용이라고 볼 수 있다. CAP 이론에 대해선 따로 정리하지 않겠다.\n정규화보단 비정규화와 중복 나는 모 기업의 기술 면접에서 ‘관계형 데이터베이스에서도 JOIN을 하지 말자는 주장을 내는 사람들도 있는데, 이유가 뭘까요?’ 라는 질문을 받은 적이 있었다. 앞서 말했듯 나는 데이터베이스 관련 지식이 많이 부족하기도 했기 때문에 질문에 대한 대답은 둘째치고 ‘관계형 데이터베이스에서 JOIN을 쓰지 말자는 사람도 있나\u0026hellip;?’ 라는 생각 뿐이었다.\n계속 얘기하지만 관계형 데이터베이스는 일관되고 이상 현상이 없는 데이터들을 저장하기 위한 솔루션이다. 그리고 그를 위해 테이블 간의 관계를 나타낼 때 테이블을 정규화하고 참조한 뒤 조회할 때에는 필요한 정보들을 JOIN으로 가져온다. 하지만 JOIN은 꽤나 비싼 작업이다.\n반면 NoSQL은 어느 정도 데이터의 불일치를 허용하기도 하기 때문에 조회 시 JOIN을 수행하기 보다는 비정규화된 형태로서 중복으로 저장된 데이터를 조회하는 경우가 많다.\n개인적으로는 이렇게 비정규화한 형태로 중복된 데이터를 저장하는 방식은 CQRS와도 어느 정도 비슷한 형태를 갖지 않나싶다. CQRS에서는 데이터를 조작하는 Command 와 데이터를 조회하는 Query의 책임을 분리하는 형태로, Query하는 측에서는 자신들이 조회하기 용이한 형태인 고도화 된 뷰(비정규화된 데이터)로 데이터를 저장한다. 데이터의 원본은 Command를 담당하는 곳에 저장된다.\n마치며 이렇게 NoSQL의 공통된 특징에 대해 책에 등장한 내용과 약간의 개인적인 의견을 정리해봤다. 책이 담고 있는 내용은 꽤나 마음에 든다! 특정한 NoSQL 데이터베이스의 기능 자체를 줄줄이 소개하는 것이 아니라 실용적인 예시 상황과 함께 각각의 NoSQL과 RDB에 대한 비교를 해볼 수 있었다.\n별점을 준다면 4.5점 정도..? 0.5점은 좀 더 예시가 디테일하고 자세했으면 너무나 좋았을 것 같다. 예시가 아예 없는 것은 아니라 간간히 실제 유즈케이스들이 등장하긴 하지만 너무 간단하게만 짚고 넘어가서 조금 아쉬웠다.\n사실 저번 하반기부터 올해 상반기까지 내가 한 게 성장한 게 있나, 새로 배운 게 있나 싶었는데 문득 책을 읽게 된 계기를 회상해보니 저장소 측면에서는 많이 배워가고 있는 것 같아 다행이다.\n‘이런 데이터를 나타내려면 당연히 테이블을 이런 식으로 정규화해서\u0026hellip;’ 와 같이 단순히 관계형 데이터베이스에서 테이블을 설계하는 기본적인 원칙을 기반으로 한 관점이 아니라 ‘이런 데이터는 관계형 데이터베이스가 아닌 이런 이런 NoSQL을 사용하면 더 적합할 것이고, 전체 데이터 조회용으론 이런 NoSQL을 사용하면 더 적합할 것 같다.’ 와 같이 탈 RDB적 관점도 어느 정도 장착하게 된 것 같아 시각이 좀 넓어지고 있는 듯하다.\n작년 2021년 회고에서 2022년에는 짧은 아티클 뿐만 아니라 체계적인 정보를 담은 책을 통해서도 공부를 해보겠다는 결심을 했었는데 2022년 1월이 되자 마자 구매했던 2022년의 첫 책을 포기하지 않고 끝까지 읽게 되어 다행이다! 아마 다음 책은 Real MySQL이나 Spring Boot나 JPA 관련 책이 되지 않을지?! ㅎㅎ\n","date":"2022-03-11T23:46:54+09:00","image":"https://umi0410.github.io/blog/computer-science/book-review-nosql-introduction-1/cover_hue5cdea400e5bd3f90bce9c064a33eba6_152854_120x120_fill_q75_box_smart1.jpg","permalink":"https://umi0410.github.io/blog/computer-science/book-review-nosql-introduction-1/","title":"(도서) \"NoSQL 철저 입문\" 후기 및 NoSQL의 공통적 특징"},{"content":"시작하며 매년 그랬지만 2021년도 참 빠르게 지나간 것 같다. 벌써 연말이라니.\n오늘은 2021년을 돌아보며 잘 했던 부분에 대해선 나 자신을 칭찬해주고, 아쉬웠던 부분에 대해서는 개선할 방안을 찾아봄으로써 새해는 더 뜨겁고 의미있게 보내고자한다~!\n개발 외적인 거는 개인적으로 정리할 계획이고, 본 회고록에서는 개발 관련된 내용 위주로 작성했다.\n2021년 개발 관련 활동 요약 다양한 이유로 미뤄왔던 스프링을 사용해보기 시작했다. 바닥부터 주도적으로 설계/개발해나갈 수 있는 초기 스타트업에 합류해 나름 많은 경험을 해볼 수 있었다. 실제 개발 프로젝트에 여럿 참여해 협업을 해보았다. (교내 사이드 프로젝트, 개발 동아리, 스타트업) CS 지식을 넘어 개발자로서 필요할 만한 내용들을 많이 접하고 공부했다. DB, Transaction, Cache, Redis, Concurrent programming, TDD MSA, DDD, 메시지 큐, CQRS, SAGA 등에 대해 AUSG(AWS 대학생 동아리)와 SOPT(개발 동아리)에서 진행한 세미나에서 ElasticBeanstalk 관련 세션을 진행했다. 취준을 시작했다. AWS SAA 자격증을 공부 중이다. 백엔드/클라우드 분야에 있어 대부분의 기술 세미나와 기술 블로그 글들을 어느 정도 다 이해할 수 있어졌다. 무엇보다 올해에는 마음이 맞는 소중한 사람들을 많이 만나게 된 것 같아 감사하다. 좋았던 점 스타트업에 합류해 많이 성장할 수 있었다!\n작년까진 대부분의 프로젝트를 혼자해왔던 것 같은데 그러다보니 기획, 디자인, 프론트 등에 너무 에너지를 뺐겨서 백엔드에 집중하기가 힘들었던 것 같다. 이런 점에서 아쉬움을 느꼈고, 최대한 실무와 유사한 경험을 해보고 싶다. 그래서 제대로 사이드 프로젝트를 구하는 플랫폼에서 팀을 구해보고자 했다.\n당시에 개발 동아리가 아닌 팀을 따로 구하고자 했던 이유는 사실 개발 동아리는 인터뷰 과정을 거쳐 팀을 정할 수 있는 게 아니라 보통 동아리원들 중 랜덤으로 팀 빌딩이 이루어지다보니 사실 팀 빌딩에 있어 운이 많은 부분을 차지하지만 내 프로젝트를 그렇게 운에 맡기고 싶지 않았기 때문이었다. 실제로 대부분의 동아리가 한 1/3 정도는 프로젝트나 협업이 원활히 이루어지지 않는 것 같다.\n어쨌든 나름의 그런 판단하에 개발 동아리보단 따로 사이드 프로젝트 팀을 구해서 합류하는 게 더 좋은 팀원들을 만날 수 있을 것 같았고, 결과적으로는 사이드 프로젝트가 아니라 초기 스타트업에 합류하게 되어 학생으로서는 적지 않은 급여를 받으며 주도적으로 개발해나가고 많이 성장할 수 있었던 것 같다.\n나는 이 팀에서 개발 리드를 하며 클라우드 인프라 설계/구축, Spring Boot RESTful API 서버 개발, 프론트 일정 관리 및 QA 등의 업무를 담당했다.\nSpring도 JPA도 테스트 코드도 처음이었는데, 잘 적용해볼 수 있었다. 지인들과 스터디 그룹을 만들어 토의하고 인프런에서 강의를 수강하며 기술을 공부하고 적용해나갔다. 서비스를 개발하면서 N+1 쿼리의 문제점 같은 부분들을 실제로 경험해볼 수도 있었고 개선해나가면서 Latency가 낮은 서비스 제공하기위한 전략들에 대한 관심도 갖게 된 것 같다.\nCS 지식을 넘어 개발자로서 필요할 만한 내용들을 많이 접하고 공부했다.\n‘MSA에 관심있다, 관심있다.’ 말만 하다가 지인이 선물해준 “마이크로서비스 패턴”이라는 책을 시작으로 DDD라는 주제와 조금 더 자세한 MSA에 대해 알게 되었다.\n실질적으로 마이크로서비스들이 어떻게 낮은 의존성과 느슨한 결합으로 동작할 수 있는지 공부해보다보니 메시지 큐와 CQRS, SAGA와 같은 내용들도 알게 되었다.\n그리고 캐시나 테스트 코드들은 막연하게만 알고 있던 상태였는데 실제로 도입해보면서 주니어 치고는 썩 나쁘지 않은 정도까지는 알게 된 것 같다!\n그리고 기술 관련된 아티클을 읽는 걸 참 좋아하는데 열심히 읽은 내용들이 그냥 스쳐지나가는 것이 아쉽기도 하고 남들은 어떻게 생각하나 궁금해서 아티클 내용에 대한 요약이나 내 생각을 적어나가기 시작했다. 이렇게하니 확실히 내용이 잘 기억나기도 하고, 가끔 본인의 생각을 공유해주시는 경우도 있어서 그를 통해 배워나가기도 한 것 같다.\n이런 공부들을 바탕으로 결과적으로는 과거에 내가 꿈꿨던 개발자 상에 좀 더 가까워진 것 같다. 당시엔 대부분의 기술 세미나가 오프라인으로 진행됐었고 나는 뭐라도 배워보려고 세미나에 참여했으나 대부분의 내용을 이해하지 못했고, 주변을 둘러보며 ‘아니\u0026hellip; 여기있는 사람들은 저 얘기를 다 이해할 수 있는건가..? 대단하다’라는 생각을 가지곤했다.\n그래서 나도 나중에는 이런 내용을 다 이해하고 공감할 수 있는 개발자가 되고 싶었다. 그리고 올해를 마무리하며 돌이켜보니 이제 대부분의 기술 세미나는 무리 없이 이해할 수 있었던 것 같아 뿌듯하다. +_+\n올해에 재미있게 봤던 세션들은 대충 다음과 같았던 것 같다.\n[2019] PAYCO 쇼핑 마이크로서비스 아키텍처(MSA) 전환기 [우아콘2020] 배달의민족 마이크로서비스 여행기 [우아한테크세미나] 191121 우아한레디스 by 강대명님 Google I/O 2012 - Go Concurrency Patterns 당근마켓의 고언어 도입기, 그리고 활용법 [NHN FORWARD 2021] Redis 야무지게 사용하기 생각을 공유하고 공감할 수 있는 소중한 사람들 만나게 됐다.\n사실 그동안은 이런 저런 이유로 전역한 뒤로는 나랑 잘 맞는 사람들을 만날 기회가 적었다. 아무래도 학교가 고립되어 있고 너무 어렸던 게 주된 원인이었던 것 같다.\n하지만 올해에는 잘 맞는 사람들을 만나 함께 취업 준비를 하면서 각자가 가진 커리어에 대한 가치관이나 가고 싶어하는 회사 기준을 공유하면서 서로 자기 자신에 대해서도 잘 알아갈 수 있었던 것 같고 많은 힘이 됐던 것 같다.\n개발하다가 드는 궁금증이나 이해 안되는 점을 마음 놓고 공유할 수 있는 그룹이 생겼다. ㅎㅎ.\n‘왜 세션 말고 JWT를 쓸까’, ‘데이터랑 트래픽이 얼마나 커져야 RDB가 감당하기 힘든 수준이 될까’, ‘인터페이스와 구현체로 구분할까 그냥 구현체만 쓸까’ 이런 내용들에 대해 자주 대화를 나눴던 것 같은데 딱 정해진 정답이 있는 질문들은 아닌 것 같다. 하지만 그에 대해 혼자서 고민만 하기보다 ‘나도 그거 궁금해’ 하면서 같이 고민하고 공부해나가다보니 훨씬 더 재밌더라!\n아쉬웠던 점, 개선할 점 선택과 집중을 잘 하지 못했던 것 같다.\n개인적인 일로 슬럼프가 찾아왔던 것 같고 그를 끊어내려고 이래 저래 일을 벌렸던 것 같다. 그러다보니 한 활동이나 프로젝트에 몰입하지 못하고 병렬적으로 진행하면서 약간 늘어지게 된 것 같은데, 확실하게 하나씩 하나씩 쳐냈으면 더 좋지 않았을까 싶다.\n그리고 프로젝트 뿐만 아니라 공부에 있어서도 나름 깊게 파는 걸 좋아한다고 생각했는데 면접 준비를 하면서 내가 아직 많이 부족하단 걸 느끼게 됐고, 그동안 선택과 집중을 잘 하지 못했다는 걸 알게 되었다.\n클라우드 인프라 조금, 컨테이너 조금, Java spring 조금, DB 조금, MSA/DDD 조금, 캐시/Redis 조금, Go 조금, \u0026hellip; 뭔가 다 두루두루 알고는 있는데 클라우드 인프라를 제외하면 깊이가 그리 깊진 못했던 것 같다. 게다가 사이드프로젝트에서 안드로이드도 건드리고 리액트도 살짝 건드리고 쿠버 생태계쪽도 건드리고 그러다보니 집중이 분산된 것도 있는 것 같다.\n개발을 즐기는 건 너무 좋지만, 결국 프로가 되려면 즐기면서 재밌는 것만 할 수는 없는 것 같다. 때로는 조금 재미가 없는 것도 감수하고 노력해야 프로가 될 수 있는 것 같다.\n그래서 새해에는 관심있는 분야 폭을 정해 책을 통해 좀 더 확실히 다이브를 해보고자한다. 각 분야별로 베스트셀러 같은 책들을 한 권씩은 읽어보고자 결심했다.\nRDB - Real MySQL NoSQL - Real MongoDB NoSQL - 각 NoSQL 비교하는 내용의 어떤 책 하나 DDD/MSA - 도메인 주도 설계로 시작하는 마이크로서비스 개발 Redis - Redis in Action Spring Boot - Spring Boot를 다룬 어떤 책 하나 Go - Effective Go 기타 - 가상 면접 사례로 배우는 대규모 시스템 설계 기초 멘탈이 약했다.\n취업 준비를 시작하기 전부터 멘탈이 많이 흔들렸던 것 같다. 나는 그냥 설계하고 개발하는 게 좋은데 취업을 위해선 코딩 테스트를 준비해야한다는 것부터 스트레스였고 JD를 보면 요구 조건은 물론 우대 사항까지 모두 자신있어야만 할 것 같은 압박도 스트레스였던 것 같다.\n그러다보니 지레 겁을 먹고 지원 자체를 미루거나 지원과 동시에 갑자기 피로감이 몰려오곤 했던 것 같다. 그러다보니 일상 생활이나 공부에도 지장이 있었던 것 같다.\n그래서 새해에는 커리어와 관련해서 좀 더 강한 멘탈로 열정적으로 임하면 더 좋을 것 같다.\n2022 목표 지식의 깊이를 더 깊게 해야겠다 분야별 책 읽기 동영상 강의 꾸준히 시청하기 선택과 집중을 하자 너무 다양한 분야보다는 내가 우선순위에 둔 기술들을 위주로 개수는 적더라도 깊이있게 공부해나가자 운동을 소홀히 하지말자 요 몇 달 운동을 소홀히 했더니 몸이 뻐근해지기 시작한 것 같다. 새해에도 운동을 꾸준히 해야겠다. 자신감 있되 겸손한, 한결 같이 열정적인 개발자가 되고 싶다.\n마치며 올해에는 계속해서 ‘길게 보자’는 말을 되새겼던 것 같다. 지금 당장 내가 코딩 테스트에 통과했는지, 기술 면접에 통과했는지에 집착하기보다는 나 자신이 그러한 지식들을 숙지한 엔지니어가 되는 것을 목표로 하는 것이 더 중요하다고 생각하기 때문이다.\n하지만 사람 마음이라는 게 참\u0026hellip; 머리로는 알아도 ‘혹시라도 구렁이 담 넘어가듯 운 좋으면 어떻게 될 수도 있지 않을까\u0026hellip;?’ 이런 알량한 기대를 하게 되더라.\n새해에는 좀 더 본질적으로 지식이 깊고 실력 있는 개발자가 되기 위해 여유를 갖고 나아가야할 것 같다!\n","date":"2021-12-20T15:46:54+09:00","permalink":"https://umi0410.github.io/blog/thought/2021-review/","title":"2021년 회고"},{"content":"시작하며 Sentry monitoring - 느려진 API Latency 짧은 영상을 바탕으로 배틀을 할 수 있는 서비스를 Spring Boot MVC + JPA로 개발하던 중 위와 같이 API Latency가 처참하게도 느린 요청들이 감지되기 시작했습니다. 그동안은 쿼리 최적화보다는 로직 개발이 더 우선시하다보니 쿼리 최적화를 미뤄왔는데, 이번 기회에 이에 대해 다뤄보려합니다. 간단하게 코드를 짜서 테스트해보느라 Go 언어를 이용했지만 JPA를 이용하든 뭘 이용하든 문제와 그에 대한 해결책의 요지는 동일할 것입니다.\n아무래도 백엔드에서 API를 개발하면서 파일 데이터를 주고 받는 게 아니라면 주로 Latency가 늘어나는 이유는 다음과 같을 것입니다.\n너무 많은 Write 불필요한 Column(필드) 혹은 관계를 Eager Loading. 추가적으로 경우에 따라 N+1 쿼리 문제 발생 필요한 Column(필드) 혹은 관계를 뒤늦게 Lazy Loading. 추가적으로 경우에 따라 N+1 쿼리 문제 발생 요청마다 매번 수행되는 Join이나 일종의 연산들 N+1 쿼리 문제? - N개의 데이터를 조회한 경우, 각 데이터들의 연관 관계를 추가적으로 조회하기 위해 한 번씩 더 조회해야하는 문제\u0026hellip; 아주 악명 높은 문제로 자료를 쉽게 찾아볼 수 있다.\n사실 어떻게 보면 N+1 쿼리 문제가 워낙 악명이 높아 그에 대한 해결책들도 간단하게는 많이 소개되는 것 같기도 합니다. 따라서 이번 글에서는 조금은 특이하게 조회수나 댓글 개수처럼 일종의 Count 기능이 필요한 경우의 N+1 쿼리 문제나 요청마다 매번 수행되는 Join이나 연산들로 인한 오버헤드들을 줄여 최적화하는 방법은 어떤 것들이 있을지에 대한 제 고민을 소개해보려 합니다.\n(이 글은 RDB를 메인 DB로 사용하는 경우를 기준으로 작성했습니다.)\n영상 조회수 같은 Count 성격의 값에 대한 작업을 최적화하지 않으면?! 쏟아지는 영상 조회 이벤트마다 RDB에 데이터를 Write =\u0026gt; RDB에 너무 큰 부하 매번 특정 영상과 관련된 조회 내역을 RDB에서 Join 후 그 개수를 계산 =\u0026gt; RDB에 부하, 느린 작업 RDB에 영상 조회 이벤트를 Write할 때 영상 정보 테이블에 view_count 컬럼을 추가한 뒤 view_count 컬럼에 조회수를 캐시 =\u0026gt; 캐시 정확도 문제, 테이블 정의 변경 필요 영상 조회수 같은 Count 성격의 값을 최적화해주지 않으면 위와 같은 문제들이 발생할 수 있다고 봅니다.\n따라서 영상 조회 이벤트처럼 쏟아지는 데이터는 너무 많은 Write 요청은 RDB가 아닌 NoSQL을 사용하는 게 좋을 수 있을 것입니다.\n하지만 NoSQL에 영상 조회 이벤트를 저장한다 해도 매번 영상 조회 내역을 가져온 뒤 그 개수를 계산하는 방식은 어떤 DB를 사용하던 불필요한 Read도 많이 발생할 것이고, 당연히 느리겠죠. 만약 어떤 영상의 조회수가 10만인 경우 10만개의 이벤트들을 조회한 뒤 그 개수를 이용해야할테니 말입니다. 경우에 따라서는 영상 조회 시 각 영상에 대한 조회 내역을 추가적으로 조회하게 되는 N+1 쿼리 문제를 겪을 수도 있습니다.\n따라서 view_count 같은 column을 추가적으로 두는 건 어떨까싶기도 합니다만 일반적으로는 동시성 이슈로 인해 view_count의 정확도가 떨어지게 될 것입니다. 또한 스키마가 강요되는 RDB의 특성에 의해 테이블 정의가 변경되어야할 수도 있습니다.\n동시성 이슈 - 만약 100명의 유저에 대한 영상 조회를 동시에 처리하는 경우 100개의 스레드는 모두 view_count=0 으로 조회를 한 뒤 자신의 조회 이벤트로 인한 +1을 더해 view_count=1로 업데이트 커맨드를 날리겠지만 사실 view_count는 1이 아닌 100이 되어야 정확한 것이다.\nRedis를 도입해보면 어떨까 Redis는 메모리를 기반으로하는 Key-Value 형태의 NoSQL라서 일반적인 RDB에 비해 아주 빠르면서 싱글 스레드 기반이기 때문에 정확한 Count와 Increment가 가능합니다. 따라서 아까 발생했던 문제들을 다음과 같이 해결할 수 있을 것입니다.\n쏟아지는 영상 조회 이벤트마다 RDB에 데이터를 Write\n=\u0026gt; RDB에 너무 큰 부하 (X), 빠른 Write (O)\n매번 특정 영상과 관련된 조회 내역을 RDB에서 Join 후 그 개수를 계산\n=\u0026gt; RDB에 부하, 느린 작업 (X), Redis에 최근 조회 내역을 저장, 상황에 따라 NoSQL에서 Replicate해서 영구화하는 것도 좋음 (O)\nRDB에 영상 조회 이벤트를 Write할 때 영상 정보 테이블에 view_count 컬럼을 추가한 뒤 view_count 컬럼에 조회수를 캐시\n=\u0026gt; 캐시 정확도 문제 (X), Redis의 싱글스레드 기반의 정확한 계산\n=\u0026gt; 테이블 정의 변경 필요 (X), NoSQL의 유연함 (O)\n이런 이유로 인해 추후에 조회수나 댓글 수, 팔로워 수 등등에 대해 Redis를 이용하면 어떨까 싶은 생각이 드네요~! 그럼 실제로 앞서 소개했던 문제 상황이나 해결 방법들이 각각 성능이 어떨지 직접 데이터와 쿼리를 이용해 실험해보겠습니다.\n실제 실험을 통해 각각의 방식 성능 비교 상황 소개 Local에서 Redis container, MySQL container을 이용. Go언어로 가볍게 애플리케이션 작성 MySQL의 Video 테이블에 1000개의 영상 데이터 존재. 조회수를 캐시해놓은 view_count 컬럼 존재. MySQL의 View 테이블에 약 2천만 개의 조회 내역 존재.(즉 영상 당 수천개의 조회 내역 존재) 인덱스는 잘 걸어놓음. Redis에 key=video_view_count:{{video_id}}, value={{view_count}} 형태로 view_count 캐시 랜덤하게 5개의 영상에 대한 정보를 제공할 것인데 이때 조회수도 포함되어야한다. 비교할 방식들 소개 Select 후 Redis에 캐시된 조회수 이용 영상에 대한 정보 자체는 Video 테이블을 이용해 Read 조회수는 Redis에 캐시된 값을 MGET을 통해 배치(벌크)로 조회해와서 이용 Select 하며 Column에 캐시된 조회수 이용 영상 정보와 조회수 모두 Video 테이블을 통해 바로 Read Select 후 Batch로 In-Query JPA를 이용하면서 흔히 발생했던 N+1 쿼리를 Batch로 해결할 때와 동일하게 Video 테이블 조회 후 View 테이블에서 view.id in (?,?,\u0026hellip;) 의 형태로 In-Query를 이용 Select 후 나중에 각각을 Join N+1 쿼리 문제 그 자체..! 실험 결과 실험 결과 1 🥇 Select 하며 Column에 캐시된 조회수 이용 🥈 Select 후 Redis에 캐시된 조회수 이용 🥉 Select 후 나중에 각각을 Join Select 후 Batch로 In-Query 우선은 어떤 쪽으로든 캐싱을 이용하는 게 참 빠르구나 싶었습니다. 하지만 조금 의아할 수 있는 부분들도 존재하는데요.\nRedis에 조회수를 캐싱한 경우보다 MySQL에 캐싱한 경우가 더 빠르네..? 어떻게보면 당연하겠지만 MySQL에서 Join 없이 view_count를 조회할 수만 있다면 Redis를 거치는 것보다 MySQL만으로 처리하는 것이 더 빠를 것입니다.\n하지만 Redis를 사용하는 이유가 Read 속도때문만은 아닐 것입니다. 앞서 말씀드린 대로 싱글 스레드 기반의 정확한 count가 가능할 것이고, RDB의 테이블 스키마를 변경할 필요도 없죠.\n그리고 무엇보다 영상 조회 이벤트 발생마다 RDB의 특정 row의 조회수를 +1 하여 update 하는 것보다는 redis에서 increment하는 것이 빠를 것입니다! 즉, write까지 고려하면 속도 측면에서도 redis가 빠를 것입니다.\nN+1 쿼리 문제를 야기하는 방식이 Batch로 In-Query하는 방식보다 빠르네..? 이 부분은 정확한 원인은 모르겠지만 아마 추측컨대 Batch 방식으로 한 번에 몇 만개 수준의 너무나 많은 데이터를 카티션 곱으로 조회한 뒤 각 video의 조회수 내역으로 넣어주려다보니 조회 후 연산이 많은 시간을 잡아먹은 게 아닐까 싶습니다. 반면 N+1 쿼리 방식은 각 video에 대해 한 번씩 쿼리한 뒤 바로 그 결과 count 할 수 있으니 이런 특수한 경우(데이터가 엄~청 많은 경우)에는 오히려 Batch 방식이 느릴 수 있는 게 아닐까 싶습니다.\n실제로 조회수가 약 10개 정도인 영상을 똑같이 5개 조회해보니 우리의 일반적인 예상대로 Batch 방식이 N+1 쿼리보다 빨랐는데요! 아마 카티션 곱이 일어났지만 양 자체가 적은 쿼리 결과를 처리하는 것이 redis와의 N번의 통신이 더 느리기 때문이 아닐까 싶습니다.\n조회수를 위해 가장 이상적인 아키텍쳐는?! 상상 속 아키텍쳐 실험 결과에서도 Redis를 사용하는 것이 나쁘지 않은 것으로 보여집니다. 그래서 저라면 정말 이런 식으로 최적화를 해볼 수 있는 기회가 있다면 Redis를 도입해볼 것 같습니다. 그 동안 수없이 고통받아왔던 N+1 쿼리 문제도 어느 정도 해결할 수 있겠죠?! 불필요한 양방향 연관 관계는 최대한 단방향 연관 관계로 제한하고, Count가 필요할 때 각 엔티티마다의 특정 Count 값을 위해 N번의 쿼리를 수행할 필요도 없어질 것입니다.\n하지만 Redis를 도입한다고 만사가 해결되는 것은 아닐 것입니다. Redis는 영속성이 보장되지 않으므로 영상 조회 내역이 증발해버릴 수도 있고, 양이 한정적일 수 있죠. 그래서 저는 단순 Redis 뿐만 아니라 다음과 같은 방식은 어떨까 생각해보고 있습니다.\n최근 영상 조회 내역은 view_histories:{{username}} 형태의 key, {{video_id}} 형태의 value로 1차적으로 Redis에 저장한다. 조회수는 view_count:{{vide_id}} 형태의 key, {{view_count}} 형태의 value로 Redis에 캐시한다. 최근 영상 조회 내역을 redis를 통해 빠르게 읽고 쓸 수 있다. 영상 조회수를 Redis의 Increment로 비교적 정확하게 계산할 수 있다. 유저의 최근 영상 조회 내역을 각 유저별로 TTL을 걸 수 있다. =\u0026gt; Redis 메모리 절약 영상의 조회수를 영상별로 TTL 걸 수 있다 =\u0026gt; Redis 메모리 절약 영상 조회 이벤트 저장 시 Redis의 event:video_viewed 라는 key의 List에 앞서 언급한 조회 내역과 동일한 조회 내역을 저장한다.(메시지 큐에 Enqueue하는 느낌) SQS 같은 Message queue는 조회수 이벤트가 쏟아지는 것에 비해 latency가 느림. 따라서 Redis를 큐로 사용하는 것도 괜찮아보임. Redis event:video_viewed 라는 List에서 조회 이벤트를 뽑아 서버리스한 NoSQL인 DynamoDB에 유저의 조회 내역을 영구적으로 저장한다. 영상 조회 내역을 redis에 캐시할 뿐만 아니라 영구적으로 저장하기 위함. 마치 Redis를 버퍼, Write-back cache로 이용하는 느낌인데 NoSQL이라해도 주기적인 Bulk write은 주기때마다 부담될 수 있음. 그리고 조회 이벤트는 계속 계속 빠르게 확장될 수 있기 때문에 NoSQL 중에도 서버리스인 DynamoDB를 사용하면 어떨까 싶음. (주의: DynamoDB 안써봄\u0026hellip;) (단 위의 아키텍쳐는 제 개인적인 생각일 뿐, 실제로는 어떻게들 사용하시는지 궁금하네요..! 이렇게 직접 한 필드에 대해 복잡한 방식을 이용하기보단 좀 더 단순히 캐싱 프레임워크를 이용하는 것도 좋은 선택지일 수도 있을 것 같구요.)\n마치며 이렇게 Count 성향의 작업을 어떻게 최적화할 수 있을지 상상과 실험을 통해 정리해봤습니다. 중요한 것은 하나의 정답이 존재하는 것이 아니고, 처한 상황마다 천차만별의 솔루션들이 있을테니 각각을 잘 비교해보고 잘 PoC 한 뒤 사용하는 것인 듯합니다.\n캐시나 Redis에 대해 관심이 많은 편이지만, 아직 많이 부족하다보니 좋은 말씀 댓글로 달아주시면 감사히 배워나가겠습니다~! 감사합니다.\n","date":"2021-11-24T03:46:54+09:00","image":"https://umi0410.github.io/blog/optimizing-count-query-strategy/index_hu2e0266c55b356006b8efd84bbe6e386e_394889_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/optimizing-count-query-strategy/","title":"쿼리 최적화하기 - 조회수와 같은 Count 성격의 작업 최적화하기 (N+1 문제 관련, feat. Redis)"},{"content":"시작하며 “오픈 그래프 지원해주세요.” 오픈그래프가 사용된 인스타 DM, 페북 메신저, 카톡 예시 서비스 출시를 앞두고 마무리 작업을 하던 중 잊고 있었던 SNS 관련 기능들이 있었습니다. 그 중 OpenGraph를 통해 \u0026ldquo;공유하기\u0026rdquo; 기능 수행 시에 적절한 메타데이터를 제공해줘야 했습니다. 별로 어려운 기능은 아니지만 이를 제공하기 위해 적절한 방향을 찾기가 쉽지 않았던 것 같습니다. 고민과 삽질 끝에 저는 CloudFront, API Gateway, Lambda 서비스들을 이용하기로 결정했고, Serverless framework를 이용해 Golang 바탕의 마이크로서비스를 개발했습니다.\nOpenGraph? SSR? 본 글은 아키텍쳐나 기술 선정에 대한 내용을 많이 담을 것이라서 정의에 대한 자세한 설명은 생략하겠습니다.\nOpenGraph는 \u0026lt;head\u0026gt; 태그 안에 \u0026lt;meta property=\u0026quot;og:title\u0026quot; content=\u0026quot;배틀팡::짧고 재밌는 동영상 배틀\u0026quot; /\u0026gt; 과 같은 형태로 메타데이터들을 정의해 SNS나 크롤링 봇들이 사이트의 정보를 좀 더 통일된 방법으로 데이터를 수집할 수 있게 하는 것과 관련된 개념입니다. 그리고 SSR(Server Side Rendering)은 클라이언트가 서버에게는 껍데기 같은 파일만 제공받고 실제 페이지의 내용은 Client 측에서 API와 js 등을 이용해 만들어나가는 방식입니다. 아마 대부분의 SPA 앱들은 CSR로 배포/개발 되고 있는 것 같은데요. 그 이유는 프론트엔드를 제공할 때 S3나 CloudFront, Github Page 등 서버의 기능과 완전히 분리하여 제공할 수 있기 때문이라는 점이 크게 작용하지 않을까 싶습니다.\n\u0026ldquo;왜 OpenGraph를 제공하는데 SSR이 필요해?\u0026rdquo;\n크롤링 봇들은 JS를 실행하지 않는 경우가 대다수입니다. 반면 일반적인 CSR 방식의 SPA 앱들은 JS를 이용해 서버에게 API를 호출해 그 응답으로 페이지를 만들죠. 따라서 JS를 실행하지 않는 크롤링 봇은 기대되는 데이터가 서버로부터 제공되기 이전인 빈 껍데기의 파일만 제공받게 됩니다. 따라서 \u0026ldquo;공유하기 링크\u0026quot;는 크롤링 봇이 오픈그래프를 읽을 수 있도록 SSR로 오픈 그래프 메타데이터들이 담긴 HTML을 제공해줘야하는 것이죠!\n오픈 그래프 제공을 위한 SSR(Server Side Rendering) 마이크로서비스 그래서 오픈 그래프를 위한 정보만을 SSR로 제공하는 마이크로서비스를 만들고자했습니다. HTML 파일을 제공하는 시점에 단순 정적 파일 제공이 아닌 로직 수행 후 동적으로 파일을 제공해야한다면 사실 개발 중이던 Spring 서버에서 엔드포인트 하나만 더 파서 개발을 하면 될 수 있습니다. 하지만 이렇게 되면 메인 백엔드 코드에 SSR 관련 페이지 제공 기능이 추가되어야하는데 API 서버와 같은 곳에서 일종의 프론트의 기능을 하는 SSR이 공존하는 형태는 선호하지 않았습니다. 게다가 백엔드인 Spring boot에서도 어떤 요청에 대해 에러로 응답할 때 SSR에 대한 에러 처리로 해줘야할 지 API에 대한 에러 처리로 해줘야할 지 등등 꽤나 개발적으로 골치가 아픈 부분이 많았습니다.\n{ \u0026#34;message\u0026#34;: \u0026#34;올바르지 않은 페이지입니다.\u0026#34;, \u0026#34;error\u0026#34;: \u0026#34;NotFound\u0026#34; } 예를 들면 API에 대한 에러 응답은 위와 같은 형태를 띄는 것이 좋을 것이고, 그래야 프론트엔드에서도 처리를 하기 편할 것입니다. 반면 SSR에 대한 에러 응답은 어쨌든 사용자가 직접 대면하는 페이지이다보니 결국 메인 서비스의 프론트엔드로 리다이렉트가 되어 메인 서비스의 프론트엔드가 처리할 수 있게 해줘야할 것이구요.\n\u0026lsquo;이게 무슨 소리지?\u0026rsquo; 라고 생각하실 수 있는데, SSR로 제공된 페이지는 결국 \u0026ldquo;공유하기\u0026quot;를 통해 링크를 전달 받은 사용자가 직접 접속하는 페이지이면서 동시에 크롤링 봇이 오픈 그래프를 통해 데이터를 수집해가는 페이지이기도 합니다.\n크롤링 봇은 JS를 수행하지 않는다 따라서 올바른 메타데이터가 담긴 HTML 만들기 위해선 SSR이 필요하다. SSR로 제공된 페이지로 유저가 접속했을 때 서비스로 올바르게 접속할 수 있어야한다. 하지만 서비스 자체를 SSR 방식으로 변경하기에는 무리가 있다! 위와 같은 특징들로 인해 오픈 그래프 제공을 위한 SSR 마이크로서비스는 다음과 같이 동작하면 좋지 않을까?! 싶었어요.\n오픈그래프 정보가 담긴 동적으로 생성된 HTML은 SSR 마이크로서비스에 의해 제공된다. SSR 마이크로서비스는 요청이 들어왔을 때 API 서버에게 해당 요청과 관련된 정보를 질의한다. 이때 SSR이 직접 DB(스프링이 이용 중인 DB)를 이용하지 않는 게 좋은 이유는 정말 많겠지만 우선은 다음과 같다. SSR 마이크로서비스가 지지고 볶고 뭘 해도 스프링의 DB 설계에는 영향을 주지 않는다. SSR로 인해 메인 스프링이 흔들리는 것을 방지해준다. 일단 간단하다. 간단한 조회만을 위해 DB를 설정하는 것보다는 필요한 API의 필요한 필드들만 정의해서 API로 조회하는 게 더 쉬울 것이다. 반면 스프링이 API 스펙을 변경할 때 SSR에도 영향이 있을지를 고려해야한다는 것과 추가적인 인프라 관리가 필요하다는 단점도 있긴하다. 고려했던 인프라 및 설계 ElasticBeanstalk Lambda + ALB Lambda + API Gateway + CloudFront 고려했던 인프라는 위와 같았고 결국에는 1, 2번 방식은 단점들이 존재해서 3번으로 선택했습니다!\nOption 1) ElasticBeanstalk 바탕의 인프라 빈스톡 바탕의 인프라 ElasticBeanstalk은 eb cli를 통해 편리하게 배포를 할 수 있고, auto scaling group도 편리하게 관리할 수 있다는 장점이 있습니다. 손 쉬우면서도 안정적으로 관리하기는 딱이죠. 그리고 기존 메인 서버인 스프링 서버도 빈스톡으로 관리를 하고 있었기 때문에 빈스톡에 익숙하다는 장점도 있었습니다.\n내부적으로 빈스톡의 도커 플랫폼에 대한 궁금증도 있었고 Go라면 nano 인스턴스로도 어떻게 잘 굴려볼 수 있지 않을까 싶은 마음에 fiber 프레임워크를 바탕으로 개발을 했습니다. 하지만 아무리 Go라도 생각보다 nano 인스턴스로 굴리기는 힘들었고, micro도 아슬아슬하더라구요.(micro도 빌드할 때 OOM으로 실패함.)\n빈스톡 인스턴스 내부에서 이미지를 빌드하고 태그를 수정해서 배포를 트리거 시키는 방식을 이용하면 빌드 도중의 OOM 이슈는 없겠지만 ECR을 통해 이미지 레지스트리를 관리해줘야하고, 빌드할 때마다 docker-compose.yml에서 이미지 태그를 올바르게 업데이트 시켜주는 과정도 번거로울 것 같았죠.\n스프링이 t3.small로 돌아가고 있는데(스프링은 JAR를 젠킨스에서 빌드) Go를 쓰면서 t3.small을 쓰자니 이건 웬 공유하기 기능하나에 본 서버의 몇 분의 1에 해당하는 리소스를 사용해야하는 이상한 아키텍쳐가 되어버릴 것만 같았죠. 비용도 비용이구요.\n그래서 fiber 개발한 SSR 마이크로서비스를 ElasticBeanstalk에 배포하는 방식은 기각되었습니다.\nOption 2) Lambda + ALB \u0026lsquo;SSR을 위해 서버를 관리하는 건 너무 낭비가 크다.\u0026lsquo;는 생각에 서버리스로 방향을 돌렸습니다. serverless 프레임워크를 기존에 이용하고 있었기 때문에 크게 낯선 기술은 아니기도 했구요.\nLambda를 이용할 때 Go언어 같은 경우에는 AWS Lambda의 콘솔 에디터 화면이 제공되지 않아 좀 불편해 파이썬으로 개발을 할까 싶었는데, 단방향이긴 하지만 API로 밀접하게 메인 API 서버랑 통신을 해야하는 SSR 서비스의 특성상 type을 명확히 정의해서 API 스펙과 비교하기도 편하고, 개발/동작도 안정적으로 할 수 있는 Go를 선택했습니다.\n파이썬은 API에게 응답받은 json을 어떻게 역직렬화할까부터도 고민이었고, 귀찮아서 그냥 json이나 dictionary로 우겨넣어서 개발을 하면 당시에는 편하지만 일주일만 지나도 무슨 코드인지 알아보기 힘들고 API 스펙이 변경되는 경우의 디버깅은 거의 극악의 난이도이기 때문에 파이썬도 배제했습니다.\n하지만 ALB =\u0026gt; Lambda로 요청을 바로 넘기는 경우, ALB에서 넘어온 요청을 처리하는 방식이 너무 간단해 커스터마이징하기가 더 까다로운 형태였습니다. 예를 들면 경로를 바탕으로 라우팅하는 로직을 직접 구현해야했었는데요. 보통의 웹 프레임워크에서 제공하는 /*, /b/* 이런 표현들을 바탕으로 한 경로와 핸들러간의 맵핑이 불가능했던 것입니다. 이유는 쉽게 말해 Lambda에는 string으로 path가 들어오고 응답도 string으로 줘야하는 겁니다. 이 방식은 기존의 웹프레임워크를 이용해 개발하던 방식과는 거리가 멀죠.\n따라서 이 방식도 기각되었습니다.\nOption 3) Lambda + API Gateway + CloudFront Lambda 바탕의 인프라 드디어 최종 채택안! \u0026quot;CloudFront로 HTTP/S 관련 기능을 처리하고, 그 Origin을 API Gateway로 둔 뒤 Lambda로 요청 처리하기\u0026rdquo; 입니다. 사실 API Gateway의 이런 저런 기능이 별로 필요 없었기에 \u0026lsquo;Lambda를 ALB로 노출시킬 수 있으면 간단하게 그렇게 하자\u0026lsquo;라고 생각했던 방식이 Option 1이었습니다. 하지만 람다로 ALB TargetGroup에 대한 요청을 직접 처리하기에는 개발적으로 많은 불편히 있었어요. 정말 간단한 작업용 같은 느낌!\n반면 Lambda가 APIGateway에게서 요청을 전달받으면 아주 편리하고 익숙한 방식으로 개발을 할 수가 있었습니다.\n// 코드 일부를 예시로 가져왔습니다. func init(){ app = fiber.New() app.Use(logger.New()) app.Get(\u0026#34;/health\u0026#34;, func(ctx *fiber.Ctx) error { return ctx.SendString(\u0026#34;OK\u0026#34;) }) app.Get(\u0026#34;/\u0026#34;, handleIndex) app.Get(\u0026#34;/c/:clipId\u0026#34;, handleClip) app.Get(\u0026#34;/b/:battleId/:clipId\u0026#34;, handleBattleWithClips) app.Get(\u0026#34;/*\u0026#34;, handleDefaultNotFound) } func main() { // 람다가 실행된다. // 요청마다 Handler를 호출한다. lambda.Start(Handler) } func Handler(ctx context.Context, req events.APIGatewayV2HTTPRequest) (events.APIGatewayV2HTTPResponse, error) { return fiberadapter.New(app).ProxyWithContextV2(ctx, req) } 위와 같이 람다로 들어오는 요청을 처리하기 위한 핸들러를 찾아 호출하는 로직을 직접 구현하지 않고 기존의 유명한 웹 프레임워크들과 그에 대한 어댑터를 통해 개발할 수 있었습니다! (참고: aws-lambda-go-api-proxy) 람다의 핸들러가 fiber 앱에게 요청을 넘겨서 그 응답을 리턴하는 형태죠. fiber 앱의 핸들러들은 SSR 방식의 동적으로 생성된 HTML들을 리턴해줬습니다.\nawslabs Github - https://github.com/awslabs/aws-lambda-go-api-proxy AWS에서 개발 중인 오픈소스 같습니다. 최근 커밋도 6일 전인 활발한 프로젝트인가봐요! (근데 왜 ALB(TargetGroup) - Lambda에 대한 어댑터나 프록시 오픈소스는 개발을 해주지 않은 걸까요.. 궁금합니다.)\n어쨌든 Lambda + API Gateway를 이용하면서 저렴하면서도 편리하게 개발을 할 수 있었습니다. 여기서 끝나면 좋았겠지만 다음 삽질이 있었습니다.\nAPI Gateway를 커스텀 도메인 네임으로 이용하기. 즉 기본적으로 API Gateway는 AWS 자체적인 도메인 네임을 통해 제공되고, 인증서도 AWS 측에서 제공을 해줍니다. 하지만 Custom domain을 이용하고 싶다면? 우리의 도메인을 인증해줘야겠지요. 하지만 기본적으로는 그렇게 할 수 없고 위의 링크의 방법을 통해서는 가능합니다. API Gateway에 Custom domain name을 달기 위한 추가적인 방법들이에요.\nAPI Gateway doesn\u0026rsquo;t directly support http without TLS, presumably as a security feature \u0026hellip; - \u0026ldquo;Redirect http:// requests to https:// on AWS API Gateway (using Custom Domains)\u0026rdquo;\n하지만 이렇게 API Gateway를 Custom Domain name으로 이용할 수 있게 했더니 이번엔 또 다른 이슈가 존재했죠. 즉, API Gateway는 443 HTTPS만을 지원하고 80 HTTP는 지원을 하지 않고 HTTP-\u0026gt;HTTPS로의 리다이렉션도 지원하지 않는다는 것이었습니다 ㄱ-\u0026hellip; 그럼 어떤 문제냐 있냐\u0026hellip; 해당 HTTP로 링크를 접속하면 아무런 응답이 없습니다. 치명적이죠. 따라서 HTTPS Redirect를 수행할 수 있는 CloudFront를 앞단에 붙이고 API Gateway를 Origin으로 설정하는 방식으로 개선했습니다. 그 최종 아키텍쳐가 위의 그림과 같구요.\n결과 결과적으로 위와 같이 URL이 HTTP 이건 HTTPS 이건 상관 없이 원활히 오픈 그래프를 통해 적절한 정보를 제공할 수 있었고, 이제 정말 출시가 다가오는 것 같아 걱정 반 설렘 반이기도 합니다.\n마치며 SSR로 오픈 그래프 하나 제공하겠다고 일요일 내내 삽질을 하게 된 것 같습니다. 평소에 서버리스는 아무래도 벤더 lock-in 이슈나 개발/디버깅 과정에서 로컬과 실제 클라우드 환경의 차이로 인한 불편 등등으로 인해 별로 선호하지 않았는데 그래도 공유하기 기능은 이제 앞으로 트래픽 걱정을 크게 하지 않고 가볍게 잘 사용할 수 있을 것 같아 좋습니다 ㅎㅎ\n아쉬운 점은 좀 더 각 서비스를 각 서비스의 역할이나 특성에 맞춰 사용할 수 있었으면 좋았을텐데, 단순히 특정 상황을 해결하기 위한 수단으로 이용하게 된 점이 조금 아쉽습니다.\n예를 들어 Lambda를 노출시키기 위해 굳이 API Gateway를 사용하지 않아도 ALB로 간단하게 충분히 노출 시킬 수 있었지만, 개발적으로 API Gateway-Lambda간의 작업이 Adpater framework을 통해 개발하기 편해서 API Gateway를 이용하게 됐죠. API Gateway의 API Key를 이용하는 인증/인가 기능이나 쓰로틀링 제한과 같은 기능들은 하나도 필요가 없는데 말이죠.\n거기에 CloudFront는 사실상 Content Delivery나 캐싱이라는 주용도로 사용되면 좋았겠지만 단순히 API Gateway를 사용하면서 HTTPS로의 리다이렉트를 위해서 사용됐을 뿐이라는 점이 아쉽습니다.\n물론 CloudFront나 API Gateway 같은 서비스들도 기본적으로 켠 만큼 돈이 나가는 게 아니라 호출한 만큼 돈이 나가는 형태이면서도 매우 저렴한 서비스들이기 때문에 지금은 불필요하지만 특정 상황을 해결하기 위한 서비스들처럼 느껴지더라도 후에 저희 서비스가 성장하면 분명히 CloudFront의 주기능인 캐싱 자체의 중요성도 증가할 수도 있을 것이고, 그 정도 규모가 된다면 간단한 서버리스 아키텍쳐가 아닌 좀 더 정교한 구축형 아키텍쳐로 개선할 수도 있을 것 같습니다!\n이상 오픈 그래프를 위한 SSR 서비스 구축기였습니다~!\n","date":"2021-11-07T03:46:54+09:00","image":"https://umi0410.github.io/blog/ssr-serverless-og/architecture_hu5fca6240e5304ba626b4c28b580410af_204723_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/ssr-serverless-og/","title":"AWS Lambda + API Gateway를 통해 SSR로 OpenGraph 메타데이터 제공하기"},{"content":"시작하며 Github PlayStore 출시 프로젝트 및 팀 소개 페이지 학교 수업을 듣다보니 한 학기에 2개 3개의 프로젝트를 개발해야하는 경우가 있었습니다. 완전 개발이 처음이었던 시기에는 이러한 양적인 개발 속에서도 배울 점이 많았지만 어느 정도 지나고 나니 양적인 개발 속에서는 크게 배울 점이 느껴지지 않았고, 머리가 아닌 손으로 개발을 하고 있는 느낌을 받게 되었습니다.\n따라서 저는 단순히 찍어내기 식의 개발을 반복하기 보다는 한 서비스를 꾸준히 개발해나가며 그 속에 많은 생각을 담아내고자 했습니다. 처음에는 가볍게 관심있던 기술들을 자유롭게 적용해볼 수 있는 개발 놀이터를 만들고자 하는 마음에 시작했고 제가 관심있던 기술들은 다음과 같았습니다.\nKubernetes Golang을 이용한 서버 개발 TDD MSA, DDD(Domain Driven Design) ArgoCD를 이용한 CD(Continuous deploy \u0026amp; delivery) 베프 하나와 함께 시작했던 서비스에서 기존에 친분이 없던 디자이너님, 개발자님들이 추가되면서 리더로서 책임감이 조금 생겨나기도 했고, 기간이 길어지다보니 단순히 개발 놀이터보다는 출시를 해보자는 목표를 갖게 되었습니다.\n그리고 대부분의 코드를 오픈소스로 관리하고 있는데 개인적으로는 학생들이 실제 프로덕션 수준의 작업에 참여해볼 수 있는 경험이 많지 않은데 우리 학교 학생들은 자신이 사용하는 오픈소스 서비스인 쿠뮤에 자유롭게 기여함으로써 실제 개발에 참여해볼 수 있는 기회의 장이 되었으면 하는 꿈이 있습니다. 아무래도 학생들이 각자 다른 개발 언어와 관심사를 갖다보니 MSA를 선택한 것이 이 부분에 있어서는 좋게 작용할 수 있을 것 같네요.\n쿠뮤? 뭐하는 서비스지? 쿠뮤는 공지사항 푸시 알림과 교내 커뮤니티를 주로 제공하는 서비스에요! 경희대학교의 흩뿌려진 공지사항을 한 곳에서 조회할 수 있고, 새로운 공지사항은 선택적으로 푸시알림을 받아볼 수 있답니다. 또한 교내 커뮤니티로서 학교 인증이 학교 사이트와 연동되어 간편하고 다양한 주제에 걸쳐 학생들의 의견이 적극 반영될 수 있습니다. 보통의 커뮤니티는 게시판 형태로 구현되어 게시판을 하나 하나 찾아다녀야 하지만 쿠뮤는 마치 인스타에서 해시태그들을 팔로우하듯 게시판을 팔로우 해두면 피드에서 자신이 팔로우 해둔 게시글을 모두 모아볼 수 있어요.\n내가 좋아하는 생각이 담긴 개발 저는 단순하게 손으로 하는 개발보단 이런 저런 생각들이 담겨있는 개발을 좋아하고 그에 대해 자유롭게 얘기나누는 시간을 사랑합니다. 제가 쿠뮤를 개발하면서 했던 고민들과 그런 생각들을 어떻게 녹여냈는지 간단히 정리해보겠습니다.\n\u0026lsquo;RESTful하게 API를 개발해나가려면 어떻게 해야할까?\u0026rsquo; RESTful하다는 말의 정의는 이곳 저곳 다들 다르게 표현하고들 있는 것 같지만 HTTP method로 동작을, URL로 리스소를 나타내는 점은 공통적으로 표현되는 특징인 것 같습니다. 그래서 CRUD는 Post, Get, Patch, Delete 등의 메소드와 연결되죠. 근데 분명 CRUD와 깔끔히 맞아떨어지지 않는 작업들이 존재합니다!\n관련 글\nHow to design a REST API to handle non-CRUD operations? - Stack exchange REST API design for non-CRUD actions, e.g. save, deploy, execute code - Stackoverflow 출처 - https://toss.im/slash-21/sessions/1-7\n위와 같이 토스에서도 RESTful하지 않은 API들에 대해 고민을 많이 했던 것 같고 이 경우 다음과 같은 컨벤션들을 적용한 것 같습니다.\nPath의 마지막에 동작을 나타내는 동사를 적용 이런 API들은 POST 메소드를 이용 # 기존 방식 좋아요 생성 POST /likes {article: 1} 좋아요 삭제 DELETE /likes/{{like_id}} # 개선한 방식 좋아요 POST /articles/1/like 좋아요 취소 POST /articles/1/unlike 실제 쿠뮤 작업을 예로 들자면 게시글에 대한 좋아요 작업 수행 시 위와 같이 API를 좀 더 직관적으로 개선했습니다. 기존에는 좋아요 기능을 구현하기 위해 like라는 리소스로 이용하면서 like 리소스에 대한 생성, 삭제에 맵핑시켰습니다.\n하지만 이는 다소 기계적이고 API를 사용하여 개발하는 클라이언트(사람)입장에서는 와닿지 않을 수 있습니다. 또한 클라이언트는 좋아요를 취소하려면 자신이 취소하고자 하는 좋아요 내역의 ID를 알아야하므로 불편하죠.\n그래서 아래와 같이 API 형태를 개선했고, 클라이언트는 좀 더 편리하게 API를 이용할 수 있어졌습니다~!\n\u0026lsquo;API의 Latency를 줄이기 위해서는 어떻게 해야할까?\u0026rsquo; 개발을 하다보니 데이터가 그렇게 많은 것도 아닌데 어느 순간부터 latency가 길다는 느낌을 받게 되었습니다. 그래서 어떻게 latency를 줄일 수 있을까를 알아보게 됐죠. 주된 원인은 N+1 문제라고 불리는 DB Query가 이슈였던 것 같습니다. 게시글을 여러 개 조회한 뒤 맵핑을 할 때 게시글 하나 하나마다의 댓글 개수, 좋아요 개수, 북마크 개수 등을 순차적으로 조회하다보니 이 부분에서 병목이 발생한 것이었어요!\n저는 이 부분에서 redis를 적용해보았고 redis를 적용하자 상황마다 다르겠지만 대체로 latency가 약 1/5로 줄어드는 효과를 얻을 수 있었습니다. redis를 적용하던 당시엔 N+1 문제와 그것을 Fetch join(혹은 Eager loading)로 해결하는 방법에 대한 인지 자체가 없었어서 redis를 적용했던 것 같은데 Fetch join으로도 글을 쓰면서 생각해보니 간단히 해결할 수 있었을 것 같긴하네요.\n시간이 된다면 redis 뿐만 아니라 fetch join도 적용해서 더 최적화해볼 수 있으면 좋을 것 같아요. 그래도 redis를 써보면서 많은 것들을 배워볼 수 있었던 것 같아 재미있었습니다! ㅎㅎ\n\u0026lsquo;MSA에서 관심사/책임 분리를 어떻게 할 수 있을까?\u0026rsquo; 출처: 마이크로 서비스에 대한 데이터 고려 사항 - 마이크로소프트 블로그\n처음 쿠뮤를 설계하면서 MSA를 도입하고자 했을 때는 어떻게 마이크로서비스를 나눠야할까에 대한 고려가 거의 없었던 것 같습니다. 지금은 DDD를 공부하며 높은 응집도와 독립성을 취하면서 서비스를 나누는 게 좋겠다는 생각이 들지만 당시엔 그냥 나누고 싶은 대로 나눠버렸던 것 같습니다. 약간 오버 엔지니어링이 될 수 있는 설계를 한 것은 그렇다치고 그 속에서 서비스들을 구현해 나갈 때에도 실수했던 것이 하나 있습니다. 바로 위의 사진처럼 다른 서비스가 하나의 DB를 바라보면 안된다는 원칙을 어긴 것인데요.\n예를 들어 게시글 서비스와 댓글 서비스가 분리되어 있을 때 위의 이미지와 같이 어떤 댓글의 작성자가 해당 게시글의 작성자와 일치하는지에 대한 정보가 필요하다고 가정해봅시다. 기존에는 댓글 서비스가 게시글 DB를 직접 조회해서 article.author와 comment.author를 비교했습니다. 하지만 이렇게 분리된 서로 다른 서비스가 같은 데이터에 접근하는 형태는 MSA에서는 매우 안 좋게 평가되는 안티 패턴입니다.\n따라서 저는 댓글을 조회할 때 게시글 서비스에게 게시글 정보를 조회해 article.author와 comment.author를 비교할 수 있도록 했습니다. 만약 게시글 서비스가 비정상으로 동작하더라도 댓글 서비스에는 큰 타격 없이 댓글 작성자가 게시글 작성자와 일치하는지 정보만 false로 처리될 뿐입니다.\n아마 더 고도화된 형태는 댓글 서비스에서 자기 서비스만이 이용할 게시글 작성자 정보 테이블을 설계하거나 덜 정규화한다면 댓글마다 게시글의 작성자 id도 기입하는 방식을 이용할 수 있을 것 같아요. 하지만 그런 형태를 적용하기 이전에 우선은 좀 더 latency가 길 수는 있지만 간단히 마이크로서비스간에 동기적으로 통신(게시글 작성자 정보를 댓글 서비스가 API로 직접 호출)하는 방식을 적용해봤습니다.\n\u0026lsquo;어떤 서비스를 이용해 빌드/배포를 자동화할까?\u0026rsquo; 과거에는 배포 도구로서 Spinnaker를 주로 사용했지만 Spinnaker는 그 관리 리소스가 만만치 않게 든다는 생각이 들었습니다. Jenkins나 그 외의 서비스들과 연동시켜 하나의 Pipeline을 만드는 데에는 Spinnaker가 적합했지만 꼭 그런 연동 작업이 필요가 없다면 굳이 Spinnaker를 사용할 필요가 없을 것 같았습니다.\n실용주의 데브옵스 for MSA - 카카오테크 CI/CD 도구 및 방법론 도입기 - 메쉬코리아, VROONG 그래서 위의 글들에도 소개되는 평소 관심 있던 ArgoCD를 배포 도구로 선택했고, 오픈소스로서 관리되는 프로젝트이다보니 Github 내에서 많은 작업이 이루어지므로 CI는 Github Action을 이용하기로 결정했습니다.\n그리고 Spinnaker를 사용했을 땐 Helm 차트를 통한 배포가 막 지원되던 시기였어서 버전 문제인지 버그인지 Helm 차트가 잘 적용이 안됐던 이슈가 있어 정말 아쉬웠는데요. ArgoCD는 kustomize나 Helm을 적극적으로 지원하고 있는 것 같아 Helm으로 배포하는 것도 너무나 편리했어요. 지금은 Kubernetes의 Nginx Ingress Controller나 ArgoCD Notification 같은 서비스들은 Helm으로 배포/관리하고 있답니다.\n프로젝트 리드와 팀원들간의 의사소통 저희 팀은 주로 노션을 통해 회의 내용 기록 및 진행 사항 공유를 하고, 회의는 디스코드로 진행했습니다. 이렇게 보니 해당 페이지에 기록하기 전까지 합하면 딱 1년 정도 된 것 같네요 벌써..! 시간이 참 빠른 것 같아요 ㅎㅎ\n디스코드로 회의하기 힘든 상황엔 자유롭게 카톡으로 이슈를 공유하기도 합니다. 얼마 전에 운영 환경을 구축하면서 S3 + CloudFront를 구축할 때 겪은 이슈 관련 톡을 가져와 봤습니다. 특히 서로 포지션이 다른 경우 이슈에 대해 자세히 묘사하려 노력하고 팀원의 작업에 항상 감사를 표하는 걸 잊지 않으려 노력하고 있어요. 코로나와 거리 상의 문제로 한 번도 뵙지 못한 팀원들도 계시지만 나름 내적 친밀감이 많이 쌓인 것 같아요 ㅎㅎ 꼭 쿠뮤가 아니라도 기회가 된다면 앞으로도 같이 작업하거나 만나뵙고 싶기도 합니다.\n앞으로의 미래..? 사실 개발 놀이터로 시작한 주제라서 그렇게 참신하지도 않고, 대학생 커뮤니티에는 에브리타임이라는 큰 장벽이 있어서 우리 서비스의 미래가 어떻게 될 지는 모르겠습니다. 처음엔 커뮤니티 기능을 주기능으로 밀어붙여 개발했던 반면 활성 유저가 별로 없을 경우를 고려하니 커뮤니티보다는 공지사항과 학사일정을 편리하게 전달할 수 있는 기능이 더 우선시 되어야할 것 같았어요. 그래서 그 부분들을 보완하여 MVP를 출시 준비 중이고 이후 다양한 피드백을 받아보면서 계속해서 디벨롭해나갈지 유지만 할 지 팀원들과 상의해봐야할 것 같긴 합니다.\n그래도 대학교를 졸업하기 전에 이렇게 길게 한 프로젝트를 진행해볼 수 있었던 것도 뿌듯하고 팀원들과 꾸준히 협업해나가는 경험을 할 수 있었던 점, 다양한 이슈를 맞이하고 해결할 수 있었던 점에서 서비스의 미래가 어떻게 되든 소중한 경험을 한 것 같습니다.\n쿠뮤 팀원들 모두 꽃길 걷고 행복했으면 좋겠습니다~! 화이팅!!!\n","date":"2021-10-02T15:46:54+09:00","image":"https://umi0410.github.io/blog/khumu/khumu-dev-review/thumbnail-wide_hu54b7b06ccaab22efd2d5450276cad720_83729_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/khumu/khumu-dev-review/","title":"쿠뮤 MVP 개발기"},{"content":"시작하며 드디어 에러를 다뤄볼 차례가 됐네요. 작년까지만 해도 Error나 Exception 처리의 중요성을 잘 몰랐던 것 같습니다. 하지만 API 클라이언트랑 작업을 하면 할수록 에러 처리의 중요성을 느끼게 되는 것 같아요. 왜냐하면 백엔드에서 각각의 에러 케이스에 대한 status_code나 error_type등을 명확히 정의하지 않으면 클라이언트는 에러를 해석할 방법이 없기 때문입니다.\n또한 에러를 잘 처리한 뒤 올바르게 응답하거나 로그를 남기는 것은 유지/보수 시에 백엔드 개발자 스스로 에게도 정말 중요할 수 있습니다. 우리는 실제 운영 환경의 로그를 마치 우리가 로컬에서 개발할 때 처럼 항상 보고있을 수는 없기 때문이죠. 즉 에러 로그만 따로 \u0026ldquo;편리하게\u0026rdquo; 볼 수 있거나 에러 로그로 인한 Notification System이 있으면 훨씬 에러를 추적(Tracing)하기 쉽겠죠!\n그리고 Golang은 다른 언어들과 달리 Try Catch의 형태로 에러를 처리하지 않는다는 점을 모두들 알고 계실 겁니다. 하지만 이 에러를 그럼 어떻게 처리하지에 대한 방안은 뚜렷이 인지하지 못한 경우가 많을 수 있습니다.(제가 그랬습니다. ㅎㅎ..) 그래서 대충 에러 로그를 출력하게만 하고 통일성 없는 방식으로 그때 그때 떠오르는 대로 에러처리를 해왔던 것 같아요.\n그럼 이번 글에서는 에러를 왜 우아하게, 잘 처리해야하는지 살펴보고 그를 위한 방법도 소개해보겠습니다.\n📦 에러를 Wrapping 하자 다른 언어들은 에러를 상속해 정의하는 방식을 이용하지만 Golang은 에러를 Wrapping하는 방식을 이용합니다. 좀 낯설어서 그렇지 익숙해지면 훨씬 간결한 것 같습니다.\n예를 들어 Redis 관련 에러는 모두 추상적인 에러인 RedisError에 속하고 조회 도중 발생한 에러는 RedisGetError라고 정의하는 경우가 있다고 해보겠습니다. 자바의 경우는 다음과 같이 정의하겠죠.\npublic class RedisError extends Exception{ public RedisError(String msg){ super(msg); } } public class RedisGetError extends RedisError{ public RedisError(){ super(\u0026#34;레디스 조회 관련 에러\u0026#34;) } } public class Something{ public void somethingWithRedis(){ try{ redis.get(\u0026#34;X\u0026#34;); } catch (RedisGetError e){ ... // 레디스 조회 관련 에러 케이스에 대한 대응 } catch (RedisError e){ ... // 레디스 조회를 제외한 레디스 관련 에러 케이스에 대한 대응 } catch (Exception e){ ... // 그 외의 모든 에러 케이스에 대한 대응 } } } 이렇게 Java의 경우는 상속을 통해 각 에러 케이스에 대해 대응합니다. 하지만 Golang에서는 주로 다음과 같은 방법으로 에러를 처리합니다.\nerror.New(msg string) 으로 sentinel error(더 이상 쪼갤 수 없는 최상위 에러 같은 느낌)를 정의\nfmt.Errorf(\u0026quot;%s: %w\u0026quot;, msg, sentinelErr) 으로 sentinel error를 감싼(wrap) 자식 에러를 정의\n에러를 Wrapping 하면 sentinel error에 좀 더 정보를 추가할 수 있어 친절한 에러가 되죠!\nerrors.Is(err, parentErr) 으로 err의 wrapping chain에 parentErr이 존재하는지 판단\n에러를 Wrapping한다는 개념이 잘 이해되지 않는 분들은 Wrapping and Un-wrapping of error in Go (Golang)을 참고해주시면 좋을 것 같습니다.\n그럼 Go에선 위와 같은 케이스가 어떻게 구현될 수 있는지 적어보겠습니다.\nvar ( ErrRedis = errors.New(\u0026#34;레디스 관련 에러\u0026#34;) ErrRedisGet = fmt.Errorf(\u0026#34;조회 도중 에러: %w\u0026#34;, ErrRedis) ) type Something struct{} func (s *Something) somethingWithRedis(){ _, err := redis.Get(\u0026#34;X\u0026#34;); if err != nil{ if errors.Is(err, ErrRedisGet){ ... // 레디스 조회 관련 에러 케이스에 대한 대응 } else if errors.Is(err, ErrRedis){ ... // 레디스 조회를 제외한 레디스 관련 에러 케이스에 대한 대응 } else{ ... // 그 외의 모든 에러 케이스에 대한 대응 } } } Go에서 error는 interface이기 때문에 간혹 type assertion을 이용해 에러의 타입을 비교하거나 직접 error variable와 비교하는 경우도 있긴합니다만 바람직하지는 않습니다. type assertion은 애초에 다소 불안하거나 번거로운 면이 있고 직접 error value와 비교하는 경우는 해당 error value를 제공하던 package가 추후에는 어떤 error value를 재정의할 지 모르기 때문입니다. 예를 들어 ErrRedisGet 를 리턴하던 패키지가 ErrRedisGet 를 Wrapping한 ErrRedisHashGet 를 리턴하는 형태로 변경됐다고 가정합시다.\nvar ( ErrRedis = errors.New(\u0026#34;레디스 관련 에러\u0026#34;) ErrRedisGet = fmt.Errorf(\u0026#34;조회 도중 에러: %w\u0026#34;, ErrRedis) ) func (r *redis) HashGet(key string) string, error{ if (Hash Get 도중 에러가 발생한 경우){ return \u0026#34;\u0026#34;, ErrRedisGet // 이 버전에서는 ErrRedisGet을 리턴함. } ... // 에러가 발생하지 않은 경우 } func (s *Something) somethingWithRedis(){ _, err := redis.HashGet(\u0026#34;X\u0026#34;); if err != nil{ if err == ErrRedisGet{ // Redis 조회 도중 에러가 발생한 경우에 대한 대응 } ... } } 기존 버전에선 ErrRedisGet 을 리턴하고, 그 결과를 확인하는 메소드에서도 ErrRedisGet 과 직접 ==으로 비교를 하니 올바르게 동작합니다.\nchain.png var ( ErrRedis = errors.New(\u0026#34;레디스 관련 에러\u0026#34;) ErrRedisGet = fmt.Errorf(\u0026#34;조회 도중 에러: %w\u0026#34;, ErrRedis) ErrRedisHashGet = fmt.Errorf(\u0026#34;해쉬 조회 도중 에러: %w\u0026#34;, ErrRedisGet) ) func (r *redis) HashGet(key string) string, error{ if (Hash Get 도중 에러가 발생한 경우){ return \u0026#34;\u0026#34;, ErrRedisHashGet // 이 버전에서는 ErrRedisGet을 리턴함. } ... // 에러가 발생하지 않은 경우 } func (s *Something) somethingWithRedis(){ _, err := redis.HashGet(\u0026#34;X\u0026#34;); if err != nil{ // 만약 아래처럼 errors.Is를 이용하면 ErrRedisGet은 실제로 리턴된 // ErrRedisHashGet의 wrapping chain에 존재하므로 기존과 동일하게 // 올바르게 동작할 수 있음. // if errors.Is(err, ErrRedisGet){} // 하지만 이제는 ==을 통한 직접 비교는 예상한대로 동작하지 못함 ㅜㅜ if err == ErrRedisGet{ ... // ㅜㅜ } ... } } 하지만 다음 버전에선 redis.HashGet() 이 Hash에 대한 조회 도중 에러가 발생했을 땐 ErrRedisGet이 아닌 그것을 Wrap한 ErrRedisHashGet을 리턴한다면 기존의 ==을 이용한 직접 비교는 올바른 결과를 가져다주지 못할 것입니다.\n다소 어렵거나 낯설 수 있는 내용이라 간단한 내용을 위주로 예시를 들어봤습니다. 관심이 있으신 분들은 기본으로 제공되는 errors 패키지 외에 https://github.com/pkg/errors 라는 패키지를 이용하는 경우도 있으니 한 번 살펴보시면 좋을 것 같습니다.\n자, 그럼 에러를 Wrap하라는 내용 하나 갖고 얘기를 주절 주절 길게 했으니 이제 실제로 웹 백엔드 개발에서의 에러 처리에 대해 알아봅시다~!\n🤔 웹서버가 에러를 제대로 처리하지 않으면? 에러를 제대로 처리하지 않으면 일반적으로 웹 프레임워크들은 500 Internal Error로 응답하기 때문에 Client는 500 Internal Error 응답 밖에 받을 수 없습니다\u0026hellip; 경우에 따라서는 Body는 올바르게 받을 수도 있긴하지만 매우 불안하죠. 에러의 원인을 모르니 클라이언트 측에서도 유저에게 \u0026ldquo;알 수 없는 이유로 요청에 실패했습니다.\u0026rdquo; 같은 메시지를 띄운다면 유저 경험도 당연히 좋지 않아지겠죠.\n각 에러 케이스에 적절히 대응해서 응답하면 클라이언트와 유저가 모두 잘 반응할 수 있을 것입니다.\n로그인하지 않은 유저의 요청이라 에러가 난 경우 =\u0026gt; 401 Unauthorized 응답 =\u0026gt; 클라이언트는 유저를 로그인 창으로 안내함. 권한이 없는 유저의 요청이라 에러가 난 경우 =\u0026gt; 403 Forbidden 응답 =\u0026gt; 클라이언트는 유저에게 권한이 없어서 불가능하다는 친절한 오류 메시지를 띄움. 존재하지 않는 리소스에 접근해 에러가 난 경우 =\u0026gt; 404 Not Found 응답 =\u0026gt; 클라이언트는 유저에게 해당 리소스가 존재하지 않는다는 친절한 오류 메시지를 띄움. Fiber에서 에러 처리하는 법 fiber는 golang 웹 프레임워크들 중에서 문서가 참 잘 되어있는 편이라고 생각합니다. Error Handling 파트를 보면 웹 프레임워크단에서는 어떻게 에러를 처리할 지 간단히 살펴볼 수 있습니다. Default Error Handler는 기본적으로 사용되는 것이고, Custom Error Handler을 따로 정의해 이용을 할 수도 있습니다. 하지만 저는 저번 시간에 다뤘던 Middleware 방식을 이용해 에러를 잘 처리해보겠습니다.\nerrorhandler.png 간단히 조금 설명을 드리자면 Fiber에서 제공하는 Error Handler는 fiber.Handler들이 리턴한 error 타입을 실제 HTTP Response로 어떻게 변환을 할까에 대한 내용을 포함하고 있는 것이고, 제가 정의하는 미들웨어 방식의 Error Handler는 하나의 middleware로서 다른 fiber.Handler(미들웨어 혹은 요청 핸들러)가 리턴한 error을 fiber의 Default Error Handler가 이해할 수 있는 fiber의 error type으로 변환시키는 것이라고 보시면 됩니다.\n예시 - username으로 User 조회 전체 코드는 이전 글들과 마찬가지로 제 깃헙 umi0410/how-to-backend-in-go에 올려두겠습니다. 글에서는 간단한 코드 비교와 HTTP 응답만 설명드릴게요.\n에러를 잘 Wrapping한 정도에 따라 다르게 응답하는 세 가지 경우가 있습니다.\nWorst - 에러를 어딘가에 정의하지도 Wrap하지도 않은 익명 에러 리턴 =\u0026gt; 적절히 에러를 판단할 수 없어 500 Error Ungraceful - 어딘가에 정의된 Error을 리턴 =\u0026gt; 에러를 적절히 판단해 올바른 Status code로 응답은 가능하지만 에러에 대한 자세한 문맥이나 설명이 없음. Graceful - 에러를 잘 Wrap해서 리턴 =\u0026gt; 에러를 적절히 판단해 올바른 Status code로 응답할 수 있고 에러에 대한 자세한 문맥이나 설명도 클라이언트에게 제시됨. 코드로 보면 다음과 같습니다.\nvar ( ErrResourceNotFound = errors.New(\u0026#34;존재하지 않는 리소스\u0026#34;) users = []*User{ \u0026amp;User{Name: \u0026#34;우미잉\u0026#34;, Age: 25, Username: \u0026#34;umi0410\u0026#34;}, } ) ... 생략 func (svc *UserService) GetUserWorst(username string) (*User, error) { for _, user := range users { if user.Username == username { return user, nil } } return nil, errors.New(\u0026#34;유저가 존재하지 않는 유저(username=\u0026#34; + username + \u0026#34;)\u0026#34;) } func (svc *UserService) GetUserUngracefully(username string) (*User, error) { for _, user := range users { if user.Username == username { return user, nil } } return nil, ErrResourceNotFound } func (svc *UserService) GetUserGracefully(username string) (*User, error) { for _, user := range users { if user.Username == username { return user, nil } } return nil, fmt.Errorf(\u0026#34;입력된 username=%s: %w\u0026#34;, username, ErrResourceNotFound) } 위와 같이 에러와 에러를 리턴하는 서비스 메소드들을 정의하고 커스텀 에러 핸들러로서의 미들웨어는 아래와 같이 작성했습니다.\nfunc customErrorHandler(ctx *fiber.Ctx) error { // 다음 핸들러를 호출한 뒤 // 본 미들웨어의 작업은 사후처리! err := ctx.Next() if err != nil { log.Error(err) if errors.Is(err, ErrResourceNotFound) { // fiber의 Default Error Handler가 알아들을 수 있는 형태의 fiber Error을 리턴 return fiber.NewError(404, err.Error()) } } return err } 이 미들웨어는 다른 일반적인 미들웨어들과는 약간은 다르게 동작한다고 볼 수 있는데요. 대부분의 미들웨어들은 요청을 실제로 처리하기 전에 자신의 작업을 수행하는 사전처리 방식을 이용하는데요. 예를 들어 저번 글에서 살펴본 basic auth 미들웨어는 요청을 처리하는 핸들러 이전에 미들웨어로서 요청자의 인증 정보를 기입(사전처리)하는 역할을 했죠. 하지만 이 미들웨어는 요청을 처리하는 핸들러의 결과 에러를 이용하기 때문에 미들웨어가 시작하자마자 다음 핸들러를 호출하고 그 결과 에러를 이용합니다. 사후처리 방식이죠!\n그럼 이제 customErrorHandler가 리턴한 에러를 fiber의 Default Error Handler가 잘 처리해줄 것입니다.\n$ curl http://localhost:8000/worst/jinsu -w \u0026#34;\\t%{http_code}\\n\u0026#34; 유저가 존재하지 않는 유저(username=jinsu)\t500 $ curl http://localhost:8000/ungraceful/jinsu -w \u0026#34;\\t%{http_code}\\n\u0026#34; 존재하지 않는 리소스\t404 $ curl http://localhost:8000/graceful/jinsu -w \u0026#34;\\t%{http_code}\\n\u0026#34; 입력된 username=jinsu: 존재하지 않는 리소스\t404 응답을 보시면 worst case의 경우 status code가 500이기 때문에 클라이언트가 해당 에러 케이스에 대해 올바른 처리를 할 수 있는 방법이 없습니다.. 응답을 정규표현식을 이용해 판단해야할까요..? ㅎㅎ\u0026hellip;\n그리고 ungraceful case의 경우 status code는 404로 잘 정의되었지만 응답 바디가 다소 불친절해 클라이언트측에서 디버깅하기가 힘들겠죠.\n반면 graceful한 경우에는 status code도 잘 정의되었고 에러 바디도 자세합니다 굿굿이에요!\n마치며 정말 작년까지는 에러를 핸들링하는 것에 대해 별 생각이 없었던 것 같습니다. 하지만 사이드 프로젝트를 하면서 에러를 제대로 처리하지 않아 Index Out Of Range 런타임 에러 하나에도 서버가 종료되어버린다거나 모든 에러를 500 Internal Error로 리턴해버리는 케이스를 겪으면서 \u0026lsquo;아\u0026hellip; 에러를 잘 처리해야 하는구나\u0026lsquo;를 느껴볼 수 있었고, 실제 프로덕트를 개발하면서 \u0026lsquo;클라이언트 측에서 에러에 대한 적절한 액션을 취하기 위해선 status code나 error_type등을 잘 전달해야겠구나\u0026rsquo; 라는 생각도 해볼 수 있었던 것 같습니다.\n게다가 Go에서 에러를 처리하는 방법에 대해 깊게 생각을 못해봤었는데 yorkie 라는 오픈소스에 기여(PR #222 )하게 되면서 멘토님께서 잘 이끌어주셔서 에러를 Wrapping한다는 개념에 대해 자세히 알아볼 수 있었던 기회가 된 것 같습니다.\n이번에 디프만 10기로 활동하게 되었는데 10기 인터뷰 당시에도 클라이언트와 협업하면서 어려웠던 점과 해결해나간 방식과 관련된 질문을 받았을 때에도 에러 핸들링과 관련지어 답변을 했을만큼 에러 핸들링은 \u0026lsquo;그래도 내가 한편으로는 아직 성장하고 있나보다.. ㅎㅎ\u0026lsquo;를 느낄 수 있었던 새로운 경험이었던 것 같습니다! 이 글을 읽으시는 여러분들도 이제 에러 잘 처리하고 저와 함께 클라이언트에게 칭찬받는 백엔드 개발자가 되어봅시다~!\n참고 Working with Errors in Go 1.13 - https://go.dev/blog/go1.13-errors Wrapping and Un-wrapping of error in Go (Golang) - https://golangbyexample.com/wrapping-and-unwrapping-error-golang/ Add RPC Config validation and handle the errors (yorkie PR #222) - https://github.com/yorkie-team/yorkie/pull/222 fiber docs Error Handling - https://docs.gofiber.io/guide/error-handling Error는 검사만 하지말고, 우아하게 처리하세요. - http://cloudrain21.com/golang-graceful-error-handling ","date":"2021-09-05T01:46:54+09:00","image":"https://umi0410.github.io/blog/golang/how-to-backend-in-go-errorhandle/chain_hua96618047d845851ee10ee4de25bc827_61443_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/golang/how-to-backend-in-go-errorhandle/","title":"Golang으로 백엔드 개발하기 - 5. Error Handling. 에러 잘 처리하기 (feat. fiber)"},{"content":"시작하며 안녕하세요. 저번 글에서는 fiber 라는 웹 프레임워크로 간단히 웹 애플리케이션을 만드는 방법에 대해 알아봤으니 이번엔 웹 애플리케이션을 만들다보면 꼭 필요해지는 middleware 작성법에 대해 알아보겠습니다. 여태까지 백엔드에서 서버를 개발하면서는 JS의 Express 프레임워크를 제외하고는 미들웨어 개발이 그닥 쉽게 이해되는 부분은 아니었던 것 같습니다. Express에서 미들웨어를 작성하는 방법은 굉장히 직관적이고 문서도 많은 편이었거든요. 하지만 Spring은 Filter나 Interceptor를 이해하기 위해 많은 내용을 알아야하고, Django는 Class based나 function based, 그리고 middleware를 추가하는 법 등이 좀 복잡한 편이라고 생각합니다.\n그리고 Golang은 아무래도 코드로 말하는 사람들이 많아서인지 그닥 middleware 작성과 같은 부분들이 문서로 잘 나와있진 않고, 코드를 까보면서 만드는 경우가 많았던 것 같습니다. 그래서 이번 글에서는 fiber 로 웹서버를 띄우면서 간단히 미들웨어 하나 만들어보겠습니다.\n제공되는 Basic Authentication middleware을 이용해 요청 유저를 식별 식별된 유저 정보가 삽입된 logger를 context에 주입 이렇게 하면 이후 service layer등에서는 요청 유저가 삽입된 logger을 통해 로그를 작성하니 로그 추적하기가 용이해지겠죠? ㅎㅎ (BasicAuth를 이용하는 이유는 남이 작성한 미들웨어를 가져다쓰는 방법을 편하게 알아보기 위함일 뿐 인증 방법으로 권장해서는 아닙니다.)\n미들웨어?? 그게 뭐야? 왜 필요해?! 웹 프레임워크에서 말하는 미들웨어는 보통 요청에 따른 핸들러를 수행하기 전 혹은 수행한 후에 수행하는 작업을 말합니다. 주로 체인의 형태라고 보시면 되고 한 미들웨어에서 응답을 완료하면 걔가 요청 핸들러의 역할을 하게 된다고 보면 될 것 같습니다. 응답을 완료하지 않으면 다음 미들웨어를 호출하죠.\n서버 개발을 처음할 때는 미들웨어의 필요성을 잘 느끼지 못할 수 있습니다. 하지만 어느정도 개발을 하거나 배포를 해나가다보면 \u0026lsquo;아\u0026hellip; 모든 요청마다 ~~~한 작업을 수행하고 싶은데\u0026rsquo; 이런 생각이 들 때가 있을 수 있습니다. 예를 들어 API 별로 레이턴시를 체크하고 싶을 수도 있고, body나 query string을 로그로 남기거나 어떤 유저의 요청인지 매 요청마다 정보를 주입하고 싶을 수 있죠. 이런 경우 middleware를 이용하면 모든 요청은 미들웨어를 거치면서 처리되기 때문에 편리하게 작업할 수 있습니다!\n대표적으로는 인증이나 캐싱, 로깅, 응답 타임아웃 등에 사용하고 있는 것 같아요!\n남이 만든 미들웨어 사용해보기 - BasicAuth Go를 통해 개발하는 이상 저희는 문서에만 의존할 수는 없습니다. 코드를 까봐야죠 !_! Guidelines for creating Middleware - fiber Github Issue 이슈에는 미들웨어 작성에 대한 몇 가지 가이드라인이 제시되어있더라구요. 참고해보시면 좋을 것 같기도 합니다.\n저는 gofiber/fiber/middleware/basicauth 의 코드를 바탕으로 해석해보겠습니다. 링크를 따라가면 README가 있긴하지만 정보가 많지는 않죠.\n// fiber의 basicauth 미들웨어 코드 중 일부입니다. // Config defines the config for middleware. type Config struct { // Next defines a function to skip this middleware when returned true. // // Optional. Default: nil Next func(c *fiber.Ctx) bool // Users defines the allowed credentials // // Required. Default: map[string]string{} Users map[string]string // Realm is a string to define realm attribute of BasicAuth. // the realm identifies the system to authenticate against // and can be used by clients to save credentials // // Optional. Default: \u0026#34;Restricted\u0026#34;. Realm string // Authorizer defines a function you can pass // to check the credentials however you want. // It will be called with a username and password // and is expected to return true or false to indicate // that the credentials were approved or not. // // Optional. Default: nil. Authorizer func(string, string) bool // Unauthorized defines the response body for unauthorized responses. // By default it will return with a 401 Unauthorized and the correct WWW-Auth header // // Optional. Default: nil Unauthorized fiber.Handler // ContextUser is the key to store the username in Locals // // Optional. Default: \u0026#34;username\u0026#34; ContextUsername string // ContextPass is the key to store the password in Locals // // Optional. Default: \u0026#34;password\u0026#34; ContextPassword string } 위의 코드가 BasicAuth 미들웨어의 설정을 담당하는 Config 타입에 대한 코드입니다. 간단히 몇 개만 알아두면 이해하는 데에 도움이 되는 것들만 짚어보겠습니다.\nNext - nil이 아닌 경우 Next(*fiber.Ctx)가 true를 반환하면 Basic Auth 미들웨어 자체를 건너뜀. Users - 이 map에 정의된 유저만이 유효한 유저로 이용될 수도 있음. (Authorizer가 nil인 경우 이 정보를 바탕으로 인증 진행) Authorizer - 인증 로직을 담당함. nil로 둘 경우 Users에서 일치하는 유저를 찾음. 여길 커스터마이징해서 DB에서 찾거나 할 수도 있음. ContextUsername, ContextPassword - 인증 후 Context에 어떤 키로 각 정보를 담을지. (기본값은 \u0026ldquo;username\u0026rdquo;, \u0026ldquo;password\u0026rdquo;) 오호.. 막상 코드를 까보니 그닥 어렵지 않죠?! 그럼 과연 이 코드들이 설명한 게 맞는지 한 번 서버를 띄워봅시다.\nBasic Auth 미들웨어를 다양하게 설정해보기 1. 항상 인증에 성공하는 Authorizer를 이용하는 Config // newBasicAuthConfigAlwaysAllow 는 언제나 인증에 성공하는 Authorizer를 // 이용하는 Config를 만듭니다. func newBasicAuthConfigAlwaysAllow() *basicauth.Config{ return \u0026amp;basicauth.Config{ Authorizer: func(username string, password string) bool { return true }, } } func main(){ ... app.Use(basicauth.New(*newBasicAuthConfigAlwaysAllow())) ... } # curl -u username:password는 해당 username과 password로 Basic auth를 하는 요청을 보냅니다. $ curl -u wrong:wrong localhost:8000 Welcome! $ curl -u foo:bar localhost:8000 Welcome! Authorizer가 username과 password가 어떻든 true를 반환해 인증에 성공합니다.\n2. 미리 정의된 유저에 대해서만 인증에 성공하는 Authorizer를 이용하는 Config // newBasicAuthConfigAlwaysAllow 는 Users에 존재하는 유저 정보에 대해서만 // 인증에 성공하는 Authorizer를 이용하는 Config를 만듭니다. func newBasicAuthConfigAllowOnlyAdmin() *basicauth.Config{ return \u0026amp;basicauth.Config{ Users: map[string]string{ \u0026#34;foo\u0026#34;: \u0026#34;bar\u0026#34;, }, } } func main(){ ... app.Use(basicauth.New(*newBasicAuthConfigAllowOnlyAdmin())) ... } # curl -u username:password는 해당 username과 password로 Basic auth를 하는 요청을 보냅니다. $ curl -u wrong:wrong localhost:8001 Unauthorized $ curl -u foo:bar localhost:8001 Welcome! 앞서 말했듯 Authorizer를 설정하지 않고 nil로 둘 경우 Users 맵의 데이터를 이용하는 Authorizer가 기본값으로 설정됩니다. 저는 username=foo, password=bar인 유저 정보를 맵에 기록했기 때문에 foo:bar 요청만이 승인되는 것을 볼 수 있습니다.\n자, 이렇게 남이 작성한 미들웨어를 설정만 조금 바꿔서 이용해봤는데, 코드를 읽는 것도 그걸 바탕으로 가져다가 쓰는 것도 어렵지 않다는 걸 알게 됐군요. 이제는 직접 미들웨어를 작성해보겠습니다.\nCustom middleware 작성하기 커스텀으로 미들웨어를 작성할 때에는 그닥 정해진 기준이 많지는 않습니다. 대부분의 미들웨어들은 Skipper 등의 개념으로(basic auth 미들웨어에선 Next라고 정의됨.) 해당 미들웨어의 기능을 실행하지 않고 넘어가는 설정이나 뭐 각종 설정들이 존재는 하지만 우리는 근본적으로 커스텀 미들웨어를 어떻게 작성하는지에 대해 궁금하니까 그 부분만 알아보겠습니다.\npackage fiber ... // Handler defines a function to serve HTTP requests. type Handler = func(*Ctx) error fiber 프레임워크에서는 middleware을 정의할 때 fiber.Handler 라는 일종의 함수 타입을 이용합니다. 그리고 fiber.Handler는 보통은 클로져를 이용하는 형태로 제공됩니다. 제가 볼 땐 좀 특이한 것 같은데 아마 echo도 이런 식으로 클로져를 이용해 미들웨어를 생성했던 것 같습니다. 다음과 같이 말이죠.\nfunc NewMiddleware(config Config) fiber.Handler { // config에 기본값을 넣은 config를 클로져로 접근하려 함. config = injectDefaultValues(config) // 실제 미들웨어 역할을 할 함수. // config에 클로져로 접근한다. return func(c *fiber.Ctx) error{ ... // 다음 미들웨어(혹은 요청 핸들러)를 수행한다. return c.Next() } } 근데 이런 경우는 아무래도 config를 설정해야하는 경우인 것 같고 저희는 그냥 최대한 심플하게 만들어볼게요. 목표는 Basic Auth를 거친 뒤 유저 정보가 기입된 logger를 context에 삽입해주는 것입니다. 그럼 어떤 계층에서든 ctx.Get(\u0026quot;logger\u0026quot;) 등을 이용해 logger에 편리하게 접근할 수 있죠! log 프레임워크는 sirupsen/logrus 를 이용하겠습니다. 컬러도 잘 나오고 확장성도 좋은 걸로 알고 있어요!\nvar injectLoggerMiddleware = func (ctx *fiber.Ctx) error{ // basic auth middleware가 주입한 username을 이용합니다. username := ctx.Locals(\u0026#34;username\u0026#34;) logger := log.WithField(\u0026#34;user\u0026#34;, username) // context에 logger라는 키로 유저 정보를 주입한 logger를 전달합니다! ctx.Locals(\u0026#34;logger\u0026#34;, logger) // 다음 미들웨어(혹은 요청 핸들러)를 수행합니다. return ctx.Next() } func main(){ ... // injectLoggerMiddleware는 basicauth가 context에 주입한 username을 이용하기 때문에 // 꼭 basicauth middleware 다음에 수행되어야합니다! app.Use(basicauth.New(*newBasicAuthConfigAllowOnlyAdmin())) app.Use(injectLoggerMiddleware) // 요청 핸들러는 이런 식으로 middleware에게 주입받은 logger를 context에서 꺼내쓸 수 있습니다. app.Get(\u0026#34;\u0026#34;, func(ctx *fiber.Ctx) error { logger := ctx.Locals(\u0026#34;logger\u0026#34;).(*log.Entry) logger.Info(\u0026#34;유저가 접속했습니다\u0026#34;) return ctx.SendString(\u0026#34;Welcome!\\n\u0026#34;) }) ... } log.png 그럼 다음과 같이 Basic Auth를 통해 각각 다른 유저로 요청을 보내면 해당 요청을 처리하는 동안은 해당 context 속의 logger을 이용해 로그를 남길 수 있는 걸 볼 수 있습니다. 이렇게 정리해보니 golang에서도 middleware를 작성하는 게 그리 어렵지 않네요~!\n마치며 echo 프레임워크를 쓰면서는 미들웨어를 작성할 때 좀 애를 먹었던 기억이 있는데, 막상 fiber로 미들웨어를 작성해보니 생각보다 많이 직관적이고 쉬워서 좀 놀랐습니다. (사실 글을 쓸 필요가 없었을지도..?) 그래도 Go로 작성된 한글 자료도 많아졌으면 좋겠고, 누군가는 fiber 미들웨어를 작성하는 방법이 좀 이해되지 않고 어려우실 수 있으니 도움이 될 수 있으면 좋겠습니다 ㅎㅎ 다음엔 서버 개발에 있어 또 하나의 중요한 점인! 에러 핸들링에 대해 알아보겠습니다! 감사합니다.\n참고 Guidelines for creating Middleware - fiber Github Issue Fiber middleware Fiber error handling Redhat - 미들웨어란 ","date":"2021-09-01T12:46:54+09:00","image":"https://umi0410.github.io/blog/golang/how-to-backend-in-go-middleware/log_hu6da6c55463c6a62988b186d685cd2c7c_23812_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/golang/how-to-backend-in-go-middleware/","title":"Golang으로 백엔드 개발하기 - 4. Custom Middleware(미들웨어) 작성해보기 (feat. fiber)"},{"content":"시작하며 이번에는 Go언어에서 유명한 웹 프레임워크를 비교해보고 간단하게 Pingpong 서버를 개발해보겠습니다. 다만 \u0026ldquo;Golang으로 백엔드 개발하기\u0026ldquo;를 주제로 글을 쓰면서 주 목적으로 했던 것은 개발하면서 계속 궁금했지만 어딘가에서 뚜렷한 설명을 찾아보기 힘들었던 내용들을 다뤄보고자하는 것이었기 때문에 이번 주제는 그닥 주 목적에 해당하는 주제는 아니므로 이번엔 아주 가~볍게 훑어보고 넘어가겠습니다. 왜냐하면 웹 프레임워크를 사용하는 것은 해당 웹프레임워크의 문서나 이런 저런 블로그들에 이미 너무도 많이 자료가 존재하기 때문입니다.\nGolang으로 웹 애플리케이션을 개발하는 방법들 보통 다른 언어 같은 경우 Java에서는 Spring MVC 혹은 Spring WebFlux, Python은 FastApi, Flask, Django, Node.js는 Express, nest, koa처럼 각 언어마다 손에 꼽을 만한 대표적인 웹 프레임워크들이 있는 것 같은데 Go는 그렇지가 않습니다. Go는 잘 나가는 웹 프레임워크들이 너무도 다양하게 존재합니다. 아주 아주 다양한 프레임워크가 존재하지만 그닥 우위가 뚜렷하진 않습니다. 어떤 한 기술이 생태계를 독점해버리거나 그 기술만이 강요되는 일 없이 서로(프레임워크간)가 경쟁하며 좋은 기술로 나아가는 게 장점이 될 수도 있고, 너무 프레임워크가 다양해서 선택하기 곤란하다는 것이 단점이 될 수도 있을 것 같습니다. 커뮤니티가 한 데 집중된다면 더 버그 픽스하기 쉬울텐데 그런 면에서는 조금 아쉽기도 합니다.\n다만 한 가지 흐름이 있다면 Python을 사용하는 개발자분들이 \u0026quot;Pythonic\u0026quot;이라는 철학(?)을 굉장히 좋아하시듯 Go를 사용하는 개발자분들은 \u0026quot;Go스러움\u0026quot;을 굉장히 좋아하는 경향이 있어 어떤 프레임워크가 계속해서 인기를 끄는 데에는 \u0026quot;Go스러운가\u0026quot;가 큰 영향을 끼치는 것 같습니다. 아마 Go스러움이란 다음과 같은 특징들을 얘기하는 것일 겁니다.\n타입을 통해 안정적으로 개발한다. (interface{} 와 같은 빈 인터페이스를 이용하면서 type assertion이나 reflect를 이용하는 것을 지양) 가볍고 간결하다. (Spring MVC와 같은 방식은 지양. 프레임워크에서 쓸 데 없이 많은 걸 지원하면서 서비스가 무거워지는 것을 지양.) 빠르다. 동시성을 goroutine과 channel을 이용해 잘 사용한다. (근데 이 부분은 web framework을 고를 때는 영향을 끼치지 않는 것 같고 개발자의 코드에 대한 go스러움일 것입니다.) 프레임워크 추천 net/http - Go 표준(빌트인) 패키지 github.com/labstack/echo - 스타 20k github.com/gofiber/fiber - ❤️ 스타 14.6k github.com/gin-gonic/gin - 스타 50k github.com/beego/beego - 스타 26k \u0026hellip; 이런 특징들을 가진 웹프레임워크로 한국에서 많이 쓰이는 듯한 프레임워크들은 fiber, echo, gin 이 세 친구들인 것 같습니다. 저도 다 깊이 있게 써본 건 아니지만 그 동안 리서치 해온 것들이나 한국 Go 커뮤니티에서 오가는 이런 저런 내용들을 참고했을 때의 의견은 다음과 같습니다. gin은 오랜 시간 개발되어 온 프로젝트이고 꾸준히 스타도 많지만 아마 좀 \u0026lsquo;무겁다. 느리다.\u0026rsquo; 이런 얘기가 있었던 것 같습니다. echo는 간결하게 사용할 수 있으면서 필요한 기능들은 다 있고 개발이 활발하게 이루어지고 있는 듯한 느낌이지만 문서가 그닥 친절하지 않고 썩 빠른 건 아니다는 생각이 듭니다. 문서가 거의 그냥 예시 코드 작성한 정도입니다. 그래도 Go 언어 특성상 남의 코드를 까서 봐도 해석하기 쉽고 주석도 잘 달려있는 편이라 큰 어려움은 없었습니다. fiber는 요즘 가장 핫한 프레임워크 같습니다. 빠르게 깃헙 스타 수도 늘어나고 있고 fasthttp를 사용해서인지 아마 월등히 빠른 것으로 알려져있습니다. 게다가 문서도 너무 잘 작성되어있고 개발도 활발히 이루어지고 있습니다.\n이러한 이유로 저는 현재 fiber와 echo를 이용 중이고 추후 새로운 서비스를 개발해야한다면 fiber를 이용할 예정입니다! (사실 gin이 스타는 꾸준히 많이 늘어나고 있고 스타 수 자체도 1타인데 왜 한국에서 많이들 사용하지 않는지는 잘 모르겠습니다. 아시는 분이나 gin을 사용해보신 분 있으시면 의견 부탁드립니다..!)\n예시 코드 간단하게 net/http 나 fiber 중 하나를 이용해 GET /ping에 응답하는 웹 애플리케이션을 만들어보았으니 참고하실 분들은 참고하시길 바랍니다. 웬만하면 공식 문서의 Getting started나 Example들을 보시는 걸 추천드립니다.\n다른 글들과 마찬가지로 예시 코드는 https://github.com/umi0410/how-to-backend-in-go의 webapp 디렉토리에 업로드 해두겠습니다.\npackage main import ( \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/gofiber/fiber/v2\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) var ( framework string port = flag.Int(\u0026#34;p\u0026#34;, 8888, \u0026#34;서버가 Listen할 port 번호를 입력해주세요.\u0026#34;) ) func init() { flag.Parse() if len(flag.Args()) != 1 { log.Fatal(\u0026#34;하나의 인자를 전달해 framework를 정의해주세요. (e.g. http, echo, fiber)\u0026#34;) } framework = flag.Arg(0) } func main() { switch framework { case \u0026#34;http\u0026#34;: RunNewHttpServer() case \u0026#34;fiber\u0026#34;: RunNewFiberServer() default: log.Fatal(\u0026#34;지원하지 않는 framework 입니다.\u0026#34;) } } func RunNewHttpServer() { addr := fmt.Sprintf(\u0026#34;:%d\u0026#34;, *port) log.Printf(\u0026#34;Server is listening %d\u0026#34;, *port) http.HandleFunc(\u0026#34;/ping\u0026#34;, func(w http.ResponseWriter, req *http.Request) { if _, err := w.Write([]byte(\u0026#34;PingPong by net/http\\n\u0026#34;)); err != nil { log.Print(err) } }) if err := http.ListenAndServe(addr, nil); err != nil { log.Print(err) } } func RunNewFiberServer() { addr := fmt.Sprintf(\u0026#34;:%d\u0026#34;, *port) app := fiber.New() app.Get(\u0026#34;/ping\u0026#34;, func(c *fiber.Ctx) error { return c.SendString(\u0026#34;Pingpong by fiber\\n\u0026#34;) }) log.Printf(\u0026#34;Server is listening %d\u0026#34;, *port) if err := app.Listen(addr); err != nil { log.Print(err) } } output.png 마치며 간단하게 Go언어에서 유명한 프레임워크들을 비교해보고 Pingpong 서버 예제 코드를 작성해봤습니다. 앞서 언급드렸듯이 아무래도 웹 프레임워크로 hello world를 찍는 수준의 서버를 작성하는 내용은 이미 어느 곳에서든 찾아볼 수 있기 때문에 이번 글은 가볍게 마무리하고 이런 저런 프레임워크들의 공식 문서를 보시는 걸 추천드립니다 ㅎㅎ 다음 글은 미들웨어 작성이나 에러 처리에 대해 다뤄볼 예정인데 이번 글 보다 재밌을 것으로 예상됩니다. \u0026lsquo;에러를 어떻게 처리할까\u0026lsquo;나 \u0026lsquo;미들웨어는 어떻게 작성할 수 있을까\u0026lsquo;와 같은 내용은 그닥 흔히 찾아볼 수 있는 내용은 아닌 것 같았기 때문입니다. 그럼 다음 글에서 봐요 우리~ ㅎㅎ 👋\n참고 간단한 웹 서버 (HTTP 서버) - http://golang.site/go/article/111-%EA%B0%84%EB%8B%A8%ED%95%9C-%EC%9B%B9-%EC%84%9C%EB%B2%84-HTTP-%EC%84%9C%EB%B2%84 Understanding Listen Ports and Addresses - https://www.ateam-oracle.com/listen-addresses https://www.esparkinfo.com/top-golang-web-framework-development.html https://blog.devgenius.io/best-web-framework-of-golang-in-2021-aae4b2ad9bf 내가 Go 언어를 선택한 이유 - https://pronist.tistory.com/67 ","date":"2021-08-13T12:46:54+09:00","image":"https://umi0410.github.io/blog/golang/how-to-backend-in-go-webapp/output_hu537a11d34916ac1c2def7909b69ddbc5_22456_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/golang/how-to-backend-in-go-webapp/","title":"Golang으로 백엔드 개발하기 - 3. 웹 애플리케이션 개발해보기 (feat. fiber)"},{"content":"시작하며 저는 Go 언어를 공부하면서 처음으로 테스트 코드라는 것을 접하게 되었습니다. 과거에는 그저 기능을 구현하는 것에만 관심이 있었지 애플리케이션을 어떻게 계층을 나눠 설계할지, 어떻게 해야 유지 보수하기 쉬우면서 안정적인 개발을 할 수 있을지에 대한 고민을 하지 않았습니다.\n하지만 Go를 공부하면서부터는 클린 아키텍쳐나 MSA, 동시성 패턴 등을 비롯해 테스트 코드에 대해서도 공부해볼 수 있었습니다. 그 중에서도 테스트 코드에 대해 공부해보며 익혔던 점들을 바탕으로 이번 글에서는 Golang으로 백엔드 개발을 하면서 테스트 코드를 어떻게 작성할 수 있을지에 대해 알아보려합니다.\n본 글에서 사용된 예시 코드는 저의 Github how-to-backend-in-go 레포지토리에 업로드 해둘테니 참고하실 분들은 참고해주세요~!\nTDD (Test Driven Development)와 유닛 테스트 사용자의 반응에 맞춰 재빠르게 대응할 수 있도록 개발 주기가 빨라지면서 DevOps 문화 뿐만 아니라 TDD도 많은 인기를 얻게 된 것 같습니다. TDD란 개발을 먼저 다~ 진행한 뒤 한 번에 배포하거나 시간 날 때 테스트 코드를 짜는 것이 아니라 우리가 개발해야할 최소한의 내용들을 검증할 수 있는 테스트 코드를 먼저 작성한 뒤 해당 테스트를 통과하는 코드를 개발해나가는 개발 방법론입니다.\nTDD에 대해 설명하자면 TDD만으로도 책 몇 권의 분량이 될 수 있고, 저 또한 TDD를 잘 아는 것은 아니기에 자세한 설명은 생략하겠습니다. 이번 글의 주된 내용은 TDD에 대해 소개하는 것보다는 Golang으로 유닛 테스트를 작성하는 요령입니다.\n유닛 테스트는 다른 계층과 무관하게 독립적으로 각각의 메소드의 기능을 테스트하는 것이라고 볼 수 있겠습니다. 계층마다 해당 테스트를 통해 검증하고자 하는 주요한 내용은 다를 수 있습니다. 유닛 테스트는 주로 자신만을 테스트 하기 때문에 자기 계층의 동작을 테스트하지 다른 계층의 동작까지 테스트하진 않습니다. 또한 경우에 따라 아예 다른 계층을 실제로 이용하지 않고 모킹(Mocking)한 타입을 이용하는 경우도 있습니다.\nData access layer\nuserRepository.Create(user *entity.User) - 유저 테이블에 해당 데이터가 잘 저장되는지 테스트 Business layer\nuserService.Create(ctx *fiber.Ctx, user UserCreateInput) - 유저 생성 관련 비즈니스 로직을 주로 테스트.\nDB에 잘 영속화 되었는지 까지는 테스트 할 필요 없음. 이는 Data access layer에서 테스트 할 내용\nPresentation layer\nuserHandler.Craete(ctx *fiber.Ctx) - 응답 코드가 바라던 대로 201 번이 맞는지. JSON으로 잘 응답하는지\n마찬가지로 DB에 잘 영속화 되었는지까지는 테스트할 필요 없음. 이는 Data access layer에서 테스트 할 내용.\n또한 어떤 business layer의 비즈니스 로직에 대해서도 자세히 테스트 할 필요 없음. 이 또한 Business layer에서 테스트할 내용\n참고로 저는 Business layer를 위주로 테스트하고 있습니다. Data access layer의 경우 많은 부분이 Database framework(ORM)을 통해 검증이 되었고, Presentation layer도 Web framework들에 의해 많은 부분이 검증되었을 뿐 아니라 포스트맨 같은 도구들을 통해 확인해는 것이 더 편한 경우도 존재하기 때문입니다. 반면 Business layer의 동작은 매우 중요하고, 테스트 코드 없이 매번 수작업으로 그 동작을 검증하는 것은 매우 번거롭기 때문입니다.\n(계층이나 관심사를 어떻게 나누냐에 따라 Business layer는 다양하게 불리는 것 같습니다. 예를 들어 내부 계층, 도메인 계층, 코어 로직 계층 등등으로 불리기도 합니다.)\nUnit test를 도입하기 전에 알아두면 좋은 것들 유연성을 위한 의존성 주입 및 변수 초기화 방법 mocking에 대하여 assert 구문 이용 Spring을 바탕으로 개발하는 경우 JUnit이라는 테스트 프레임워크를 주로 이용하고 이를 통해 테스트하는 방식이 거의 정형화 되어있는 것 같습니다. 하지만 Golang은 그닥 테스트 코드를 작성하는 형태가 정형화되어있지는 않습니다. 하지만 어떤 언어나 프레임워크를 통해 유닛 테스트 코드를 사용하든 위의 세 가지 항목들을 어떻게 이용할 지 알아야할 것입니다.\n1. 유연성을 위한 의존성 주입 및 변수 초기화 방법 어떠한 모듈이 자신의 멤버 변수들을 내부적으로 생성하는 것이 아니라 외부에서 주입 받는 형태인 의존성 주입은 유연한 의존성 관리를 위해 권장됩니다. 예를 들어 User에 대한 도메인/비즈니스 로직을 실행하며 UserRepository를 이용해 User를 영속적으로 저장하기도 하는 UserService가 UserRepository에 의존하는 경우를 살펴봅시다.\n// 1. 의존성 주입 형태 - Good func NewUserService(userRepository UserRepository) *UserService{ return \u0026amp;UserService{ userRepository: userRepository, } } // 2. 의존성을 주입하지 않는 형태 - Bad func NewUserServiceWithoutInjection(client *ent.UserClient) *UserService{ return \u0026amp;UserService{ userRepository: NewUserRepository(client), } } 첫 번째 - 의존성 주입 형태는 테스트 하는 경우나 개발 환경에 따라 유동적으로 적절한 UserService가 이용할 UserRepository를 주입받을 수 있습니다. 즉 실제로 API 서버를 구동시킬 때에는 NewUserService(userRepository UserRepository) 메소드에 실제로 DB를 이용하는 Repository인 UserRepositoryImpl 을 주입하고, 유닛 테스트를 수행할 때에는 모킹한 타입인 MockUserRepository를 주입하는 것입니다.\n두 번째 - 의존성을 주입하지 않는 형태는 자신이 의존하는 UserRepository가 이미 NewUserServiceByItSelf라는 메소드에서 NewUserRepository 라는 메소드를 통해 특정 UserRepository를 이용하도록 정의되어있기 때문에 Mocking을 적용하기도 힘들고, 만약 다른 모듈들 또한 UserRepository를 이용하는 경우 Singleton으로 이용하지 못하고 여러 UserRepository가 생성될 수도 있습니다. 사실 ent.UserClient라는 DB Client 또한 주입받지 않고 내부적으로 새로 생성해야 의존성을 완전히 주입받지 않는 형태이겠지만 그런 방식은 너무도 구현하기 번거로워 편의상 DB client는 주입받았습니다. 이것만 봐도 개발하는 형태가 일관되지 못하고 불편한 사항이 많다는 것을 알 수 있죠.\n2. Mocking에 대하여 모킹이란 실제로 서버가 실행될 때 사용하는 모듈이 아닌 가짜 모듈을 이용하는 기법입니다. 앞에서 잠깐 언급했듯이 종종 테스트 코드 작성 시에 종종 Mocking이 이용됩니다. 의존성 주입 패턴을 통해 테스트 코드에서는 실제 type이 아닌 Mocking type을 유동적으로 주입할 수 있습니다. 잘 와닿지 않을 수 있는데 예를 들어 UserService의 비즈니스 룰과 로직이 잘 동작하는지를 확인할 때 Data access layer의 동작까지 테스트하고 싶지 않을 수 있습니다. 이 경우 Data access layer의 모듈을 가짜 타입으로 모킹합니다.\ntype MockUserRepository struct{} func (m MockUserRepository) Create(input *UserCreateInput) (*ent.User, error) { return \u0026amp;ent.User{ ID: input.ID, Password: input.Password, Name: input.Name, }, nil } func TestUserService_Create(t *testing.T){ // 해당 이름의 유저가 있는지 확인 s := NewUserService(new(MockUserRepository)) _, err := s.Create(\u0026amp;UserCreateInput{ID: \u0026#34;jinsu_umi\u0026#34;, Name: \u0026#34;Jinsu Park\u0026#34;, Password: \u0026#34;123123123\u0026#34;}) assert.NoError(t, err) } 이런 식으로 MockUserRepository 타입에서는 Create 메소드가 직접 DB에 유저를 영속화하는 작업을 생략한 뒤 그럴싸한 리턴값을 리턴하도록만 구현함으로써 UserRepsotiroy를 모킹하는 것입니다.\n하지만 이렇게 수작업으로 Mock type을 매번 정의하는 것은 귀찮을 수 있죠. 따라서 이번 글의 뒤에서는 mocking framework들에 대해서도 정리해보겠습니다.\n종종 Stub과 Mock에 대해 혼동이 올 수 있습니다. 기본적으로 실제 모듈이 아닌 가짜 모듈을 이용하는 기법 자체를 mocking이라고 합니다. 이를 좀 더 세분화해서 봤을 땐 stub은 단순히 테스트를 간단히 통과시키기 위해 실제 모듈을 대체하는 가짜 객체를 말하고 mock은 예상값까지 비교를 하고, 때때로 테스트를 실패시키기도 하는 더 스마트한 stub이라고 볼 수 있다고 합니다.\n참고: https://stackoverflow.com/questions/3459287/whats-the-difference-between-a-mock-stub\n3. assert 구문 이용하기 func TestUserService_Create(t *testing.T){ // 해당 이름의 유저가 있는지 확인 s := NewUserService(new(MockUserRepository)) t. _, err := s.Create(\u0026amp;UserCreateInput{ID: \u0026#34;jinsu_umi\u0026#34;, Name: \u0026#34;Jinsu Park\u0026#34;, Password: \u0026#34;123123123\u0026#34;}) // 이런 식으로 로그를 통해 하나 하나 테스트 결과를 확인하는 것이 아니라 //t.Log(\u0026#34;에러가 존재하지 않아야합니다.\u0026#34;) //t.Log(\u0026#34;err == nil 인지 확인하십시오. err == nil: \u0026#34; , err == nil) // assert를 이용해 자동으로 성공/실패를 판단합니다. assert.NoError(t, err) } assert 라는 방식을 통한 테스트 케이스 검증은 다양한 테스트 프레임워크에서 사용됩니다. 눈으로 하나 하나 테스트 케이스들의 결과를 확인하는 것이 아니라 assert라는 방식을 통해 자동으로 테스트의 성공/실패를 판단하는 것입니다. 저는 주로 github.com/stretchr/testify 패키지의 assert 메소드를 이용하고 있습니다.\ntest-output.png assert를 통해 테스트의 성공/실패 여부를 판단하면 성공시에는 크게 stdout을 확인할 필요 없고, 실패한 경우에는 그것에 대한 에러 메시지들만 확인하면 되기 때문에 편리합니다. 만약 로그나 프린트 문을 통해 수작업으로 테스트 결과를 판단해야한다면\u0026hellip; 매번 테스트마다 너무 불편하겠죠?!\nGolang에서 Mocking 이용하기 이번엔 Golang에서 Mocking을 이용하는 방법을 좀 더 자세히 알아보겠습니다. golang에서 Mocking을 이용하는 방법은 크게 3가지로 나눌 수 있을 것 같습니다.\n직접 Mock type 구현하기 golang/mock 즉 gomock 프레임워크 이용하기 stretchr/testify/mock 프레임워크 이용하기 자바의 경우 따로 미리 mock 타입(즉 가짜 타입)을 작성해놓지 않더라도 그리고 기존 타입이 interface가 아닌 concrete한 일반 class이더라도 상속 혹은 프록시를 통해 mock 타입을 이용할 수 있습니다. 반면 Golang은 투명한 동작과 엄격한 타입이 특징인 만큼 테스트 진행 시에 자동으로 적절한 mock 타입을 생성하거나 프록시를 이용하거나 interface가 아닌 struct에 다른 type의 struct를 할당할 수도 없습니다. 따라서 Go에서 mocking을 이용하기 위해서는 아래와 같은 제약 조건들이 존재합니다. 이러한 점을 생각해본다면 Go에서는 Mocking을 이용하는 것은 다른 언어에 비해 쉬운 편은 아닌 것처럼 느껴지기도 합니다.\ninterface로 선언된 변수에만 mock type을 할당할 수 있다. mock type을 직접 정의하거나 mocking framework을 이용해 코드를 생성한다.(테스트 진행 시 자동이 아님. 직접 우리가 실행시켜야함.) 주로 1번(직접 Mock type 구현하기) 같은 경우는 이 mock 타입이 우리 서비스에서 자주 사용될 것이고 그냥 예상치못한 call이나 missing call(호출되길 예상했지만 호출되지 않는 경우)에 대한 대응이 필요 없는 경우 사용하면 좋을 것 같습니다. 반면 2, 3번은 이번 테스트를 위한 1회성 mocking인 경우 많이 사용되기도 하고 mock 타입의 메소드에 대한 unexpected call이나 missing call 에 대한 대응을 수행하고자하는 경우 이용할 수 있습니다.\n추가적으로 1번 방식이 아닌 2, 3 번 방식처럼 모킹 프레임워크를 이용하는 게 더 편했던 적이 있는데요. 바로 메소드가 여러 개 존재하는 타입을 모킹할 때 입니다. Golang이 지향하는 방향 자체가 interface는 많은 메소드를 갖지 않는다는 것이긴 하지만 뭐.. 경우에 따라 메소드가 많아질 수도 있죠 ㅎㅎ. interface를 모킹하기 위해 별로 테스트 시에 필요도 없는 메소드들을 구현하는 것은 꽤나 귀찮습니다. 하지만 mocking 프레임워크들을 이용하면 기본적으로 모킹하려는 interface들의 메소드들은 모두 구현되고, 우리는 필요한 경우에만 추가적으로 구현을 해나가는 형태이므로 모킹 프레임워크를 이용하는 것이 편리할 수 있습니다.\n반면 mocking 프레임워크를 이용할 때의 단점은 프레임워크 사용법을 익혀야한다는 것이나 내가 직접 작성하지 않은 코드가 많이 생성된다는 점이 되겠네요. 그럼 이제 gomock과 testify/mock을 비교해보겠습니다. \u0026ldquo;GoMock vs. Testify: Mocking frameworks for Go\u0026rdquo; - codecentric blog에도 내용이 잘 정리되어있으니 참고해보세요.\nGolang mocking 프레임워크 gomock vs testify/mock 비교 star-comparison.png golang에서 직접 제작하는 gomock과 stretchr/testify라는 유명한 테스트 관련 레포지토리의 하위 패키지인 testify/mock 은 Golang에서 사용할 수 있는 가장 유명한 모킹 프레임워크들입니다. 깃헙 스타만 놓고 봤을 때에는 testify가 훨씬 많지만 testify는 다른 테스트를 위한 패키지들도 함께 있어서 정확히 비교는 안되겠네요.\n저도 두 프레임워크의 특징에 대해 정확히는 모르지만 간단히 아는 선에서 주관적인 추천을 하자면 gomock을 추천드리겠습니다. testify/mock에 비해 원본 interface 타입을 바탕으로 모킹 메소드를 좀 더 safe하게 정의할 수 있었던 것 같습니다. 다만 정말 뭘 쓰든 상관 없을 것 같습니다. 각자 조금의 장단점은 있지만 뭐 하나가 특출나거나 모자란 느낌은 아닙니다. 두 프레임워크의 특징을 아래와 같이 정리해봤습니다. (참고로 알아보니 제가 좋아하는 서비스인 ArgoCD (K8s의 GitOps 방식의 배포 도구)에서는 testify/mock을 이용하더군요\u0026hellip; T.T)\n코드 생성 도구를 이용해 원하는 interface의 mock type을 정의, 구현하는 코드를 생성하는 것이 testify/mock이 좀 더 편함.\ntestify/mock 이 좀 더 메소드를 mocking하고 expectation을 설정하는 게 직관적이고 간결하다. 이 부분이 testify/mock의 큰 장점인듯 함.\ntestify/mock 의 코드 생성 도구인 mockery가 gomock의 코드 생성 도구인 mockgen보다 쓰기 편하다는 의견이 있음. (본인은 잘 체감 못하겠음)\ngomock이 좀 더 type safe하게 모킹 코드를 작성할 수 있다. 일단 메소드가 gomock은 타입을 기반으로 자동완성 되지만 testify/mock은 메소드 명을 문자열로 받음. 이 부분이 gomock의 큰 장점인듯 함.\n두 프레임워크 모두 unexpected call이나 missing call, 특정 횟수 만큼 call 등등의 기능들은 제공하는 것 같음. 단 조사한 바로는 testify/mock은 모킹 메소드 구현 시에 인자 값을 이용할 수 없음.\n그럼 말로만 설명하지 말고 코드로 한 번 간단하게 비교드리겠습니다!\n1. 직접 mock 타입 작성 type ManualMockUserRepository struct{} func (m *ManualMockUserRepository) Create(input *UserCreateInput) (*ent.User, error) { log.Info(\u0026#34;직접 Mocking. Args: \u0026#34;, input) return \u0026amp;ent.User{}, nil } func TestUserService_CreateWithManualMock(t *testing.T){ s := NewUserService(new(ManualMockUserRepository)) _, err := s.Create(\u0026amp;UserCreateInput{ID: \u0026#34;jinsu_umi\u0026#34;, Name: \u0026#34;Jinsu Park\u0026#34;, Password: \u0026#34;123123123\u0026#34;}) // 이런 식으로 로그를 통해 하나 하나 테스트 결과를 확인하는 것이 아니라 //t.Log(\u0026#34;에러가 존재하지 않아야합니다.\u0026#34;) //t.Log(\u0026#34;err == nil 인지 확인하십시오. err == nil: \u0026#34; , err == nil) // assert를 이용해 자동으로 성공/실패를 판단하십시오. assert.NoError(t, err) } 앞서 말씀드린 대로 직접 mock type을 작성하는 경우입니다. 정말 정말 safe하지만 테스트 케이스마다 메소드가 return하는 값이나 동작을 다르게 하고 싶다면 코드가 복잡해지거나 한 interface에 대한 다양한 mock type을 여러 개 작성해야할 수 있습니다.\n2. testify/mock 이용 testify/mock을 이용하는 경우 mockery라고 하는 코드 생성 CLI 도구를 이용하면 편리합니다. 본 코드는 mockery를 통해 생성된 mock 타입을 이용했습니다.\nfunc TestUserService_CreateWithMockery(t *testing.T){ mockUserRepository := \u0026amp;MockUserRepository{} // method를 문자열 자체로 설정해야해서 safe하지 않음. mockUserRepository.On(\u0026#34;Create\u0026#34;, mock.Anything).Run(func(args mock.Arguments) { t.Log(\u0026#34;testify/mock과 mockery를 이용한 Mocking. Args: \u0026#34;, args.Get(0)) }).Return(\u0026amp;ent.User{}, nil) // 해당 이름의 유저가 있는지 확인 s := NewUserService(mockUserRepository) _, err := s.Create(\u0026amp;UserCreateInput{ID: \u0026#34;jinsu_umi\u0026#34;, Name: \u0026#34;Jinsu Park\u0026#34;, Password: \u0026#34;123123123\u0026#34;}) assert.NoError(t, err) } 앞서 말씀드린 대로 메소드 명이 unsafe하게 .On(methodName string, ...)의 형태로 문자열을 이용하고 있습니다. argument 또한 단순히 mock.Arguments를 통해 얻을 수 있고 이에 대해 .Get을 통해 얻는 값은 interface{}이기 때문에 type assertion을 거쳐야만 내부 값을 복사해서 이용할 수 있습니다.\n3. gomock 이용 gomock은 mockgen(참고: https://github.com/golang/mock)이라는 코드 생성 CLI 도구를 이용합니다. 본 코드는 mockgen을 통해 생성된 mock 타입을 이용했습니다.\nfunc TestUserService_CreateWithMockgen(t *testing.T){ // gomock controller을 만들고 Finish 시켜주는 등의 불편함 존재. ctrl := gomock.NewController(t) defer ctrl.Finish() mockUserRepository := NewGomockRepository(ctrl) // 원래의 type을 기반으로 method가 safe하게 제공됨. mockUserRepository.EXPECT().Create(gomock.Any()).DoAndReturn( func(input *UserCreateInput) (*ent.User, error) { t.Log(\u0026#34;Gomock을 이용한 Mocking. Args: \u0026#34;, input) return \u0026amp;ent.User{}, nil }) // 해당 이름의 유저가 있는지 확인 s := NewUserService(mockUserRepository) _, err := s.Create(\u0026amp;UserCreateInput{ID: \u0026#34;jinsu_umi\u0026#34;, Name: \u0026#34;Jinsu Park\u0026#34;, Password: \u0026#34;123123123\u0026#34;}) assert.NoError(t, err) } gomock controller라는 녀석을 생성해주고 finish 시켜줘야하는 불편함이 생겨났습니다만 모킹하고자 하는 메소드를 safe하게 제공받을 수 있습니다. .DoAndReturun() 메소드의 인자로서 어떤 함수로 모킹할 지를 전달하는데 사실 이 함수 자체를 interface{}가 받기 때문에 인자는 제가 정의하기 나름이라 원본 interface와 싱크되지는 않는 약간의 아쉬움이 존재합니다. 하지만 testify/mock처럼 args의 순서에 따라 .Get()을 한 뒤 interface{} 타입의 각각의 인자들을 type assertion할 필요도 없고 가독성이 더 좋다고 생각합니다.\n테스트 코드 작성해보기 *_test.go 형태의 파일 생성하기 func Test* (t *testing.T){...} 형태의 테스트 케이스 작성하기 로그나 Print문이 아닌 assert 문을 이용해 테스트 성공/실패 판단을 자동화하기 Golang에서 테스트 코드를 하기 위해선 우선 위의 3가지 사항만 알아두면 됩니다.\n그리고 추가적으로 golang에선 테스트 케이스 속에 nested된 테스트 케이스를 얼마든지 집어넣을 수 있습니다. 너무너무 편리하죠~! 그럼 이번에도 코드로 예시를 보여드리겠습니다. User에 대한 Domain/Business layer에서의 도메인 룰을 테스트하는 코드입니다. Data access 계층을 통해 User가 영속적으로 잘 저장되었는지보다는 도메인 룰을 테스트하려는 목적이기 때문에 Data access 계층(Repository)에 대해서는 모킹을 적용했습니다.\n// nested test case를 작성하고 싶은 경우. 이렇게 작성하십시오. func TestA(t *testing.T){ t.Run(\u0026#34;A-a\u0026#34;, func(t *testing.T){ t.Run(\u0026#34;A-a-1\u0026#34;, func(t *testing.T){ ... }) }) } // service_test.go 예시 func TestUserService_유저_생성(t *testing.T){ // repository mocking ctrl := gomock.NewController(t) defer ctrl.Finish() mockUserRepository := NewGomockRepository(ctrl) // 원래의 type을 기반으로 method가 safe하게 제공됨. mockUserRepository.EXPECT().Create(gomock.Any()).DoAndReturn( func(input *UserCreateInput) (*ent.User, error) { t.Log(\u0026#34;Gomock을 이용한 Mocking. Args: \u0026#34;, input) return \u0026amp;ent.User{}, nil }) // 해당 이름의 유저가 있는지 확인 s := NewUserService(mockUserRepository) t.Run(\u0026#34;성공\u0026#34;, func(t *testing.T) { _, err := s.Create(\u0026amp;UserCreateInput{ID: \u0026#34;jinsu_umi\u0026#34;, Name: \u0026#34;Jinsu Park\u0026#34;, Password: \u0026#34;123123123\u0026#34;}) assert.NoError(t, err) }) t.Run(\u0026#34;에러) 너무 짧은 아이디\u0026#34;, func(t *testing.T) { _, err := s.Create(\u0026amp;UserCreateInput{ID: \u0026#34;short\u0026#34;, Name: \u0026#34;Jinsu Park\u0026#34;, Password: \u0026#34;123123123\u0026#34;}) assert.ErrorIs(t, err, ErrInvalidUserID) assert.ErrorIs(t, err, ErrInvalidValue) }) t.Run(\u0026#34;에러) 너무 긴 이름\u0026#34;, func(t *testing.T) { _, err := s.Create(\u0026amp;UserCreateInput{ID: \u0026#34;jinsu_umi\u0026#34;, Name: \u0026#34;니노막시무스 카이저 쏘제 쏘냐도르앤 스파르타 죽지 않아 나는 죽지않아 오오오오 나는 카이저 쏘제\u0026#34;, Password: \u0026#34;123123123\u0026#34;}) assert.ErrorIs(t, err, ErrInvalidUserName) assert.ErrorIs(t, err, ErrInvalidValue) }) t.Run(\u0026#34;에러) 너무 짧은 비밀번호들\u0026#34;, func(t *testing.T) { errorPasswords := []string{ \u0026#34;123\u0026#34;, \u0026#34;abc\u0026#34;, \u0026#34;a1b2\u0026#34;, \u0026#34;asd\u0026#34;, } for _, errorPassword := range errorPasswords { t.Run(\u0026#34;테스트 케이스) \u0026#34; + errorPassword, func(t *testing.T) { _, err := s.Create(\u0026amp;UserCreateInput{ID: \u0026#34;jinsu_umi\u0026#34;, Name: \u0026#34;Jinsu Park\u0026#34;, Password: errorPassword}) assert.ErrorIs(t, err, ErrInvalidPassword) assert.ErrorIs(t, err, ErrInvalidValue) }) } }) } TestUserService_유저_생성 이라는 테스트 케이스 안에 성공하는 경우와 올바르지 않은 ID나 이름, 비밀번호로 인해 에러가 발생해야하는 경우를 nested된 테스트 케이스로 작성했습니다. 특히나 비밀번호는 한 번 더 nested된 케이스를 작성했습니다. 같은 로직을 다양한 인풋을 이용해 테스트 하고 싶은 경우에는 errorPasswords 부분처럼 인풋 혹은 결과값을 배열로 선언한 뒤 테스트 하는 경우를 종종 보게되더군요. (참고: \u0026ldquo;Using Subtests and Sub-benchmarks\u0026rdquo; - Go Blog, \u0026ldquo;Uber Go style guide\u0026ldquo;의 test-tables 파트 - uber)\ntest-with-ide.png run-each-test.png 게다가 GoLand를 비롯해 Go언어를 지원하는 IDE를 이용 중이시라면 높은 확률로 이렇게 편리하게 테스트를 실행하고 결과를 확인하실 수도 있습니다. 그리고 두 번째 사진에 보이듯 내부 테스트 케이스에 대한 단일 실행도 가능하다는 점~! 이제 좀 더 테스트 코드를 쉽게 이용할 수 있겠군요. ㅎㅎ\n마치며 이번 편을 준비하면서 처음 유닛 테스트를 작성할 때 찾아봤던 자료들도 다시 한 번 읽어보고 etcd나 argocd 같은 Go로 작성된 유명 오픈 소스들은 어떻게 테스트 코드를 작성하는지도 찾아보는 등 꽤 조사를 많이 하게된 것 같습니다. 아무래도 테스트 코드라는 것이 어떠한 방향으로 테스트 코드와 개발을 설계해나갈지, 어디까지 테스트할 지와 같은 방법론적인 부분과 테스트 프레임워크 자체의 사용법, 모킹 프레임워크 사용법과 같은 약간은 Golang 특화적인 내용 등등 다양한 영역에 걸친 내용이라 쉽진 않은 것 같습니다. 그래도 이번을 기회 삼아 저도 평소에 궁금했던 mockery vs testify/mock 의 비교도 해볼 수 있었던 것 같고, mocking을 적용하는 방법에 대해 좀 더 자세히 알아볼 수 있었습니다! 감사합니다~!\n예시 코드: https://github.com/umi0410/how-to-backend-in-go/tree/master/testcode\n참고 자료 What\u0026rsquo;s the difference between a mock \u0026amp; stub? - https://stackoverflow.com/questions/3459287/whats-the-difference-between-a-mock-stub gomock vs testify - https://blog.codecentric.de/2019/07/gomock-vs-testify/ mockery Github Repository - https://github.com/vektra/mockery stretchr/testify Github Repository - https://github.com/stretchr/testify gomock Github Repository - https://github.com/golang/mock Using Subtests and Sub-benchmarks - https://blog.golang.org/subtests uber-go Guide - https://github.com/uber-go/guide/blob/master/style.md ","date":"2021-07-17T12:46:54+09:00","image":"https://umi0410.github.io/blog/golang/how-to-backend-in-go-testcode/test-with-ide_hu7b61495424919d2e1c8d663236a41b57_159980_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/golang/how-to-backend-in-go-testcode/","title":"Golang으로 백엔드 개발하기 - 2. 테스트 코드 작성 및 Go에서 Mocking 이용하기 (gomock, testify 이용)"},{"content":"시작하며 Go 언어를 처음 시작한 지 벌써 1년이 지났다니 시간이 참 빠른 것 같습니다! Java는 Spring이라고하는 거대한 프레임워크가 자리 잡고 있어 딱히 어떤 프레임워크나 라이브러리를 사용할지에 대한 고민이 별로 필요 없었던 것 같습니다. 반면 Go 언어는 정형화된 아키텍쳐에 대한 내용이 별로 없고 프레임워크나 라이브러리의 대세도 참 빠르게 변하다보니 그게 장점이라면 장점이겠지만 이래저래 고생도 했네요.\n게다가 Go 언어는 개발 입문자들이 많이 사용하지 않는 언어라 그런지 아직 웹 백엔드 개발에 익숙하지 않은 (저처럼) 사람들을 위한 자료들은 많이 없었던 것 같습니다. 예를 들면 Java의 스프링이나 Node.js의 express, Python의 django 같은 프레임워크들은 인프런만 가봐도 A to Z로 알려주는 강의가 많죠. 하지만 Go 언어는 보통 언어 입문 내용들이 많고, \u0026ldquo;자~! 여러분들은 어차피 개발 초보자들이 아니시잖아요? 이쯤하면 Golang을 맛보셨으니 알아서들 입맛대로 쓰십시오~!\u0026quot; 식으로 입문 이후의 내용은 보통 동시성 패턴에 대한 내용들일 뿐 백엔드 개발을 위한 아키텍쳐나 Unit test를 어떻게 실제로 적용하는지 같은 예시는 많이 없었습니다. 그래서 이번엔 Golang으로 웹 백엔드 개발을 하는 과정에 대해 좀 적어볼까합니다!\n이번 글은 그런 내용을 다룰 시리즈 중 첫 번째로 Golang으로 데이터베이스 작업하는 것과 관련한 내용을 담아봤습니다. 데이터베이스를 어떻게 사용하는지에 대한 세세한 내용보단 어떤 프레임워크를 추천드리고, 그 프레임워크를 사용하는 모습이 어떠한지 가볍게 쓰윽 보시면 좋을 것 같습니다.\n3 layered architecture에 대해 간단히 짚고 넘어가겠습니다 Presentation layer - 어떻게 데이터가 클라이언트에게 보여줄 지에 대한 작업을 담당. 클라이언트는 이 계층을 통해 상호작용한다. Business/Domain layer - Presentation layer와 Data access layer의 사이에 위치해 비즈니스 룰과 그 룰을 따르는 비즈니스 로직을 구현. 보통 이 계층에 Service라고 하는 클래스(혹은 타입 등등)가 위치한다. Data Access Layer - DB에 접근하는 작업을 담당. DAO(Data Access Object, repository)가 위치한다. 대부분의 웹 애플리케이션은 3 layered architecture라고하는 구조로 기초로 하여 개발됩니다. 그 속에서 여러 단순 작업이나 개별적으로 개발하긴 번거로운 기능들이 존재할 수 있는데, 우리는 프레임워크나 라이브러리가 제공하는 기능들을 영리하게 가져다 씀으로서 좀 더 유지 보수 하기 쉬운 개발을 할 수 있을 것입니다.\nPresentation layer app.Get(\u0026#34;/users/:id\u0026#34;, func(ctx *fiber.Ctx) error { userId := ctx.Params(\u0026#34;id\u0026#34;) // ... 기타 작업 생략 return ctx.JSON(map[string]string{ \u0026#34;userId\u0026#34;: userId, }) }) presentation-layer-output.png url path variable은 어떻게 인식할 것인지, Golang의 map을 어떻게 JSON으로 직렬화할 지와 같은 로직들이 모두 프레임워크에서 제공됨.\n저는 Presentation layer는 대부분 웹 프레임워크 자체나 해당 웹 프레임워크가 제공하는 형태를 따르는 핸들러 단에서 제공이 된다고 생각하고 있습니다. 예를 들어 어떻게 응답을 JSON 형태로 제공할지 HTML, XML, 이진 데이터(gRPC)와 같은 내용들 말이죠. 즉 Presentation layer에서는 우리가 그닥 개발할 것이 많지는 않습니다.\nBusiness layer Domain logic과 Business logic의 차이는 어느 정도 존재하는 걸로 알고 있습니다만 3 tier로 나눴을 때는 동일한 계층으로 보겠습니다.\n// Domain/Business layer 코드 예시 - 우리 도메인 특정 코드들이 많아 남(우리 팀 혹은 기업이 아닌 사람들)이 만든 오픈소스로 대체하기 힘듦. func (svc *UserService) updateProfileImage(requestUser *User, imageFile Image) (*UserResponse, error){ requestUser가 인증된 유저인가 인증된 유저가 아니라면 어떤 에러를 리턴할 것인가 imageFile이 제대로 된 Image인가 제대로 된 Image가 아니라면 어떤 에러를 리턴할 것인가 imageFile을 업로드 업로드 과정 중 에러 발생 시 어떤 에러를 리턴할 것인가 유저의 친구들에게 새로운 프로필 사진 업로드에 대한 알림 발송 ... 등등 생략 return 생략 } 반면 Domain/Business layer에서는 비즈니스 룰과 로직이라고 하는 우리 애플리케이션의 핵심적인 기능에 대한 구현이나 조합이 이루어지게 되는데 이 부분 타 오픈소스들로 대체되기는 힘든 부분이 많아 당연히 직접 개발하는 경우가 많습니다. 위의 코드를 보면 다양한 우리 서비스의 로직들이 구현되거나 조합되는 것을 볼 수 있습니다.\nData access layer jpa-method.png Java의 JPA 프레임워크의 경우 기본적으로 CRUD 메소드를 제공해준다.\ndjango-method.png Python의 django의 경우 Join 기능도 기본적으로 제공해준다.\n이제 Data Access Layer만이 남았네요. 이 계층 또한 대부분의 객체지향 언어들에선 ORM이라는 개념을 통해 대부분의 프레임워크들이 많은 기능을 제공합니다. 그래서 필요한 경우에만 추가적으로 정의하는 형태로 편리하게 이용할 수 있습니다.\n그렇다면 Go에서는 어떻게 Data access layer를 편하게 이용할 수 있을까요? 이번 글에서는 Golang은 이 Data Access Layer(Repository)에서 어떤 DB framework을 사용하면 좋을지 어떤 식으로 사용해나갈 수 있을지에 대해 다뤄보려 합니다.\nGolang의 좋은 DB framework 좀 추천해주세요. ent-gorm-stars.png 차트 출처: https://star-history.t9t.io/\nGolang에는 Django나 JPA와 같은 강력한 데이터베이스 관리 프레임워크가 몇 년 전까지 존재하지 않았던 것 같습니다. 생 쿼리문을 직접 짤 게 아닌 경우라면 얼마 전까지는 gorm 이라고하는 프레임워크가 거의 유일한 선택지였던 것 같습니다. 하지만 요즘 추세를 보면 무섭게 ent라는 패키지가 쫓아오고 있는 모습을 보실 수 있습니다.\n👎 gorm 프레임워크 사용 후기 제가 golang 개발을 막 시작했던 때가 2020년 여름쯤이었기에 저도 gorm을 첫 데이터베이스 프레임워크로 사용했었지만 몇 가지 단점들이 존재했습니다.\n문서의 내용이 빈약했고, 에러에 대한 설명이나 디버깅이 쉽지 않았다.\n문서의 내용들이 실질적인 다양한 케이스에 대한 예시나 설명이 부족했다고 느껴졌습니다. 또한 에러가 발생한 경우 왜 에러가 발생한 것인지 정확한 위치나 이유가 제공되지 않아 디버깅하기도 힘들었던 것 같습니다.\n불편한 struct tag 기반의 테이블, 컬럼 설정\nGo가 강력히 type을 강제하면서 높은 안정성과 편의성을 제공하는 반면 gorm은 struct tag를 기반으로 여러 설정들을 관리하도록 개발되었습니다. Go 언어를 좋아하지만 struct tag에 대해서는 가뜩이나 조금의 불만을 갖고 있었는데 이런 저런 컬럼 설정들마저 struct tag로 이용하니 정확히 이 tag가 동작하는 태그인지, 왜 에러가 나거나 적용이 안되는지도 안전하게 확인할 수 없었고 무엇보다 가독성이 너무 너무~! 좋지 않았습니다.\n제공되는 메소드가 별로 없다.\ngorm은 아주 가벼운 느낌이었습니다. 하지만 데이터베이스 프레임워크를 사용함에도 불구하고 너무나 제공되는 메소드가 없다고 느껴졌습니다. JPA나 Django는 조금의 설정만 해주면 CRUD와 Join까지 제공해주는데 gorm은 거의 정말 정말 기본적인 기능을 하는 메소드에 적절한 인자를 전달함으로써 동작시켜야합니다. 예를 들어 메소드가 어떤 매개변수를 왜 필요로 하는지에 내용이 그닥 없고 그냥 통으로 Create메소드에 알아서 적절한 inteface{}인자를 전달해야하는 형태라 Go의 장점인 강력한 type을 바탕으로한 안정성, 편의성을 누릴 수도 없었고 이럴꺼면 파이썬 쓰지\u0026hellip; 싶은 생각이 자주 들었습니다.\n👍 추천하는 프레임워크 - ent 물론 위의 Github star history만 봐도 아실 수 있겠지만 ent는 매우 무섭게 성장 중인 Golang의 데이터베이스 관리 프레임워크입니다. 게다가 아마 Facebook에서 내부적으로 사용하다가 오픈소스화한 프로젝트인 걸로 알고 있는데 따라서 어느 정도의 완성도나 편의성이 보장되겠죠. ㅎㅎ 저는 사이드 프로젝트에서 gorm을 사용하다가 많은 불편을 느꼈고, Go 개발 커뮤니티에서 ent가 그렇게나 핫하다길래 ent로 data access layer를 마이그레이션했고 만족 중입니다. ㅎㅎ\nent의 사용 후기 및 특징은 아래와 같습니다.\n타입을 바탕으로 안전하고 편리하게 테이블을 설계할 수 있다.\n테이블, 컬럼 정의들이 모두 ent 패키지의 컬럼 type 혹은 테이블 type, 관계 type 등등을 이용해서 정의할 수 있기 때문에 너무 너무 편리하죠. 게다가 복잡한 struct tag도 안녕~! ent는 미리 정의된 타입과 메소드들로 다양한 설정을 할 수 있습니다.\n다양한 메소드 제공\ngo generate라고 하는 기능을 통해 우리가 정의한 스키마(테이블 및 필드 설정)을 바탕으로 다양한 타입과 메소드들을 만들어줍니다. 이 부분으로 인해 꽤 빌드 시간이 늘어나는 느낌이긴한데 그래봤자 2초 정도인데 ent가 제공해주는 메소드들로 인해 개발적 편의와 안정성이 훨씬 크다고 느끼기 때문에 만족하고 있습니다.\n별 다른 문서가 필요 없다..?\n그냥 메소드를 기반으로 이용할 수 있다보니 그 사용 방법이 직관적인 편이고, 경우에 따라서는 코드 자체를 까보면 이해되는 별 다른 문서 없이도 이용할 수 있었던 것 같습니다.\n참조 관계 설정이 좀 헷갈린다 (몇 안 되는 단점 중 하나)\n뭔가 From이나 To등을 통해 참조 관계를 정의하는데 이 From이 어떤 테이블을 From으로 생각하는건지 뭔가 많이 애매합니다. 커뮤니티를 보면 이로 인해 많은 분들도 혼란을 겪고 있는 현황입니다.\n하지만 너무 걱정은 마세요~! 이번 글에서 정리해드리려하니까요~! ㅎㅎ\nEnt 패키지를 통해 Database 작업해보기 \u0026ldquo;시작하며\u0026ldquo;에서도 말씀드렸다시피 사실 Go 언어로 개발하시는 분들 중에 정말 개발 자체가 익숙하지 않은 분들은 정말 적으리라 생각합니다\u0026hellip;ㅎㅎ 따라서 너무 디테일한 내용을 직접 알려드리기 보다는 커다란 흐름이나 헷갈리는 요소들을 짚어드리는 방향으로 글을 작성해 보겠습니다.\n예시 Application - 여행 상품 관리 CRUD 서비스 1:N의 여행 상품 담당자 관계 - 담당자(일종의 유저)는 여행 상품과 1:N 관계 위의 관계를 갖는 여행 상품 관리 CRUD 서비스를 만들어보겠습니다. 본 글에서는 코드를 짧게 짧게 잘라서 올릴테니 원본 코드를 보고싶으신 분들은 제 깃헙을 참고해주세요.\n원본 코드 - https://github.com/umi0410/how-to-backend-in-go\n1. ent package 설치 # 참고: https://entgo.io/docs/tutorial-setup/#installation $ go get entgo.io/ent/cmd/ent 우선 ent는 다양한 명령 기능도 제공하고 저희는 그걸 필요로 하기 때문에 ent를 설치해주어야합니다. 스키마 정의 코드 또한 ent 를 이용해 자동으로 기본 형태를 제공 받을 수 있습니다. ent 패키지를 설치한 이후 저희는 다음과 같은 명령을 수행하게 될 것입니다.\n$ go run entgo.io/ent/cmd/ent init {{엔티티 이름}} - 엔티티 이름을 바탕으로한 초기 스키마 코드 생성 $ go generate ./ent - 정의했던 스키마를 바탕으로 한 많은 boilerplate 코드를 자동으로 생성 2. 스키마 설정 $ go run entgo.io/ent/cmd/ent init User # or shortly $ ent init User $ go run entgo.io/ent/cmd/ent init TourProduct # or shortly $ ent init TourProduct 위의 명령어들을 통해 User와 User가 관리하는 TourProduct라는 엔티티들의 스키마를 작성할 것입니다. 정확한 스키마 작성에 대한 설명은 생략하겠습니다. ent 공식 문서에 매우 잘 설명되어있고, examples in ent github에서 대부분의 예시 코드도 확인해볼 수 있기 때문입니다.\n⭐ 참조 관계 설정 시의 From과 To 다만 한 가지. Edge를 이용해 참조 관계를 설정할 때 From과 To의 사용에 대해 한 가지만 짚고 넘어가려 합니다. 저를 포함해 많은 ent 사용자분들께서 Edge 설정 시에 많은 혼란을 겪습니다. 그 이유는 일반적으로 우리는 1:N 관계에서 N쪽이 From, 1쪽이 To가 되며 N쪽 테이블에서 1을 참조하는 내용을 정의하는 반면 ent는 그 반대로 동작하기 때문입니다.\n(ent에서의 참조 관계 정의 방식에 대해 궁금하지 않으신 분들은 넘어가셔도 됩니다.)\no2m-relation.png 출처: https://entgo.io/docs/schema-edges\n위와 같은 경우 데이터베이스상으로는 Pet 테이블이 자신의 owner_id 컬럼을 통해 User을 참조하므로 Pet이 From, User가 To가 되며 이를 통해 owner라는 관계를 나타낼 수 있습니다. 하지만 ent는 그 반대로 User가 Pet을 관리(참조)한다는 의미로 User가 From, Pet이 To가 되어 pets라는 관계를 나타내는 식으로 정의하는 것 같습니다. 이렇게 기존의 테이블 설계 방식과는 대조되는 방식으로 인해 많은 사람들이 혼란을 겪는 것 같아요.\n이렇게 테이블을 정의하도록 ent를 개발한 개발자들의 의도나 철학은 뭔지 잘 모르겠습니다. 혹시 아시는 분이 있다면 알려주시면 좀 더 entf를 이해하는 데에 도움이 될 것 같아요!\n아무튼 ent에서는 1:N 방식에서 1이 연관 관계의 주인처럼 동작/정의한다고 생각하고 작업하고 있습니다.(실제 문서에서도 From쪽. 즉 1 혹은 User 쪽이 연관 관계의 주인이라고 기술되어있는데, 이 부분은 JPA와는 거의 반대라고 볼 수 있죠.) 저는 그냥 예시를 참고하면서 실제 테이블의 참조 관계랑 반대라고 외웠어요.(ㅎㅎ\u0026hellip;.;;)\n스키마 정의 및 적용 // ent/schema/user.go package schema import ( \u0026#34;entgo.io/ent\u0026#34; \u0026#34;entgo.io/ent/schema/edge\u0026#34; \u0026#34;entgo.io/ent/schema/field\u0026#34; ) // User holds the schema definition for the User entity. type User struct { ent.Schema } // Fields of the User. func (User) Fields() []ent.Field { return []ent.Field{ // 타입을 기반으로 안전하고 편리하게 컬럼을 정의할 수 있습니다. field.String(\u0026#34;id\u0026#34;), field.String(\u0026#34;name\u0026#34;), field.Bool(\u0026#34;isActivated\u0026#34;).Default(true), } } // Edges of the User. func (User) Edges() []ent.Edge { return []ent.Edge{ // ent에서는 To를 정의하는 스키마, 즉 여기선 User // 가 참조 관계의 주인이라고 정의합니다. // 일반적인 JPA의 방식과는 반대입니다. edge.To(\u0026#34;products\u0026#34;, TourProduct.Type), } } $ go generate ./ent 아무튼 저는 User 스키마를 이렇게 설정했고 이 스키마들을 적용해 ent가 generate해준 코드들을 이용해보겠습니다.\n(TourProduct 스키마는 생략했습니다. 원본 코드를 참고해주세요.)\n3. repository 정의 ent가 데이터베이스 관련 작업을 위한 다양한 메소드를 지원해줍니다. 하지만 ent는 간단 간단한 메소드들을 조합해서 사용해야하기 때문에 비즈니스 로직을 구현하는 계층에서 매번 ent의 메소드를 조합하며 사용하는 것은 계층의 관심사와 책임을 흐리게 할 수 있습니다. 따라서 몇 가지 메소드들을 data access layer인 repository에 정의하겠습니다.\n예를 들면 특정 user가 관리하는 여행 상품을 모두 조회하는 FindAllByManager(managerID string)와 같은 메소드 말이죠. 이 작업을 위해서는 User 테이블과 TourProduct 테이블을 조인해야하는데 ent가 역시 이 기능을 제공합니다.\n// repository.go ... type TourProductRepository struct{ Client *ent.TourProductClient } func (repo *TourProductRepository) FindAllByManager(managerID string) []*ent.TourProduct{ result := repo.Client.Query(). // 특정 manager_id의 TourProduct를 조회하도록 조인 Where(tourproduct.HasManagerWith(user.ID(managerID))). WithManager(). AllX(context.TODO()) return result } 4. 실행 // main.go func main(){ ... fmt.Println(\u0026#34;전체 유저 조회\u0026#34;) for _, user := range userRepository.FindAll() { fmt.Printf(\u0026#34;User(id=%s, name=%s)\\n\u0026#34;, user.ID, user.Name) } fmt.Println(\u0026#34;--------------------------------------------------------------------------\u0026#34;) fmt.Println(\u0026#34;전체 여행 상품 조회\u0026#34;) for _, tour := range tourProductRepository.FindAll() { fmt.Printf(\u0026#34;TourProduct(id=%d, name=%s, manager=%s)\\n\u0026#34;, tour.ID, tour.Name, tour.Edges.Manager.ID) } fmt.Println(\u0026#34;--------------------------------------------------------------------------\u0026#34;) fmt.Println(user1.ID + \u0026#34;가 관리하는 전체 여행 상품 조회\u0026#34;) for _, tour := range tourProductRepository.FindAllByManager(user1.ID) { fmt.Printf(\u0026#34;TourProduct(id=%d, name=%s, manager=%s)\\n\u0026#34;, tour.ID, tour.Name, tour.Edges.Manager.ID) } } 전체 유저 조회 User(id=umi0410, name=박진수) User(id=devumi, name=개발자) -------------------------------------------------------------------------- 전체 여행 상품 조회 TourProduct(id=1, name=미국 뉴욕 여행, manager=umi0410) TourProduct(id=2, name=유럽 여행, manager=devumi) -------------------------------------------------------------------------- umi0410가 관리하는 전체 여행 상품 조회 TourProduct(id=1, name=미국 뉴욕 여행, manager=umi0410) 간단한 테스트용으로 2명의 유저를 만들었고, 각각의 유저가 관리하는 2개의 여행 상품을 만들어서 조회하는 프로그램을 만들어보았습니다. repository 계층에서 만든 FindAll()이나 FindAllByManager()와 같은 메소드들을 통해 편리하게 작업할 수 있네요!\n마치며 글을 쓰다보니 백엔드 개발에서 데이터베이스가 왜 필요하고 어떤 식으로 사용되는지를 다루는 글도 아니고, Golang에서 ent 프레임워크를 사용하는 방법을 자세히 알려주는 글도 아닌 이도 저도 아닌 글이 된 건 아닌가 싶기도 합니다.\n그래도 앞으로 Golang으로 백엔드 개발하는 것에 대해 천천히 한 5~6편 정도의 글을 써나가볼까하는데 이때 DB 관련된 내용을 우선 짚고는 넘어가야할 것 같기도 했고, 무엇보다 한글로 된 ent 자료를 하나도 찾을 수 없다는 점에서 데이터베이스 관련 프레임워크인 ent를 소개해보고자 했습니다! ㅎㅎ\n지면 사정 상 코드들을 잘라서 올렸는데 원본 코드는 https://github.com/umi0410/how-to-backend-in-go 에 올려놓을테니 참고해주시면 감사하겠습니다.\n참고 https://entgo.io/ https://github.com/ent/ent ","date":"2021-07-03T12:46:54+09:00","image":"https://umi0410.github.io/blog/golang/how-to-backend-in-go-db/o2m-relation_huda98d5d877ff5063fa9c786910881686_21160_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/golang/how-to-backend-in-go-db/","title":"Golang으로 백엔드 개발하기 - 1. 데이터베이스 작업하기 (Ent 프레임워크 이용)"},{"content":"문제 설명 문제 출처: https://programmers.co.kr/learn/courses/30/lessons/67258\n각각의 보석상은 하나의 보석을 판매 연속된 보석상들을 쇼핑하면서 방문할 때마다 보석을 구매 모든 종류의 보석을 구매할 때까지 진행 모든 종류의 보석을 구매하는 경우 중 가장 조금의 보석을 가장 앞선 보석상에서 구매하는 경우를 구하기 문제 접근 우선 단순하게 문제의 조건을 그림으로 나타내면 위와 같다. 결국 1번 상점부터 방문하는 경우, 2번 상점부터 방문하는 경우, \u0026hellip; 이런 식으로 진행해나간 뒤 가장 보석을 적게, 앞의 상점에서 사는 경우를 구하면 된다.\n👎 첫 번째 접근 \u0026lsquo;\u0026lsquo;각 경우마다 직접 구해볼까?\u0026rsquo;\n허걱.. 보석상은 최대 10만개를 입력으로 준다. 각 경우를 매번 직접 N번 상점부터 시작해서 모든 종류의 보석을 사는 경우를 구하는 것은 너무 비효율적일 것 같다.\n👍 두 번째 접근 \u0026ldquo;그러고보니 앞의 경우와 비교하면서 방금 제외된 보석만 추가되는 경우를 참고하며 구하면 되지 않을까?\u0026rdquo;\n즉 그림을 보면 두 번째 경우는 앞의 경우(첫 번째 경우)와 비교했을 때 A가 줄었고, 내가 방문한 보석상 중 A가 없다면 A를 구매할 때까지 보석상을 방문하면 된다. 세 번째 경우는 그와 반대로 앞의 경우(두 번째 경우)와 비교했을 때 B가 줄었지만 내가 방문한 보석상 중 B를 판매하는 곳이 있기에 새로운 보석상을 방문할 필요가 없다.\n=\u0026gt; 오호라\u0026hellip; 추상적으로 접근 방식을 바라봤을 때는 효율적인 것 같다! 이제 구현에서만 잘 구현하면 되겠다.\n구현 👍 올바른 구현 # 풀이법이 떠오르지 않는다. 처음 보는 유형이다... # 그냥 1번에서 시작해서 다 포함하는 경우 # 2번에서 시작해서 다 포함하는 경우 이렇게 다 세보면 효율성이 오바일까..? 최대 10만칸임... # 약간 dp스럽게 전꺼를 바탕으로 생각하는 게 좋겠다. # 전꺼를 없앤 대신 걔가 포함되어있으면 ㄱㅊ은 거니까. def solution(gems): end = -1 gems_set = set(gems) shopping_bag={} for gem in gems_set: shopping_bag[gem] = 0 for i in range(len(gems)): gem = gems[i] gems_set.discard(gem) shopping_bag[gem] += 1 if len(gems_set) == 0: end = i break answers = [(end+1, 0, end)] for i in range(1, len(gems)): found, end = shop(gems, i, end, shopping_bag) if not found: break length = end - i + 1 if length == answers[0][0]: answers.append((length, i, end)) elif length \u0026lt; answers[0][0]: answers = [(length, i, end)] # print(answers) answer = sorted(answers)[0] return [answer[1] + 1, answer[2] + 1] # end 포함 def shop(gems, start, end, shopping_bag): left_gem = gems[start-1] is_complete = False new_end = -1 shopping_bag[left_gem] -= 1 if shopping_bag[left_gem] == 0: for i in range(end+1, len(gems)): shopping_bag[gems[i]] += 1 # 이전에 뺸 녀석을 찾은 경우 if gems[i] == left_gem: is_complete = True new_end = i break else: is_complete = True new_end = end return is_complete, new_end 내가 방문하면서 구매한 보석들의 각각의 개수를 딕셔너리를 통해 관리했다.\n이전 시도 때 구매한 보석들의 각각의 개수를 참고해 만약 이전에 1번 보석상에서 A 보석을 구매하는 것을 시작으로 했다면 이번 시도에는 2번 보석상에서 어떤 보석을 구매하게 될 것이다. 이 경우 이젠 1번 보석상에서 A 보석을 구매할 일이 없으니 딕셔너리에서 A 보석의 개수를 1개 줄여주고 시작을 하는 것이다. 이후 구매 개수가 0개가 되는 보석이 없는지 확인하고 존재한다면 해당 보석을 구매할 때까지 계속해서 보석상을 방문해나가면 된다.\n보석의 모든 종류를 구하기 위해서는 Set 자료 구조를 이용했다. 구매한 보석을 순서대로 리스트로 관리하며 리스트에 내가 구매한 보석이 있는지 찾는 방식은 __contains__ 를 이용하게 되고 이 작업만 놓고 보면 시간 복잡도가 O(n)이다. 반면 딕셔너리에 각 보석의 구매 개수를 이용해 구매하지 않은 보석이 있는지 확인하는 작업은 O(1)이다. 따라서 딕셔너리를 이용해 보석을 구매했는지를 확인하는 방식을 이용했다. 👎 실패했던 구현 이 부분은 개인적으로 기억하려는 의도로 작성한 것이기 때문에 설명이 이상할 수 있습니다.\nlist를 이용해 내가 구매한 보석을 순서대로 담았던 방식 위에서도 언급은 했지만 list를 이용해 내가 구매한 보석을 순서대로 담은 뒤 어떤 보석을 적어도 한 번 구매한 적이 있는지를 in 으로 찾는 방식을 이용한 경우이다. 게다가 이전 시도에서 left_gem을 제거한 뒤 새로운 shopping_bag을 대입하는 과정에서 아래와 같이 O(n)의 복잡도를 갖는 list slicing을 이용하다보니 더욱 비효율적이었다. 프로그래머스의 효율성 테스트를 5개 실패했다.\nshopping_bag = shopping_bag[1:] 1번 방식을 개선해 list slicing이 아닌 dequeue 자료구조 이용하기 list slicing 시 O(n)의 시간 복잡도를 갖는 list가 아닌 앞의 요소를 제거하면서도 O(1)의 시간 복잡도를 갖는 dequeue 자료구조를 이용했다. 하지만 프로그래머스 효율성 테스트를 2개 실패했다.\n즉 요점은 list든 dequeue든 둘 중 무엇을 이용하더라도 O(n)의 시간 복잡도를 갖는 in을 통해 특정 요소가 collection 내에 존재하는 것이 비효율적이므로 딕셔너리를 이용해 O(1)의 시간 복잡도를 갖는 방식을 이용하는 것이다.\n","date":"2020-09-06T19:11:07+09:00","image":"https://umi0410.github.io/blog/algorithm/kakao-gem-shopping/illust_hucf6765420ea389d80eec71a13daa7cfd_88664_120x120_fill_q75_box_smart1.jpg","permalink":"https://umi0410.github.io/blog/algorithm/kakao-gem-shopping/","title":"2020 카카오 인턴쉽 코딩 테스트 - 보석 쇼핑 풀이"},{"content":"Go 언어로 적용해보는 Computer Science의 첫 번째 내용으로 OS 관련 내용 중 이론적으로는 흔하게 접할 수 있지만 실제 적용에 대한 내용은 찾아보기 힘들었던 Mutex, Semaphore에 대해 알아보려한다.\nMutex와 Semaphore은 각각의 추상적인 개념을 바탕으로 OS나 Go 등에서 사용될 수 있기에 세부적인 내용은 문맥에 따라 달라질 수 있다고 생각한다. 예를 들어 Go에서의 Mutex는 주로 sync.Mutex를 이용한 서로 다른 Goroutine의 동시 접근에 대한 제어를 의미하는 반면, 다른 프로그래밍 언어나 OS에서는 주로 서로 다른 Kernel thread나 Process에 대한 동시 접근 제어를 의미할 수 있다.\nMutex Mutex는 Mutual Exclusion의 줄임말로 상호 배제를 의미한다. 즉 서로 다른 워커가 공유 자원에 접근하는 것을 제한한다는 말이다. Go에서는 이 \u0026ldquo;워커\u0026ldquo;가 Goroutine이 되고, 컨텍스트에 따라 프로세스가 될 수도, 스레드가 될 수도 그 외의 다른 존재가 될 수도 있다. 공유 자원이란 여러 워커가 동시에 접근하는 자원을 말하고 이 공유 자원에 대한 동시 접근을 제한해 thread-safe하게 작업하고자 하는 영역을 Critical section이라고 한다. Cricical section은 Mutex가 Lock을 수행한 뒤 Unlock되기 전까지의 영역이며, Lock과 Unlock은 Atomic한 작업이기때문에 어떠한 경우에도 동시적으로 수행될 수 없다.\nmutex-1.png 이해하기 쉽게 예시를 통해 접근해보겠다. 원래 화장실 예제를 보고 굉장히 와닿았는데, 화장실보다는 피팅룸이 좀 더 청결한 것 같아서 피팅룸으로 예를 들어보겠다. 우리 귀여운 고퍼가 피팅룸을 한 개만 운영하는 옷가게에 방문했다고 가정해보자.\nmutex-2.png 피팅룸 - Critical section 고퍼가 피팅룸에 들어가서 옷을 입는 작업은 하나의 Goroutine 동시에 여러 고퍼가 하나의 피팅룸에서 옷을 갈아입으려한다면 😟 난처한 상황이 발생할 것이다. 그렇기때문에 피팅룸에는 Lock, Unlock 기능이 존재해야하는데, 이 기능을 구현한다해도 Atomic하게 동작할 수 있도록 제대로 구현하지 않는다면 한 워커가 Lock을 하는 사이에 다른 워커가 동시에 Lock을 걸려할 수 있고, 그 경우 두 워커가 동시에 한 피팅룸(Critical Section)에서 작업을 하려할 것이다. 그렇기때문에 Lock, Unlock은 Atomic해야하며 이렇게 각 워커를 상호 배제시켜 동시에 작업할 수 없도록 하는 것이 바로 Mutex이다.\nMutex를 이용한 Go program - Counter Count = 0에서 시작해 동시적으로 +1을 10만 번, -1을 10만 번 수행하는 카운터 프로그램\n그럼 실생활에 비유한 Mutex는 이 정도로 마치고, Mutex가 어떻게 사용되는지 Go로 짠 간단한 Counter 프로그램을 통해 알아보자.\nMutex를 잘 적용한 경우 - 몇 번을 수행하든 결과는 0. Mutex가 적용되지 않은 safe하지 않은 경우 - 결과가 0이 나오지 않을 수 있다. 특히 Go에서는 sync 패키지의 Mutex라는 type을 통해 간단하게 Mutex 기능을 이용할 수 있다. 동일한 Mutex struct를 참조한다면 같은 Key를 이용한다는 개념이고, mutex.Lock()을 호출한 뒤 mutex.Unlock()이 호출되기 이전까지가 Critical section이 되며 다른 goroutine들은 같은 Mutex(즉 키)에 의한 Critical section에 진입할 수 없다.\n내가 처음 Mutex를 처음 접했을 때 한 가지 헷갈렸던 것은 당시 Critical section이라는 개념이 없었기 때문에 Lock을 호출하기만 하면 런타임에 알아서 스마트하게 Lock과 Unlock사이의 변수들에 대한 접근 중 동시적인 접근만을 잠시 블락해주는 줄 알았는데, 사실 그 사이 변수들중 동시적으로 접근하려는 변수에 대해서만 잠시 블락해주는 게 아니라 Lock과 Unlock 사이를 Critical section으로 만드는 것이었다.\n자 그럼 Count라는 하나의 int형 변수에 동시적으로 접근을 하는 상황을 극대화하기 위해 여러 고루틴으로 작업해보겠다. +1을 1만 번하는 고루틴을 10개, -1을 1만 번하는 고루틴을 마찬가지로 10개 이용하겠다.\n전체 코드 참고: https://play.golang.org/p/xaLE1YkAdvd\noutput-1.png 우리가 일반적으로 익숙한 Sync 즉 동기적인 순차적 진행 시에는 당연히 10만번 +1, 10만번 -1 후에 결과가 0이었다. 동시적 접근을 수행하자 10만번 +1, 10만번 -1을 했지만 Count += 1 을 수행하던 중 또 다른 Count += 1이 수행되는 등의 원치않던 상황이 야기될 수 있기에 결과값이 0인 경우를 찾기 힘들다. 여러 Goroutine이 concurrent하게 진행하되 Count += 1, Count -= 1 과 같은 동시적 접근이 수행되어서는 안되는 영역은 Critical Area로 설정해 상호 배제적으로 작업되게 하기 위해 Critical section의 전/후에 sync.Mutex를 이용해 Lock, Unlock 기능을 넣어주자 기대했던 대로 0의 결과를 얻을 수 있었다. 💡 Mutex.. 그래서 언제 써요? 사실 Mutex라는 개념이 그렇게 어려운 것도 아니고, 사용법 자체가 어려운 것도 아니다. 나는 하지만 항상 \u0026ldquo;언제\u0026rdquo;, \u0026ldquo;왜\u0026rdquo; 써야하는지를 궁금해하는 편이다.\nCritical section에서 많은 시간이 소요되는 경우? ⇒ ❌ 위의 counter와 같은 작업은 대부분의 작업이 critical section 속에 있다고 볼 수 있다. 이 경우 동시적인 작업과 함께 Lock, Unlock을 하며 critical section을 관리하는 것보다 애초에 작업 자체를 동기적으로 수행하는 게 나을 수 있다. 왜냐하면 동시적 작업 속에서 Mutex를 통해 동기적으로 작업할 수 있도록 하는 것에서 오는 오버헤드는 분명히 존재하고, Critical section 밖에서 효율적으로 동시적으로 작업을 진행했다하더라도 critical section에서 병목이 발생해버려 전체적인 Throughput이 안 좋아질 것이다. Critical section에서는 적은 시간이 소요되고, 대부분은 동시적으로 작업이 가능한 경우? ⇒ ⭕ 예를 들어 어떤 API를 여러 번 호출한 뒤 그 응답 중 일부를 계속해서 더해 결과를 내는 Reduce 작업을 수행한다고 치자. 순차적으로 수행할 시 오랜 시간이 소요될 API 호출은 동시적으로 진행하고, 결과에 대한 Reduce 작업만 잠시 Critical section내에서 작업한다면 아주 좋은 성능과 함께 안전하게 작업할 수 있을 것이다! Semaphore semaphore.png Mutex에서 대부분의 Lock이나 Critical section에 대한 내용을 설명했기 때문에 Semaphore에서 더 설명할 내용이 많지는 않다. Semaphore의 동작에 대한 간단한 예시는 위의 그림과 같은데, 5마리의 고퍼가 존재한다해도 동시에 접근할 수 있게하는 고퍼를 3개로 제한한다면 2마리의 고퍼는 피팅룸에 들어가지 못하고 블락된다. Locked와 Unlocked 상태 뿐인 Mutex와 달리 Semaphore는 임의의 개수를 세는 Counter처럼 동작해 임의의 개수의 워커만이 Critical section에 동시적으로 접근할 수 있도록 한다.\nCounter를 예로 들자면 10개의 워커가 존재한다해도 \u0026ldquo;5\u0026quot;를 세는 Semaphore는 한 워커가 Critical section에 진입할 때마다 Counter 값을 1씩 낮추고, Critical section을 탈출할 때마다 Counter 값을 다시 1씩 높인다. 만약 어떤 워커가 Critical section에 진입하려는데 Count 값이 0이라면 초기에 계획했던 대로 5개의 워커가 이미 동시적으로 작업중이라는 의미이므로 이 워커는 한 워커가 Critical section을 나오면서 Counter 값을 다시 1 증가시킬 때까지 Block된다.\nMutex의 Lock과 Unlock이 Atomic하기에 어떠한 경우에도 동시적으로 수행될 수 없었듯이 Semaphore의 Counting 작업 또한 Atomic해야하고 그래야만 Thread-safe한 counter로 동작할 수 있다. Semaphore가 수행하는 Counting 작업은 주로 try를 뜻하는 네덜란드어 Proberen의 앞 글자를 딴 P와 increment를 뜻하는 Verhogen의 앞 글자를 딴 V로 두 가지가 표현하는 듯하다. P는 Count 값이 0이 아니면 작업을 수행하겠다는 의미하며 만약 Count 값이 0이라면 0이 아닌 값이 될 때까지 wait했다가 0 아닌 값이 되면 count 값을 1 감소시키며 작업을 시작한다. V는 작업을 마치며 count 값을 다시 1 증가시키겠다는 의미이다.\nMutex와 Binary semaphore의 유사한 점 동시 접근 워커를 1개로 제한하는 Semaphore의 경우 Count 값이 0과 1 두 개로만 존재할 수 있는데 이를 Binary semaphore라고한다. 특히 이는 Locked와 Unlocked라는 두 가지의 상태만을 갖는 Mutex와 유사하다. 하지만 Binary semaphore는 mutex가 유사한 기능을 할 뿐 동일하지는 않다는 의견이 많다. 그 이유는 Mutex는 Lock 방식을 이용하고 Semaphore는 Signal(신호) 방식을 이용하기 때문이다. Lock 방식의 경우 Lock을 수행한 워커만이 Unlock을 할 수 있는 반면 Signal 방식은 try(작업 시도)를 수행한 워커가 아니더라도 increment(작업 완료) 신호를 보낼 수 있기에 서로 명백히 동작 방식이 다르다는 것이다. (하지만 이 부분에 대해서는 직접 실습해보지는 못했다.)\n*Semaphore를 이해하는 데에 있어 내가 착각해서 헤맸던 부분은 바로 Semaphore의 요점이 thread-safe한 count라고 착각*했던 것이다. 하지만 semaphore의 요점은 여러 워커에 대해서 thread safe한 count 기능을 제공하는 것이 아니라 임의의 숫자만큼의 동시적 접근을 허용하고, 그 이상은 Block 상태로 대기시킨다는 것이다.\nSemaphore를 이용한 Go Program 1 - Counter go에서 Semaphore를 이용하는 방법은 크게 2가지가 있는 것 같다. 아래 두 가지 방법 중 좀 더 Go스러운 channel을 이용해보겠다.\nGo 특유의 자료형인 channel을 이용하기 - channel은 여러 goroutine의 concurrent한 작업간 데이터 전송은 물론이고 동시적인 작업 중 데이터를 편리하게 동기화해주는 녀석이다. 따라서 동기적인 count와 유사한 기능을 내재하고있다. golang.org/x/sync/semaphore의 Weighted를 이용하기 - Semaphore의 P를 Acquire, V를 Release로 구현해 이용할 수 있게 했다. 전체 코드는 Mutex와 동일하며 마찬가지로 다음 링크로 참고해볼 수 있다: https://play.golang.org/p/xaLE1YkAdvd\nBuffered channel을 이용한 Semaphore in Go func DoSemaphore(maxConcurrent int64){ for i := 0; i \u0026lt; 5; i++{ Count = 0 wg := \u0026amp;sync.WaitGroup{} sem := make(chan struct{}, maxConcurrent) wg.Add(20) for i := 0; i \u0026lt; 10; i++{ go Add1Sem(wg, sem) go Sub1Sem(wg, sem) } wg.Wait() fmt.Println(\u0026#34;Concurrent goroutines + Semaphore\u0026#34;, maxConcurrent, \u0026#34;Result(Desired 0):\u0026#34;, Count, \u0026#34;\u0026#34;) } fmt.Println(\u0026#34;=====================================\u0026#34;) } func Sub1Sem(wg *sync.WaitGroup, sem chan struct{}){ defer wg.Done() for i := 0; i \u0026lt; 10000; i++{ sem \u0026lt;- struct{}{} Count -= 1 \u0026lt;-sem } } channel을 이용한 Semaphore 코드를 설명하기 위해 전체 코드 중 일부를 가져와보았다. Semaphore를 이용하기 위해 Unbuffered channel이 아닌 Buffered channel을 이용하는 이유는 다음과 같은 Unbuffered channel의 동작 방식이 Semaphore의 동작 방식과 동일하기 때문이다.\nBuffer의 크기만큼은 동시적으로 channel에 값을 넣으려는 시도가 허용됨. buffered channel이 꽉 찬 경우 채널에서 값을 꺼내지 않는 이상은 추가적으로 Channel에 값을 넣으려시도하는 goroutine은 Block됨. output-2.png (앞서 코드를 링크한 go playground에서 코드를 바로 실행해볼 수 있다.)\n동시 접근 워커를 1개로 제한하는 Semaphore는 Binary Semaphore로서 Mutex를 이용했을 때와 유사하다고 말했듯이 thread-safe하게 count 작업이 이루어져 예상되는 값이었던 0이 출력됨을 확인할 수 있다. 하지만 동시접근이 가능한 워커가 1개 이상이 되면 thread-safe하지 않게 되고, 동시접근 워커 개수가 많아질수록 결과가 더 부정확한 경향이 있는 것으로 나타났다.\n💡 Semaphore.. 그래서 언제 쓰나요? Mutex는 thread-safe한 작업을 할 때 사용하면 되는 것 같은데 Semaphore는 개념은 알겠는데 언제 써야할 지를 잘 모르겠네요. 일정 개수만큼만 동시 접근을 허용하려는 경우가 있을까요?\nSemaphore을 사용하기 좋은 케이스는 동시에 접근할 수 있는 워커 수를 제한하는 경우이고, 이는 주로 전체 작업이 늘어지는 것을 방지하고자 하는 경우에 이용된다. 많은 작업을 동시에 수행하려하면 먼저 수행될 수 있는 작업은 먼저 수행되도록 하기보다는 전체적으로 모든 작업이 늘어지게 되고 CPU나 Memory 리소스를 많이 소모하게 되고 이는 서버의 안정성에도 좋지 않다. Semaphore를 이용해 동시 접근 워커 수를 제한하고자하는 케이스는 Go의 Worker pool 패턴을 이용하는 경우나 Pipeline pattern을 이용하는 경우와 유사하다.(처음엔 Go의 모든 Concurrent pattern들을 개별적으로 구별지으려했었는데, 공부하다보니 일정한 개수의 Worker를 이용하는 Worker Pool pattern, Channel을 기반으로 작업 내역을 쪼개어 실시간 처리하는 Pipeline, 한 채널, 여러 Goroutine을 이용하는 Fan-in Fan-out pattern 등등 다들 유사하고 연관이 되어있더라.)\nSemaphore로 동시 접근 Worker 수를 제한하지 않고 모든 Goroutine을 동시적으로 수행하는 경우엔 어떻게 될까?\nLogical Processor 개수를 훨씬 넘는 모든 Goroutine 동시적으로 작업을 진행할 경우 아무리 Goroutine이 concurrent한 작업 수행에 뛰어난 성능을 보인다할지라도 과하게 많은 수의 Goroutine은 성능 저하를 야기하지 않을까 예상했다. 하지만 user-level thread 혹은 green level thread의 일종인 Goroutine은 OS(혹은 Kernel) thread와 달리 Context switch로 인한 penalty가 거의 없어서인지 거의 Throughput 면에서의 성능 차이가 없었다.\nKernel level thread는 OS가 스케쥴링을 담당하기 때문에 Go 프로그램이 뭐 어떻게 할 수 있는 게 아니지만 Goroutine은 프로그램이 실행되는 동안 Go 런타임이 스케쥴링을 담당한다. 같은 Kernel level thread에 속한 User level thread인 goroutine간의 switch는 cost가 거의 없다. 즉 goroutine이 많든 적든 Kernel level thread간의 context switch cost는 동일하다고 볼 수 있고, 해당 Kernel level thread에 속한 goroutine간의 context switch cost는 거의 없다.\n참고: Kernel level thread에 대한 스케쥴링은 주로 Preemptive 방식을, User level thread에 대한 스케쥴링은 주로 Cooperative한 방식을 이용하지만 Goroutine은 User level thread임에도 Go 1.14 버전부터는 10ns를 기준으로 goroutine을 switch 할 수 있는 asynchronously preemptive한 스케쥴 방식을 지원한다고 한다. 하지만 Go 1.14가 릴리즈된 지 얼마되지 않아서인지, asynchorously preemptive scheduling에 대해서는 명확히 설명된 문서를 찾기 힘들었다. User level thread의 예로는 RxJava in Java, Coroutine in Kotlin, Goroutine in Golang이 있다.\n하지만 Throughput 적인 측면보다는 Machine의 Resource 소모 측면에서는 모든 Goroutine이 동시적으로 수행되는 구조보다는 Semaphore을 바탕으로한 동시에 일정 개수의 워커만이 작업하는 Worker Pool 구조가 훨씬 Memory나 CPU 리소스를 적게 소모하는 듯 했다. 또한 100개의 동시 요청을 수행하는데 동일한 Throughput으로 약 10초가 걸린다고 치면, 요청당 goroutine을 생성하는 경우는 첫 번째 요청도 거의 10초가 걸린 반면 Semaphore을 이용한 경우는 먼저 온 요청은 대체로 빠르게 먼저 처리되는 경향을 보였다. 이 차이는 처음 요청을 보낸 사용자 마저 10초를 기다리게 할 것이냐, 0.1초만에 응답을 받도록 할 것이냐의 차이이다. 또한 전체 작업이 늘어지면 그 작업에 대한 메모리 점유가 지속되기에 메모리 측면에서도 비효율적이다.\nSemaphore을 이용한 Go Program 2 - 이미지 크기 변환기 마침 이번에 프로젝트에서 thumbnail 생성, image 크기 변환, hashed uri 생성 작업 등을 담당하는 image processing 서버를 개발하려했는데, image를 불러오는 I/O 작업 이후의 image processing은 CPU Bound 한 작업이기 때문에 동시적으로 동작하는 Goroutine이 일정 숫자(대게 Logical Processor 개수) 이상으로는 많아져봤자 크게 효율이 없을 것이라 예상했고, 그와 관련해 간단하게 이미지 크기 변환 프로그램을 하나 만들어 테스트 해보았다.\n약 2MB의 이미지에 대한 크기 변환 작업 요청이 동시에 30개 들어왔다는 가정을 했고, semaphore을 이용한 경우 동시적으로 최대 4개의 worker(goroutine)이 작업을 수행할 수 있게, concurrent를 이용한 경우는 30개의 요청 모두 동시적으로 작업을 하는 경우이다.\n(코드 참고(인터넷 액세스를 하는 경우 Playground에서 동작하지는 않는듯하다): https://play.golang.org/p/Vfyw6uCOIuL)\n사실 Logical Processor 8개, RAM 16GB의 개인 노트북으로는 그 차이가 많이 나지는 않아서 AWS EC2 t2.micro instance에서 성능을 테스트해봤다.\nubuntu@ip-172-31-12-2:~$ ./sem semaphore 변환하고자하는 Image들을 메모리에 Load했습니다. 2021/01/18 20:28:13 Elapsed: 1.974380959s ... 생략 2021/01/18 20:28:26 Elapsed: 1.159552779s 2021/01/18 20:28:26 Elapsed: 1.016627316s 2021/01/18 20:28:26 Total elapsed: 14.959516003s ubuntu@ip-172-31-12-2:~$ ./sem concurrent 변환하고자하는 Image들을 메모리에 Load했습니다. Killed 놀랍게도 동시적으로 30개의 요청을 보내는 경우에 30개의 모든 goroutine이 동시에 작업을 시도할 때에는 과도한 리소스 사용으로 인해 아예 OS가 프로세스를 Kill해버렸다. 즉 Semaphore로 동시 접근 Worker 수를 제한하지 않는 경우 t2.micro 인스턴스로 돌리는 image 서버에 동시에 30개의 image resizing 요청이 들어오면 process가 죽어버린다는 말이다\u0026hellip;!\n또한 앞서 말했듯이 동시에 요청하는 사용자가 많아지더라도 Semaphore을 이용한 경우는 나중에 들어온 요청일 수록 처리가 늘어지지만, Semaphore를 통해 동시 작업을 제한하지 않는 경우는 전체 작업이 늘어진다.\n이렇게 Semaphore는 동시 접근 워커 수를 제한하여 전체적인 Throughput 측면보다는 리소스 소모적인 측면과 먼저 처리될 수 있는 작업은 먼저 처리되도록 할 수 있다는 면에서 장점이 있음을 확인할 수 있었다.\n마치며 Mutex나 Semaphore의 개념이나 설명과 같은 이론적인 내용은 구글링을 통해 어렵지 않게 얻을 수 있는 흔한 지식인 반면, 정확히 언제 쓰면 좋을지, 언제 쓰일 수 있을지와 같은 실용적인 내용은 찾아보기 어려웠기때문에 Go를 통해 Mutex와 Semaphore을 이용해보는 간단한 프로그램을 만들어 실습해보았다.\n예제 프로그램으로서 코드를 간결하고 읽기 쉽게 깔끔하게 제공해보고자했는데, 그러기 쉽지 않았던 것 같아 아쉽다. 다음엔 기회가 되면 go의 benchmark test를 이용해보면 어떨까싶다.\n참고 Goroutine과 Goroutine scheduling에 대해 - https://thegopher.tistory.com/3\n화장실에 비유한 뮤텍스와 세마포어 - https://worthpreading.tistory.com/90\nsemaphore in Go https://medium.com/@deckarep/gos-extended-concurrency-semaphores-part-1-5eeabfa351ce\ncooperative vs preemptive - https://medium.com/traveloka-engineering/cooperative-vs-preemptive-a-quest-to-maximize-concurrency-power-3b10c5a920fe\npreemptive scheduling in go - https://blog.puppyloper.com/menus/Golang/articles/Goroutine과 Go scheduler\n","date":"2021-01-20T15:25:54+09:00","image":"https://umi0410.github.io/blog/golang/go-mutex-semaphore/mutex-2_hu253887d6f424b5a6c4c14b3309becda5_68035_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/golang/go-mutex-semaphore/","title":"Go 언어로 적용해보는 Computer Science - Mutex와 Semaphore"},{"content":"시작하며 요즘 Go와 Java 모두를 이용해 개발을 하다보니 각각의 장단점에 대해 느껴볼 수 있었다. Go는 리소스를 적게 먹으며 코드가 간결하고 라이브러리나 프레임워크 또한 심플해서 적용하기 편하다. Java는 이런 저런 기능이 많은 반면 그런 기능을 이용하기 위해 이해해야하는 내용들이 많고, 코드가 투명하지는 않다(다양한 Annotation을 이용하게 되면서 코드가 투명하게 그 동작을 나타내지 않음). Java의 장점 중에서는 특히나 객체지향의 대표적인 언어답게 상속과 다형성을 능력에 따라 자유자재로 이용할 수 있다는 점이 매력적이었다.\nGo 언어를 좋아하는 입장에서 개인적으로 이런 객체지향적인 특징이나 예외 처리를 제외하고는 딱히 Java가 Go에 비해 갖는 장점이 크게 느껴지지 않았다. 예외 처리는 Go가 바라보는 방향이 일반적인 예외 처리와 다르기에 어쩔 수 없지만, 객체지향적의 특징들은 어떻게 적용해볼 수 있을까하는 생각에 공부를 좀 해봤고 그 내용을 정리해본다. (기회가 된다면 Go에서 error를 다루는 철학에 대해 추가적으로 공부해보고싶다.)\n❗Go에 대한 기본적인 내용을 정리해보는 것이 아니라 객체 지향 관점에서 바라본 Go에 대한 내용을 정리해보는 것이므로 Go의 기초 내용에 대한 설명은 생략할 것이므로 Go에 대한 기초 이해가 없다면, 그 부분을 먼저 알아보는 것을 추천한다!\n예시 코드 Go에서 객체 지향을 적용한 간단한 계산기 프로그램을 예시로 작성해보았다. 전체 소스코드를 다 볼 필요는 없겠지만 필요에 따라 참고할 수 있도록 아래와 같이 첨부한다.\nmain 패키지 - /main.go\npackage main import ( \u0026#34;bufio\u0026#34; \u0026#34;calculator/calc\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;strings\u0026#34; ) func main(){ var ( calculator *calc.Calculator = calc.NewCalculator() // 계산기 struct operationUnit calc.OperationUnit // OperationUnit이라는 interface type을 통해 Polymorphism 이용 operationResult float64 // 계산 결과를 담음 operationErr error // 계산 수행에 대한 error을 담음 ) // main package에서는 calc package에 정의된 unexported name인 id에 접근할 수 없다. //calculator.id = \u0026#34;Jinsu Park\u0026#34; scanner := bufio.NewScanner(os.Stdin) // 연산 option을 위해 값을 입력받기 (e.g. 1) fmt.Printf(`A simple calculator program. ======================================================= Operation options 1. Mulitply a, b float 64 2. Sqaure val, square float64 Please input an int for your desired operation. \u0026gt;\u0026gt;\u0026gt; `) scanner.Scan() option, _ := strconv.Atoi(scanner.Text()) // args를 위해 입력받기 (e.g. 10 20) fmt.Printf(\u0026#34;Please input floats as args.\\n\u0026gt;\u0026gt;\u0026gt; \u0026#34;) scanner.Scan() inputs := make([]float64, 0) for _, input := range strings.Split(scanner.Text(), \u0026#34; \u0026#34;){ f, _ := strconv.ParseFloat(input, 64) inputs = append(inputs, f) } // 연산에 대한 multiplexing. 즉 option에 따른 연산을 수행한다는 의미 // operationUnit이라는 OperationUnit interface type을 통해 Polymorphism 이용 switch option{ case 1: operationUnit = calculator.Multiplier case 2: operationUnit = calculator.SquareMultiplier } // Operate라는 기능을 다양한 동작으로 수행할 수 있다. operationResult, operationErr = operationUnit.Operate(inputs...) // result if operationErr != nil{ fmt.Println(\u0026#34;[Error]\u0026#34;, operationErr) } else{ fmt.Println(\u0026#34;Result:\u0026#34;, operationResult) } } calc 패키지 - /calc/calc.go\npackage calc import ( \u0026#34;errors\u0026#34; ) var autoIncrementID int = 1 type Calculator struct{ id int // 제조된 Calculator을 식별하기 위한 ID. 변수명이 소문자로 시작하므로 export 되지 않는다. Multiplier OperationUnit SquareMultiplier OperationUnit } type OperationUnit interface{ IsValidInput(args ...float64) bool Operate(args ...float64) (float64, error) } type MultiplyOperationUnit struct{} type SquareOperationUnit struct{ MultiplyOperationUnit // Embed의 예시 } func NewCalculator() *Calculator{ c := \u0026amp;Calculator{ id: autoIncrementID, Multiplier: \u0026amp;MultiplyOperationUnit{}, SquareMultiplier: \u0026amp;SquareOperationUnit{MultiplyOperationUnit: MultiplyOperationUnit{}}, } autoIncrementID += 1 return c } func (unit *MultiplyOperationUnit) IsValidInput(args ...float64) bool{ if len(args) != 2{ return false } return true } func (unit *MultiplyOperationUnit) Operate(args ...float64) (float64, error){ if !unit.IsValidInput(args...){ return 0, errors.New(\u0026#34;MultiplyOperationUnit의 args가 유효하지 않습니다.\u0026#34;) } return args[0] * args[1], nil } //func (unit *SquareOperationUnit) IsValidInput(args ...float64) bool{ // ... 필요에 따라 정의하면 Override처럼 이용 가능 //} func (unit *SquareOperationUnit) Operate(args ...float64) (float64, error){ // *SquareOperationUnit에 대한 IsValidInput 메소드는 정의한 적 없지만 // Embedding을 통해 일반적인 OOP에서 부모의 메소드를 이용하듯이 이용 가능. if !unit.IsValidInput(args...){ return 0, errors.New(\u0026#34;SquareOperationUnit의 args가 유효하지 않습니다.\u0026#34;) } var ( // val^square 즉 \u0026#34;val의 square 제곱\u0026#34;에 대한 계산 val = args[0] square = args[1] reduced float64 = val // 제곱 연산 중 값을 담아 놓는 변수 cnt = float64(1) // 제곱 연산 루프를 위한 counter err error ) for ;cnt \u0026lt; square; cnt ++{ reduced, err = unit.MultiplyOperationUnit.Operate(reduced, val) if err != nil{ return 0, err } } return reduced, err } preview.png declaration.png Calculator라는 계산기 struct가 존재 ✨ id field는 소문자로 시작하기때문에 외부에서 함부로 접근할 수 없도록 data를 캡슐화 계산기에서 각각의 연산을 담당하는 OperationUnit interface존재 OperationUnit interface는 Input의 유효성을 검사하는 IsValidInput 메소드와 연산을 수행하는 Operate 메소드 존재 각각의 연산을 담당하는 Unit은 OperationUnit interface가 정의한 메소드들을 구현함으로써 duck-typing을 통해 OperationUnit interface로 사용된다. ⇒ ✨ interface를 통한 추상화와 다형성 이용 가능 OperationUnit interface로 사용이 가능한 struct의 예시 MultiplyOperationUnit struct - 곱셈 연산을 담당 SquareOperationUnit struct - 제곱셈 연산을 담당 ✨ Embed를 통해 MultiplyOperationUnit을 상속한 것처럼 field와 method를 사용 가능 객체지향적 관점에서의 Go에 대해 Go는 아무래도 객체 지향 언어라고 하지는 않는 듯하다. 이에 대해선 다양한 의견이 있는 것 같은데 Post-OOP 언어라는 사람도 있고 OOP 언어는 아니지만 Object-Oriented하게 할 수 있으므로 OOP 언어이면서 OOP언어가 아니라는 사람도 있다.\nOOP에는 크게 4가지 원칙이 있다.\n연관된 변수와 함수를 클래스로 묶으며 외부에서 특정 데이터나 기능에 접근하지 못하도록 하는 정보를 은닉해주는 캡슐화 부모 객체의 field, method를 자식 객체가 이용할 수 있도록해주고, Override할 수 있게 해주는 상속 세부 사항은 제외하고, 어떤 기능이 존재하는지 등의 추상적인 정보만으로도 이용할 수 있게 해주는 추상화 즉 하나의 타입이 여러 타입으로 이용될 수 있으며, 각각이 다양하게 동작할 수 있는 다형성 그리고 대표적인 OOP 언어인 Java는 이러한 내용들을 아주 잘 이용할 수 있게끔 되어있다. OOP 언어가 아닌 Go에서 이러한 OOP의 특징이자 장점인 요소들을 어떻게 적용할 수 있을지 알아보도록하겠다.\n💊 Encapsulation(캡슐화) Go에서는 Export를 통해 캡슐화를 이용할 수 있다. Export에 대해 간단히 설명하자면 private, public 을 이용해 변수나 함수에 접근 제한을 두는 것이 아니라 이름이 대문자냐 소문자냐에 따라 패키지 외부에서 접근을 제어하는 것을 말한다.\nGo의 Export 이용 방법은 많이 찾아볼 수 있으니 캡슐화에 초점을 맞춘 그 쓰임에 대해 알아본다.\n// Calculator에 대한 정의와 구현을 담당하는 calc 패키지 package calc type Calculator struct{ ID int // 제조된 Calculator을 식별하기 위한 ID. 변수명이 소문자로 시작하므로 export 되지 않는다. ... } 위와 같이 Calculator를 정의하는 calc라는 패키지가 있다고 가정하다. Calculator를 식별하는 ID에 대한 작업은 calc 패키지에서 담당하고 함부로 외부에서 값을 바꾸지 못하도록하고싶은 경우 field가 소문자로 시작하도록 함으로써 외부 패키지에서 직접 접근하지 못하도록할 수 있다.\n필요에 따라 Java에서 그러하듯 getter와 setter를 정의해줄 수도 있다.\npackage main ... func main(){ var ( calculator *calc.Calculator = calc.NewCalculator() ) // main package에서는 calc package에 정의된 unexported name인 id에 접근할 수 없다. //calculator.id = \u0026#34;Jinsu Park\u0026#34; } 앞서 말했듯이 이렇게 calc 패키지가 아닌 외부 패키지(예를 들어 main 패키지)에서는 unexported name인 id에 접근할 수 없다.\n주로 캡슐화와 Go에 대해선 은닉을 어떻게 하는가가 요점이라고 생각해 이 부분에 대해 다뤄보았다.\n자세한 내용은? - 연관된 변수와 함수를 묶어주는 내용은 Go의 struct, receiver와 method 등에 대해 검색해보면 더 깊이 알아볼 수 있다.\n👩‍👧‍👦 Inheriatance(상속) Go는 Composition을 이용한 Embedding이라는 방식을 통해 Inheritance와 같은 기능을 이용할 수 있게 해준다는 식으로 많이들 설명을 하는 것 같았으나 주관적인 해석을 해보자면 Go의 Embedding은 Composition이면서 자동으로 embed된 field의 method와 본인의 method인 것처럼 사용할 수 있게 해주기에 Inheritance처럼 이용할 수 있다고 볼 수 있겠다.\nGo의 Embedding은 struct의 field에 별도의 name이 아닌 type만을 적어줌으로써 이용할 수 있다. B라는 type이 는 메소드 Say()를 가지고 있는 경우 struct A가 type B를 Embed한다면 A는 B의 Say 메소드를 두 가지 방법으로 이용 가능하다.\nA.Say() - 이경우 암묵적으로 2.로 변환되어 수행되는 셈 A.B.Say() - 이렇게 명시적으로 Selector(여기선 B)를 적어줄 수도 있다. 일반적인 Composition과 동일하다. Go에서 Embedding을 사용하는 방법 자체 또한 많은 내용을 인터넷에서 찾아볼 수 있으니 상속과 Embedding에 초점을 맞춰 설명해보도록하겠다.\n일반적인 객체 지향적인 방식에서는 type MultiplyOperationUnit struct 라는 type이 존재하고\ntype SquareOperationUnit struct 가 MultiplyOperationUnit type을 상속받는다면 SquareOperationUnit은 IsValidCheck 메소드를 비롯한 MultiplyOperationUnit의 메소드와 멤버 변수를 이용할 수 있을 것이다.\ntype MultiplyOperationUnit struct{} type SquareOperationUnit struct{ MultiplyOperationUnit // type만을 전달함으로써 Embed } // 곱셈 연산에 대한 Input validation func (unit *MultiplyOperationUnit) IsValidInput(args ...float64) bool{ fmt.Println(args) if len(args) != 2{ return false } return true } func (unit *MultiplyOperationUnit) Operate(args ...float64) (float64, error){ if !unit.IsValidInput(args...){ return 0, errors.New(\u0026#34;MultiplyOperationUnit의 args가 유효하지 않습니다.\u0026#34;) } ... } func (unit *SquareOperationUnit) Operate(args ...float64) (float64, error){ // *SquareOperationUnit에 대한 IsValidInput 메소드는 정의한 적 없지만 // Embedding을 통해 일반적인 OOP에서 부모의 메소드를 이용하듯이 이용 가능. if !unit.IsValidInput(args...){ return 0, errors.New(\u0026#34;SquareOperationUnit의 args가 유효하지 않습니다.\u0026#34;) } ... } 이 경우 Go에서는 SquareOperationUnit이 MultiplyOperationUnit을 Embed하도록 한다. 평범한 Compostion 방식으로 이용할 수 있겠지만 Go의 특이한 Embed 방식을 이용함으로써 *SquareOperationUnit 에 대한 IsValidInput 메소드를 정의한 적 없지만 일반 OOP에서 부모 클래스에 정의된 메소드를 이용하듯unit *SquareOperationUnit과 같이 이용 가능하다.\nfunc (unit *SquareOperationUnit) IsValidInput(args ...float64) (float64, error){ ... } OOP에서의 Method Override와 같은 작업을 Go에서 하고싶다면 위와 같이 추가적으로 자신의 타입에 대한 method를 정의하면된다. 메소드를 추가적으로 정의해준 뒤 Selector를 지정하지 않으면 당연히 우리가 바란대로 Embed된 type의 method가 아닌 자기 자신의 method를 호출하게 된다.\n자세한 내용은? - 아무래도 Go의 Embedding과 Composition에 대한 이해가 없다면 무슨 말인지 이해하기 힘들 수 있다. 따라서 해당 내용들에 대해 알아볼 것을 추천!\nAbstraction(추상화) 추상화는 그 객체의 세부 내용이 아닌 공통된 기능을 바탕으로 추려내는 것을 의미한다.\n추상화에 있어서는 Java와 Go가 interface를 이용한다는 점에서 크게 다르진 않다.\ninterface에 추상적으로 해당 interface를 구현하는 type들이 구현하기를 바라는 method를 정의만한다.\ntype OperationUnit interface{ IsValidInput(args ...float64) bool Operate(args ...float64) (float64, error) } 이 경우 OperationUnit는 \u0026ldquo;IsValidInput과 Operate 기능을 수행할 수 있는 무언가\u0026rdquo; 이라고 추상화된 것이다.\n자세한 내용은? - 추상화의 쓰임은 다형성의 쓰임과도 밀접한 연관이 있다. Go에서 interface를 사용하는 패턴과 사용법에 대해 알아보면 좋을 것 같다.\n🌒🌓🌕 Polymorphism(다형성) 다형성이란 한 가지 타입이 경우에 따라 같은 기능에 대해 다양한 동작을 수행할 수 있는 것을 말한다. 추상화는 interface에 대한 정의에 해당하고 다형성은 interface 활용에 해당하는 듯하다.\n일반적인 OOP 언어에서는 interface가 아닌 상속 관계에서도 부모⇒자식으로 타입 변환을 통해 다형성 활용이 가능하다. 하지만 Go는 이를 지원하지 않는다. 이유는 런타임에 동적으로 method dispatch(해당 type의 객체 혹은 value가 어떤 함수를 메소드로 할 지 결정하는 것)을 수행함으로 인한 오버헤드를 줄이기 위해서 컴파일 타임에 정적으로 method dispatch할 수 있게 하기 위해서라고한다.\ntype OperationUnit interface{ IsValidInput(args ...float64) bool Operate(args ...float64) (float64, error) } type MultiplyOperationUnit struct{} type SquareOperationUnit struct{ MultiplyOperationUnit } ... 각종 메소드 정의 생략 func main(){ var ( calculator *calc.Calculator = calc.NewCalculator() operationUnit calc.OperationUnit operationResult float64 operationErr error ) // 연산에 대한 multiplexing. 즉 option에 따른 연산을 수행한다는 의미 switch option{ case 1: operationUnit = calculator.Multiplier case 2: operationUnit = calculator.SquareMultiplier } operationResult, operationErr = operationUnit.Operate(inputs...) // result if operationErr != nil{ fmt.Println(\u0026#34;[Error]\u0026#34;, operationErr) } else{ fmt.Println(\u0026#34;Result:\u0026#34;, operationResult) } } operationUnit이라는 Interface에 다른 type의 struct인 calculator.Multiplier와 calculator.SquareMultiplier가 담길 수 있다.\n이를 통해 operationUnit.Operate() 는 경우에 따라 \u0026ldquo;연산\u0026ldquo;이라는 기능으로 Multipliy 작업을 수행할 수도 있고, SqaureMultipliy 작업을 수행할 수도 있는데, 이를 다형성이라고 한다.\nGo에서의 객체 지향의 한계점과 장점 public class Example { public static void main(String[] args){ // Parent class와 Parent를 extends한 Child class에 대한 구현은 생략한다. Parent parent = new Parent(); Parent child = new Child(); parent.ShowMetaData(); // parent.GetName() 이용 /* Output ======================================= Name: Parent ======================================= */ child.ShowMetaData(); // child.GetName()이용. // 이 때에는 .ShowMetaData()가 Parent class가 아닌 Child class가 Override한 GetName() 이용 /* Output ======================================= Name: Child of Parent ======================================= */ } } 객체 지향 프로그래밍에선 부모 클래스에 정의된 메소드가 내부에서 자식이 Override한 메소드를 이용할 수도 있는데, Go는 그런 기능은 이용할 수 없다는 점이 가장 큰 한계점인 것 같다.\n예를 들어 Parent class의 .ShowMetaData()라는 method가 .GetName()이라는 메소드를 호출하는 경우, java에서는 Child가 GetName을 Override하면 child.ShowMetaData() 호출 시에 Child가 Override한 child.GetName()을 이용하지만, Go는 그럴 수 없다. 필요한 경우 함수를 인자나 field로 전달함으로써 사용할 수 있겠지만, 사용성이 제한적이다. 이 내용에 대해 여기서 설명하면 글이 길어질 것 같아 자세한 묘사는 생략하겠다.\n반면 Go에서의 객체 지향은 장점은 이 글(https://www.toptal.com/go/golang-oop-tutorial)의 후반부에 잘 나와있는데, 굳이 Java를 이용하지 않아도 이렇게 OOP가 충분히 가능하다는 것이 핵심이다. Java의 VM/JIT으로 인한 리소스 부족, 자유도가 떨어지는 프레임워크, 많은 annotation, \u0026hellip; 등등의 단점 없이도 충분히 Go를 통해 가볍게 OOP 할 수 있다는 것이 장점이다.\n마무리 아무래도 객체 지향적 개발을 하는 데에 있어서는 Java가 좀 더 직관적으로 그대로 설계, 구현해서 이용이 가능한 것 같다. 처음 Java를 공부했을 때부터 Java는 객체지향적으로 개발하는 패턴에 대해 수없이 많은 예제가 존재했고, 그 패턴이 명확했던 반면 Go는 명확한 패턴이나 깔끔하게 정의가 없다(이런 식으로 OOP 원칙을 이용해볼 수 있지 않을까~ 정도). 아마 애초에 Go는 객체 지향 언어로 설계하지 않았기 때문이 아닐까싶다.\n이를 계기로 Go에서는 OOP 원칙들이 어떻게 다양하게 적용할 수 있는지 좀 더 자세히 알아볼 수 있었다.\n참고 [번역] Go와 OOP - https://mingrammer.com/translation-go-and-oop/ Golang OOP tutorial - https://www.toptal.com/go/golang-oop-tutorial go object-oriented - https://golangkorea.github.io/post/go-start/object-oriented/ ","date":"2021-01-09T12:46:54+09:00","image":"https://umi0410.github.io/blog/golang/go-vs-java-oop/declaration_hu2809de309117ad963d612eaa5a399aab_35836_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/golang/go-vs-java-oop/","title":"Go vs Java - Go에서의 객체 지향"},{"content":"시작하며 개발 공부를 시작하고 여태까지 몇 년간 데드락을 실제로 접할 일은 없었다. 사실 동시성을 주의해야하는 작업을 해본 적도 없었고, 트랜잭션에 대한 개념도 없었기 때문일 수도 있다. 전공 과목에서 데드락에 대한 내용을 듣고서도 \u0026lsquo;아 이런 게 있구나\u0026rsquo; 정도로만 생각하고 넘겼었다. 하지만 Go 언어를 통해 개발을 하던 도중 Channel이나 Mutex로 인해 종종 데드락을 경험할 수 있었고, 이 경우 프로그램이 완전히 멈춰버리는 크리티컬한 문제가 발생하기도 했고, 디버깅하기 힘든 경우도 있었다. 그런 경험을 하면서 \u0026lsquo;데드락 이 녀석\u0026hellip; 만만치 않구나\u0026lsquo;라는 생각을 하곤 했다.\n이번 글에서는 Golang 즉 Go 언어를 통해 어떤 경우에 Deadlock이 발생할 수 있는지 실제 프로그램을 통해 알아보려한다. Mutex에 대한 Lock과 같이 일반적으로 발생할 수 있는 데드락부터 channel이나 goroutine에 대한 Cooperative scheduling과 같은 Go 언어에 특화된 내용까지를 정리해보았다.\nDeadlock이란 Deadlock이란 교착상태를 의미하며 두 개 이상의 작업이 서로 상대방의 작업이 끝나기만을 기다리고 있기 때문에 결과적으로 아무것도 완료되지 못하는 상태를 가리킨다. - 위키백과 -\n데드락의 개념 자체는 그리 어렵지 않고, 예시를 통해서 쉽게 이해가 가능하다. 위키 백과에 나온 예시를 인용하자면 하나의 사다리에 위에서 내려오려는 사람, 아래에서 올라가는 사람이 동시에 올라가있으면 아무도 내려오거나 올라가지 못하는 경우를 예로 들 수 있다.\n교착 상태의 조건 더 자세히 정의나 의미에 대해 설명할 것은 없을 듯하고, 학문적으로는 주로 아래의 4가지 조건을 통해 발생한다고 설명하는 것 같다.\n상호 배제 (Mutual exclusion) - 하나의 자원을 동시에 사용하지 못하도록 하는 것 점유 상태로 대기 (Hold and wait) - 하나의 자원을 소유한 상태로 다른 자원을 기다리고 있는 상태 선점 불가 (No preemption) - 컴퓨터 분야에서의 선점은 한국말의 선점과 다소 다른 의미. process 혹은 goroutine의 자원을 빼앗는 것을 선점이라고 함. 선점 불가란 그럴 수 없는 상태. 순환성 대기 (Circular wait) - 각 프로세스가 순환적으로 다음 프로세스가 요구하는 자원을 가지고있다. 쉽게 2개의 프로세스를 예로 들면 A는 B가 소유 중인 자원을, B는 A가 소유 중인 자원을 얻으려고 대기 중인 상황을 말함. Deadlock의 개념 자체가 어렵다거나 위의 4가지 조건을 암기하는 것이 중요한 것은 아니라고 생각한다. 중요한 것은 실제로 어떤 경우에 데드락이 발생할 수 있을 지 파악하고 주의하는 것과 그 경우 어떻게 해결할 수 있을지 인지하는 것이라고 생각한다. 그럼 Go 언어로 간단한 프로그램을 짜보며 알아보자.\nGo에서 발생할 수 있는 Deadlock Channel 주로 채널을 접한 지 얼마 안 되어 그 동작 방식을 잘 이해하지 못한 채 사용할 경우 채널로 인한 데드락이 발생한다.\nUnbuffered channel unbuffered-channel-1.png 특히나 unbuffered channel에 대한 미숙한 사용은 자주 데드락을 야기한다. unbuffered channel에서 발생하는 일반적인 데드락의 의미인 2개 이상의 작업 서로의 작업이 완료되기를 대기하는 교착 상태와는 약간 다르다고 볼 수도 있다. 왜냐하면 sender와 receiver 중 누군가가 먼저 작업을 끝내야지 그 다음으로 누군가가 작업을 수행할 수 있는 것이 아니라 서로 동시에 협력해야만 unbuffered channel에 대한 대기를 끝낼 수 있는데 이 경우는 동시에 협력해줄 그 누군가(receiver)가 없는 경우이기 때문이다.\nunbuffered-channel-2.png 그렇다고해서 위와 같이 자기 혼자 send와 receive를 하려해봤자 Unbuffered channel은 sender와 receiver가 모두 ready여야 작업을 진행할 수 있기 때문에 불가능하다. (Unbuffered channel의 동작에 대해 좀 더 궁금하신 분들은 제가 번역에 참여한 A Tour of Go를 참고해주시면 감사하겠습니다!)\nunbuffered-channel-3.png 따라서 다른 goroutine에서 A에 대한 receiver 역할을 해주면 된다.\nBuffered channel Buffered channel을 이용하면 어떨지 좀 더 자세히 들어가보자. 독자분께서 Go의 channel에 대해 별 관심이 없다면 패스~!\nbuffered-channel-1.png 맨 처음에 Unbuffered channel에서는 위와 같이 한 goroutine에서 sender와 receiver 역할 모두를 수행하려해도 이미 sender에서 block이 걸려버려 deadlock을 야기한다고 했으나 buffered channel에서는 그렇지 않다! buffered channel은 buffer size까지는 입력 작업이 블락되지 않기 때문이다.\nbuffered-channel-2.png 하지만 buffer size를 넘어서는 순간부터는 receiver가 channel 내의 아이템을 꺼내어 줄 때까지 block되어버리므로 주의해야한다.\nMutex.Lock의 중첩 Mutex의 구현에 따라 다르겠지만 Go에서는 Lock이 걸린 자물쇠에 다시 자기가 Lock을 걸려해도 Unlock이 될 때까지는 Lock을 걸 수 없다. 즉 어떤 Mutex에 Lock을 건 것이 자신(Goroutine)이라 해도 해당 Mutex에 또 다시 Lock을 걸려하면 그 작업은 Mutex가 Unlock 될 때까지 블락되고 결과적으로는 Deadlock 상태가 되어버린다.\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) var ( scores = []int{10,30,20} Mutex = \u0026amp;sync.Mutex{} ) // Lock을 건 뒤 안전하게 topScore를 구함. func GetTopScore() (playerIndex, topScore int){ Mutex.Lock() topScore = -1 for idx, score := range scores{ if topScore \u0026lt; score{ playerIndex = idx topScore = score } } Mutex.Unlock() return } // GetTopScore는 이미 Lock을 이용해 thread safe하게 작업하는데 // 그걸 인지하지 못하고 실수로 그 밖인 Award에서 Lock을 걸어버림. func Award(){ Mutex.Lock() playerIdx, topScore := GetTopScore() Mutex.Unlock() fmt.Printf(\u0026#34;%d가 %d의 점수로 우승했습니다!\u0026#34;, playerIdx, topScore) } func main(){ Award() } // deadlock 발생 위의 예시에는 thread-safe하게 top score를 구하는 GetTopScore 함수가 선언되어있다. 하지만 Award 함수에서 이미 GetTopScore는 scores에 대해 thread-safe함에도 불구하고 Lock을 걸고 GetTopScore를 수행하려하기 때문에 데드락이 발생한다. 이 경우에는 inner인 GetTopScore 혹은 outer인 Award 둘 중 한 곳에서만 Lock, Unlock 작업을 수행하도록 해주어야한다.\ninner에서 Lock/Unlock을 담당하는 게 옳을 지 outer에서 Lock/Unlock을 담당하는 게 옳을 지는 잘 모르겠다. inner에서 Lock/Unlock을 담당하면 outer에서는 데드락으로 인해 절대 Lock/Unlock을 못한다는 단점이 있지만, 해당 작업은 언제나 Atomic하게 thread-safe하다는 것이 보장된다는 장점이 있다. 반면 outer에서 Lock/Unlock을 담당하도록하면 비교적 자유자재로 Lock/Unlock을 명령할 수 있는 반면 thread-safe해야할 내부 작업이 unsafe해질 수도 있다. 실수로 Lock/Unlock 작업을 잊어버릴 수 있기 때문이다. 하지만 무엇보다도 inner에서 Lock/Unlock을 담당할 지 outer에서 담당할 지를 정확히 정할 수 없는 이유는 outer도 결국엔 또 다른 outer의 inner가 될 수 있기 때문이라고 생각한다.\nCooperative scheduling 방식에서의 무한 Loop Goroutine scheduler가 Goroutine을 스케쥴링 하는 방식은 Go 1.14 이전까지는 Cooperative 방식이었으나, Go 1.14 부터는 Preemptive한 방식으로 바뀌었다고 한다. OS가 Go process의 thread를 스케쥴 하는 방식은 OS 마다 다르겠지만 대체로 preemptive할 것이고 여기서 얘기하려는 스케쥴러는 Goroutine을 스케쥴링하는 Goroutine scheduler임을 주의하자.\nGoroutine은 syscall와 mutex, channel, 함수 콜 등으로 인해 switch 될 수 있는데, cooperative 방식을 이용하는 경우에는 이러한 goroutine switch 조건에 해당하지 않는 경우 임의로 goroutine switch 함수를 호출하지 않는 한은 같은 스레드 내의 다른 goroutine은 절대로 실행될 수 없다. preemption(선점) 즉 다른 goroutine을 block 상태로 만들어버리고 자신이 CPU를 선점하는 것이 불가능했기 때문이다. 하지만 Go 1.14부터는 약 10ms를 기준으로 preemption을 수행하고 있다. Asynchronous preemption이라고 부르는 것 같은데, 정확히 왜 asynchronous인지, 기존의 preemptive schedule과는 무엇이 다른지는 찾아봤으나 제대로 설명되어있는 곳을 찾지 못했다.\n앞으로 이어지는 내용은 데드락에 대한 예시라기 보다는 \u0026lsquo;cooperative 스케쥴링과 preemptive 스케쥴링의 차이로 인해 데드락이 발생할 수도 있고 발생하지 않을 수도 있구나\u0026rsquo;에 대해 알아보는 예시이다.\npackage main import ( \u0026#34;fmt\u0026#34; ) func Foo(){ fmt.Println(\u0026#34;No Deadlock. 나도 실행될 수 있어!\u0026#34;) } func main(){ go Foo() dummy := 0 // 그냥 for 문 안에서 dummy 작업을 수행하기 위한 변수 for { dummy += 1 } } for 문 안에서는 함수콜도 syscall도 channel/mutex도 아닌 goroutine swtich와 관련 없는 dummy += 1 작업만을 수행하고 있다. 이 경우 go Foo()를 통해 goroutine을 생성하긴 하지만 그 goroutine은 바로 실행되는 것이 아니라 대기 상태이며 만약 스레드가 하나라면 cooperative 스케쥴링으로는 Foo라는 goroutine이 실행될 수가 없을 것이다.\n# Go 버전 \u0026gt;= 1.14 일 때에도 Deadlock 발생 $ GOMAXPROCS=1 GODEBUG=asyncpreemptoff=1 go run main.go GOMAXPROCS=1 - Go 프로그램이 사용할 최대 OS Thread 개수를 1개로 제한함으로써 여러 스레드에서 Goroutine이 실행되는 것을 방지. (데드락을 야기해보려는 조건) GODEBUG=asyncpreemptoff=1 - Go 1.14부터 적용된 Asynchronous preemptive scheduling을 사용하지 않는 옵션 이 경우 Go 버전이 1.14 이상이라면 Foo는 실행되지 않는다.\n# Go 버전 \u0026gt;= 1.14인 경우 async preemption으로 인해 Deadlock 발생 X $ GOMAXPROCS=1 go run main.go No Deadlock. 나도 실행될 수 있어! 하지만 asyncpreemptoff 옵션을 생략하면 기본적으로 선점형 스케쥴링이 지원되므로 이 경우엔 Deadlock이 발생하지 않고 Foo() 함수가 실행되는 것을 \u0026ldquo;No Deadlock. 나도 실행될 수 있어!\u0026rdquo; 라는 Stdout을 통해 볼 수 있다.\npprof를 이용한 goroutine schedule 시각화 하지만 정말로 async preemption이 동작했기 때문에 데드락에 빠지지 않은 것인지 다른 이유 때문인지는 그닥 직관적으로 와닿지 않는다. 그래서 몇몇 외국 블로그의 글에서 봤던 pprof라는 도구를 사용해봤고, 처음엔 사용법이 다소 어려웠지만 조금 익숙해지니 너무나도 편리했다. pprof는 net/http 패키지 하위에 존재하고, Goroutine scheduling, syscall log, CPU 사용 등을 시각화해서 보여주는 간편한 디버깅 도구이다. 요청을 날리면 요청 이후 N초 간의 goroutine scheduling에 대한 정보를 기록해 보여주는 기능을 이용해보았다.\npprof-debugging-1.png End Stack Trace를 통해 어떤 작업으로 인해 goroutine이 잠시 중단되고 CPU를 다른 goroutine에게 양보하게 되는지 알 수 있다. 놀랍게도 dummy에 대한 무한 루프 진행 도중 처음으로 async preemption이 발생한 뒤 이어서 Foo 함수를 실행하는 goroutine이 CPU를 점유하게 된다는 것을 시각적으로 볼 수 있다.\nWall Duration은 해당 고루틴 블럭을 수행한 시간으로 보여지고 약 10ms를 기준으로 preemptive하게 switch가 일어날 수 있다는 여러 블로그의 글들과 유사하게 15,021,904ns, 즉 약 15ms만에 asyncPreempt라는 이벤트로 인해 goroutine switch가 일어났다. cooperative 스케쥴링만을 이용하는 경우에는 데드락으로 인해 한 번도 Foo를 실행하는 goroutine이 수행되지 못한다는 것도 시각화해서 제공해보고싶었지만, 당연하게도 그 데드락으로 인해 일정 기간동안 runtime을 관찰한 뒤 그 정보를 저장하는 goroutine 조차 실행할 수 없어 그 정보를 얻을 수 없었다!\n# Go의 버전을 1.13으로 제한해본다. FROM golang:1.13 WORKDIR /app COPY . . ENTRYPOINT [\u0026#34;go\u0026#34;] CMD [\u0026#34;run\u0026#34;, \u0026#34;main.go\u0026#34;] $ docker build . -t tmp \u0026amp;\u0026amp; \\ docker run -it --rm -e GOMAXPROCS=1 tmp 그렇다면 정말 Go의 1.14 이전 버전은 Preemptive한 방식이 아니라 Cooperative한 방식을 이용하고, 이 경우 데드락이 발생할까? 이전 글들과 마찬가지로 Docker를 이용해 간편하게 Go의 버전을 변경해서 실행해보자. golang:1.13 이미지를 이용해보았다.\n1.13 버전 이하로는 GODEBUG=asyncpreemptoff=1 옵션을 설정하지 않아도 cooperative한 스케쥴링만을 지원하므로 프로그램이 데드락 상태에 빠져 Foo()가 실행되지 못함을 알 수 있다.\n참고 사항) GOMAXPROCS=1 옵션을 주는 이유는? The GOMAXPROCS variable limits the number of operating system threads that can execute user-level Go code simultaneously. - go rutime package 문서 -\nGOMAXPROCS는 Go 프로그램이 사용할 수 있는 최대의 OS 스레드 개수를 의미한다. 만약 OS 스레드가 2개 이상으로 생성된다면 위의 경우 main goroutine과 함수 Foo를 실행하는 Goroutine이 서로 다른 스레드에 배치될 것이고, 그 경우 Go의 스케쥴러가 Cooperative한 방식을 이용한다해도 OS 스케쥴러가 Preemptive하게 각각의 Go 스레드(OS Level)를 스케쥴하기 때문에 Foo goroutine도 실행될 수 있는 기회가 주어진다. 이 경우에는 데드락이 발생하지 않는다. 따라서 우리는 데드락을 발생시켜보고자 GOMAXPROCS를 1로 제한한다.\n마치며 Go를 이용해 실제 프로그램을 짜보며 어떤 경우에 Deadlock이 발생할 수 있는지 알아보았다. 이 글에선 서로 같은 Mutex를 이용하는 경우를 예시로 들었지만 서로 다른 두 Mutex를 통해 서로의 작업을 기다리는 경우의 Deadlock도 거의 유사하며 일반적으로 말하는 Deadlock에 가장 가까운 경우이긴할 것이다. 그래도 Go로 개발을 하면서 아직 서로 다른 Mutex를 이용했던 경우가 딱히 없었기에 같은 Mutex에 중첩으로 Lock을 걸었던 경우를 예시로 들어보았다.\nChannel이나 Cooperative scheduling의 경우는 어느 정도 Go에 한정적인 내용이고 특히나 스케쥴링은 런타임이나 고루틴 스케쥴 방식까지 내려가는 세부적인 내용이긴하지만 Go에 특히 관심 있으신 분들께는 나름 재미있는 내용이 되지 않았을까싶다.\n그리고 이전 글에선 testing의 benchmark를 이용해 좀 더 정확하고 편리한 벤치마킹을 도입해봤다는 점과 이번엔 추가적으로 pprof를 이용해 goroutine 스케쥴링을 시각화해봤다는 점에서 Golang으로 적용해보는 컴퓨터 사이언스라는 이 시리즈를 처음 시작했을 때에 비해 컴퓨터 사이언스 뿐만 아니라 디버깅 기술이나 스케쥴링 방식 등등 다양한 주제에 대해서도 공부해보고 적용해볼 수 있었던 것 같아 뿌듯하다. 다소 Go만의 지엽적인 내용으로 여겨질 수도 있겠지만 추후에 내가 어떤 언어를 공부하든 어떤 기술을 공부하든 이러한 경험들을 얼마든지 녹여낼 수 있을 것이라 생각한다!!\n참고 Go: How are Deadlocks Triggered? https://medium.com/a-journey-with-go/go-how-are-deadlocks-triggered-2305504ac019\nGo: What Does a Goroutine Switch Actually Involve? https://medium.com/a-journey-with-go/go-what-does-a-goroutine-switch-actually-involve-394c202dddb7\nGoroutine and Preemption https://medium.com/a-journey-with-go/go-goroutine-and-preemption-d6bc2aa2f4b7\nGo: Asynchronous Preemption https://medium.com/a-journey-with-go/go-asynchronous-preemption-b5194227371c\n데드락 정의\nhttps://namu.wiki/w/%EB%8D%B0%EB%93%9C%EB%9D%BD?from=Deadlock https://ko.wikipedia.org/wiki/%EA%B5%90%EC%B0%A9_%EC%83%81%ED%83%9C Go runtime documentation https://golang.org/pkg/runtime/\n","date":"2021-01-31T18:25:54+09:00","image":"https://umi0410.github.io/blog/golang/go-deadlock/pprof-debugging-1_hu27a13bf8dcdafb0aaff224765e3a83ab_59369_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/golang/go-deadlock/","title":"Go 언어로 적용해보는 Computer Science - Deadlock"},{"content":"시작하며 저번 학기에 컴퓨터 구조를 수강하면서 간과하고 있던 로우 레벨의 지식에도 흥미가 생겼었다. 그 중 CPU와 Memory, Disk의 역할에 대해 알아볼 수 있었고 캐시는 CPU와 Memory 사이에 위치해 메모리 대신 빠르게 CPU에게 데이터를 제공하는 녀석이라고 배웠다.\n이전에는 주로 캐시라고 하면 주로 CDN과 같은 네트워크에서 쓰이는 캐시들밖에 몰랐다. 그렇다보니 L1 캐시, L2 캐시 같은 얘기를 들으면 OSI 7계층과 연관 지어 \u0026lsquo;음..? L2 캐시는 스위치에서 쓰는 캐시인가..?\u0026rsquo; 라는 상상을 하곤했다.\n이번에는 Go를 통해 배열에 여러 차례 접근하는 프로그램을 만들어보고 벤치마킹을 통해 캐시라는 녀석이 어떤 효과를 가져다주는지 직접 확인해보려한다.\n캐시란 캐시는 아주 다양한 문맥에서 사용된다. 공통적으로 \u0026ldquo;사용자가 요청할 것 같은 데이터를 작고 빠른 저장소에 저장해놓음으로써 좀 더 빨리 해당 데이터를 제공한다\u0026quot;는 목적을 갖는다. CDN, DB, REST API, Memory, CPU 등등 다양한 곳에서 쓰일 수 있을 것 같다. 그 중 이번에는 CPU와 메모리 사이의 캐시에 대해 알아보겠다.\nCPU와 메모리 사이의 캐시는 메모리의 데이터를 얻기 위해 메모리에 직접 접근하지 않고 캐시라는 빠른 저장소를 이용해 해당 데이터를 얻게끔해준다. 예를 들어 변수 a=10 이라는 데이터가 메모리에 존재한다해도 a의 값을 얻기 위해 메모리에 직접 접근하기 보다는 가까우면서 빠르게 이용 가능한 캐시에서 데이터를 가져올 수도 있다는 것이다. 사실 캐시의 개념적인 측면에서 보면 메모리 또한 디스크 대신 빠르게 값을 전달해주기 위한 경우일 수 있으니 캐시 기능을 한다고 볼 수 있다. 그리고 CPU와 메모리 사이에 정말 캐시라는 이름을 갖는 녀석들은 프로세서 속에 있는 L1 캐시, 프로세서 옆에 있는 L2 캐시, 프로세서들이 공유하는 L3 캐시가 있긴 하지만 이는 시대가 지나면서 얼마든지 변할 수 있는 내용들이기 때문에 어떤 캐시가 어디에 있고 누구랑 누가 공유하는지와 같은 세부 내용은 크게 중요하진 않을 것 같다.\n물리적인 크기나 거리는 속도와 반비례할 수 밖에 없다. 거리가 멀면 정보가 전달되는 속도가 느려지고 크기가 크면 여러 Mux나 Gate를 이용한다는 것이기 때문에 느려진다. 그렇기때문에 캐시는 작고 가까워야한다. 데이터를 요청하는 녀석은 CPU이기 때문에 캐시는 CPU 속 혹은 그 근처에 위치한다. 또한 작아야하기때문에 모든 정보를 담을 수 없고, 사용자가 요청할법한 데이터만을 담아야한다. 이 때 어떻게 사용자가 요청할 법한 데이터를 정할까? 이는 공간 지역성과 시간 지역성이라는 중요한 두 가지 성질을 기반으로 한다.\n이외에도 태그나 충돌 같은 개념들이 있긴하지만 실제로 벤치마킹해보기도 쉽지 않고 다소 지엽적인 내용이라 간단히만 정리해보면 태그 없이 주소값을 모듈러(나머지)연산해서 cache line index를 결정하고 그것만을 이용해 데이터를 저장하면 한 line 내에 저장할 워드(Word)에 대한 충돌이 발생할 수 있다. cache line이 20개인 캐시는 0번지와 20번지가 같은 line이므로 충돌이 발생해 계속해서 같은 line에 서로의 데이터가 번갈아 저장될 수 있다는 것이다. 하지만 태그를 이용하면 cache line 수는 줄어들더라도 한 line내에 여러 태그의 정보를 저장할 수 있게되어 cache line이 10개인 cache의 한 line에 0번지와 20번지의 데이터가 다른 태그로 저장되어 불필요한 충돌을 방지할 수 있다는 장점이있다. 간단히 설명하기는 힘든 내용이라 좀 더 자세히 알고싶다면 Direct mapped cache나 Fully associative cache 등으로 검색해보기를 권장한다.\nSpatial locality Spatial locality(공간 지역성)이란 지금 요청 받은 데이터와 가까운 곳에 위치한 데이터는 높은 확률로 다시 요청 받게 된다는 성질이다. 예를 들어 100번지의 a=10과 108번지의 b=20이 존재할 때 변수 a를 요청하면 이후 a와 가까운 주소에 저장된 b 또한 높은 확률로 요청된다는 것이다.\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { var ( a int = 10 b int = 20 c int = 30 ) fmt.Printf(\u0026#34;a: %p\\nb: %p\\nc: %p\\n\u0026#34;, \u0026amp;a, \u0026amp;b, \u0026amp;c) } /* Output: a: 0xc000100010 b: 0xc000100018 c: 0xc000100020 */ a, b, c의 크기는 8바이트로 주소값 또한 8바이트가 차이난다. (16진법이기에 20과 18의 차이는 8이다.) 즉 대체로 비슷한 시기에 할당된 변수는 근접한 메모리 주소를 갖게 된다. 우리는 비슷한 시기에 할당한 변수 혹은 연속된 배열 요소에 자주 빠른 시일 내에 접근을 하지 맨 위에서 선언한 변수와 저 멀리 맨 밑에서 선언한 변수를 마구잡이로 왔다 갔다 하면서 작업을 하지 않는 편이기 때문에 공간 지역성을 근거로한 캐시가 효력을 갖게 된다. 만약 공간적으로 먼 맨 위의 변수와 맨 아래의 변수를 자주 번갈아가며 접근한다면 그것은 시간지역성을 띄는 경우이다.\nTemporal locality Temporal locality(시간 지역성)이란 최근에 요청했던 데이터는 높은 확률로 다시 요청 받게 된다는 성질이다. 예를 들어 100번지의 a=10과 9999번지의 b=20은 서로 주소적인 거리는 멀지만 둘 다 최근에 호출됐다면 캐시에 적재하겠다는 것이다. 캐시는 주로 직사각형 형태로 생겼으며 가로(행)는 연속된 주소의 데이터를 저장하는 공간 지역성, 세로(열)는 최근에 호출된 데이터를 저장하는 시간 지역성을 담당한다.\n두 지역성 비교 12칸의 캐시가 있다고 가정하자. 가로로 4칸 세로로 3칸 존재한다면 공간/시간 지역성의 균형이 잡힌 캐시라고 볼 수 있다.(경우에 따라 다르겠지만)\nbalanced-cache.png 시간 지역성은 최근 불린 데이터는 다시 불릴 확률이 높다는 것이고 이는 연속된 공간이 아닌 다양한 공간(주소)의 데이터를 캐시에 저장한다는 말이기도 하다. 0번지 부근, 16번지 부근, 24번지 부근의 다양한 공간의 데이터를 저장할 수 있으면서 그 녀석들간의 주변 데이터도 제공하는 공간지역성도 만족한다.\n두 가지 지역성에 의해 다양한 캐시들이 데이터를 적재하고 제공한다. 요점은 캐시는 빠르게 동작해야하고 그러기 위해선 크기가 작고 가까워야하며 크기가 작기 때문에 모든 데이터를 담을 수 없으니 알짜 데이터만을 담아야하는데 그 알짜는 지역성을 기반으로 선별된다는 것이다. 크기가 한정적이기 때문에 한 지역성을 키우면 한 지역성은 작아질 수밖에 없다.\n공간 지역성에 치우친 캐시 구조 spatial-locality-biased-cache.png 한정된 크기의 캐시 속에서 공간지역성을 극대화시켜버리면 당연히 인접한 공간의 자료만 이용할 수 있고, 최근에 불린 데이터들은 안중에도 없고 인접한 공간의 데이터만을 저장하게 된다. 예를 들어 다음과 같은 시간 지역성이 필요한 경우에 제대로 기능을 할 수 없다.\n0~11번지 사이의 데이터가 한 번 접근 ⇒ 캐시에 0~11번지 적재 이후 12번지의 데이터에 접근 ⇒ 캐시에 데이터가 없기때문에 0~11번지의 데이터 대신 12~23번지의 데이터를 캐시에 적재 다시 최근에 접근했던 데이터인 0번지의 데이터에 접근 시도 ⇒ 0번 데이터는 최근에 접근했던 데이터임에도 시간 지역성이 활용되지 못함 ⇒ 캐시에서 데이터를 찾을 수 없음. 시간 지역성에 치우친 캐시 구조 temporal-locality-biased-cache.png 위의 경우 최근에 호출된 다양한 주소의 데이터들을 캐시에 저장해준다. 하지만 캐시의 크기는 한정되어있기 때문에 세로가 길어지면 가로는 짧아진다. 즉 최근 접근을 시도한 다양한 주소의 데이터를 저장할 수 있지만 그 데이터의 인근 데이터에 대한 저장은 많이 할 수 없다는 것이다.\n예를 들어 위의 그림과 같은 경우 최근 0, 3, 12, 18, 6, 20번지의 데이터에 접근했고 이후에도 해당 번지에 대한 데이터를 캐시를 통해 이용할 수 있다. 바로 내가 최근에 접근했던 데이터이기때문이다. 하지만 만약 18번지의 데이터에 접근한 경우 높은 확률로 공간지역성에 의거 19, 20, 21, \u0026hellip; 번지의 데이터에 접근하겠지만, 이 예시는 시간지역성에 치우쳐져 19번지의 데이터만을 캐시에서 제공받을 수 있다.\n프로그램을 통한 벤치마킹 저번에 Mutex, Semaphore를 직접 벤치마킹해보면서 Go의 내장 벤치마크 기능을 이용하면 좀 더 편리하게 결과를 보여줄 수 있을 것 같았기에 이번에 Go의 내장 벤치마크 기능을 이용해봤다.\n1000행 1000열의 2차원 int형 배열의 어떠한 요소에 접근해서 +1 하는 작업을 1000회 수행하는 것을 하나의 싸이클로 하는 벤치마크를 작성했다. 2차원 배열은 가로로는 연속적인 주소값을 갖기에 공간 지역성을 활용할 수 있지만 세로로는 N * (int형 자료형의 크기)씩 차이 나는 주소값을 갖기 때문에 공간 지역성을 활용하기 힘들고, 최근 접근했던 주소라면 시간 지역성은 활용할 수 있다.\n공간 지역성 (가로로 연속적인 데이터)\n공간 지역성을 사용하는 경우 가로로 연속된 요소에 접근. 즉 연속된 주소를 갖는 1000개의 요소에 접근 공간 지역성을 사용하지 않는 경우에는 세로로 요소에 접근. 즉 1000 * int 자료형의 크기만큼 차이나는 연속되지 않은 주소의 1000개의 요소에 접근 시간 지역성 (연속적 주소와 상관없이 최근에 불린 데이터)\n시간 지역성을 사용하는 경우 주소값이 근접하진 않지만 4개 혹은 16개의 데이터에만 계속해서 접근 시간 지역성을 사용하지 않은 경우에는 계속해서 처음 접근하는 데이터에만 접근 package main import ( \u0026#34;testing\u0026#34; ) var ( Size int = 1000 ) func generateArray() [][]int{ arr := make([][]int, Size) for i := 0; i \u0026lt; Size; i++{ arr[i] = make([]int, Size) for j := 0; j \u0026lt; Size; j++{ arr[i][j] = 0 } } return arr } func BenchmarkSpatialLocality(b *testing.B){ b.Run(\u0026#34;공간지역성 사용\u0026#34;, func(b *testing.B) { arr := generateArray() b.ResetTimer() for n := 0; n \u0026lt; b.N; n++{ for i := 0; i \u0026lt; Size; i++{ arr[0][i] += 1 } } }) b.Run(\u0026#34;공간지역성 X\u0026#34;, func(b *testing.B) { arr := generateArray() b.ResetTimer() for n := 0; n \u0026lt; b.N; n++{ for i := 0; i \u0026lt; Size; i++{ arr[i][0] += 1 } } }) } func BenchmarkTemporalLocality(b *testing.B){ b.Run(\u0026#34;시간 지역성 적극 사용. 최근 접근한 데이터 4개.\u0026#34;, func(b *testing.B) { arr := generateArray() b.ResetTimer() for n := 0; n \u0026lt; b.N; n++{ for i := 0; i \u0026lt; Size; i++{ arr[i%4][0] += 1 } } }) b.Run(\u0026#34;시간 지역성 조금 사용. 최근 접근한 데이터 16개.\u0026#34;, func(b *testing.B) { arr := generateArray() b.ResetTimer() for n := 0; n \u0026lt; b.N; n++{ for i := 0; i \u0026lt; Size; i++{ arr[i%16][0] += 1 } } }) // 사실 완벽하게 새로운 데이터는 아님. 벤치마킹하는 동안 계속해서 반복되기 때문에 b.Run(\u0026#34;시간 지역성 X. 새로운 데이터에만 접근.\u0026#34;, func(b *testing.B) { arr := generateArray() b.ResetTimer() for n := 0; n \u0026lt; b.N; n++{ for i := 0; i \u0026lt; Size; i++{ arr[i][0] += 1 } } }) } goos: linux goarch: amd64 pkg: playground/unix-socket/cache BenchmarkSpatialLocality/공간지역성_사용 50000 894 ns/op BenchmarkSpatialLocality/공간지역성_X 50000 6561 ns/op BenchmarkTemporalLocality/시간_지역성_적극_사용._최근_접근한_데이터_4개. 50000 1918 ns/op BenchmarkTemporalLocality/시간_지역성_조금_사용._최근_접근한_데이터_16개. 50000 4153 ns/op BenchmarkTemporalLocality/시간_지역성_X._새로운_데이터에만_접근. 50000 6564 ns/op 결과를 확인해보니 간단한 배열 내의 요소들에 대한 연산인데도 꽤나 차이가 컸다.\n마치며 저번 학기에 컴퓨터 구조를 수강하면서 CPU-Memory 캐시의 효과를 직접 배열에 대한 프로그램을 통해 보여주는 예시를 보고 신기했던 기억이 있어서 이렇게 벤치마킹 프로그램을 작성해봤다. 다른 CS 주요 지식들에 비해 어려울 것은 없는 편이고 우리가 쉽게 접해오던 내용이라 더 이해하기 쉽지 않았을까 싶다.\n참고 캐시란 - https://ko.wikipedia.org/wiki/캐시 캐시가 동작하는 아주 구체적인 원리 - https://parksb.github.io/article/29.html cache mapping - https://m.blog.naver.com/jkssleeky/220478400046 ","date":"2021-01-27T15:25:54+09:00","permalink":"https://umi0410.github.io/blog/golang/go-cpu-cache/","title":"Go 언어로 적용해보는 Computer Science - Cache"},{"content":"시작하며 Go 언어를 처음 배울 때 channel이나 goroutine에 대해 배우면서 Concurrency 즉 동시성으로 인해 많이 힘들었던 기억이 난다. \u0026lsquo;동시성과 병렬성이 도대체 뭐가 다르다는 것이냐..!\u0026rsquo; Rob Pike 아저씨가 동시성에 관해 했던 세미나 영상들을 보며 같이 스터디 하던 멤버들과 멘붕에 빠지기도 했고, 이런 저런 의견 교류도 했다가 며칠 지나고 보면 다시 병렬성과 헷갈리고, 제대로 된 한글 자료는 찾기 힘들었다. \u0026lsquo;동시성은 사람이 느끼기에 동시처럼 느껴지는 것이고 병렬성은 실제로 동시적인 시점에 대한 것이다.\u0026rsquo; 라는 내용이 많았지만 와닿지는 않았다.\n그리고 이번에 다뤄볼 동시성은 주로 \u0026ldquo;제어\u0026ldquo;라는 단어와 함께 언급되는 \u0026ldquo;동시성 제어\u0026ldquo;에서의 동시성이나 Concurrency와는 조금 다른 의미를 갖는다고 생각한다. 동시성 제어에서의 동시성은 그냥 동시에 무언가에 접근하는 것을 어떻게 제어하거나 막겠냐는 의미일 뿐이지만 동시성-병렬성에서의 동시성은 어떻게 작업들이 동시에 수행되느냐에 초점을 맞추는 느낌이기 때문이다.\n동시성 제어의 예시 - 여러 client가 하나의 db row에 access할 때 어떻게 제어할 것인가 이번에 다룰 동시성의 예시 - 어떤 작업을 동시성을 이용해 수행하는 것이 좋을까 병렬성을 이용해 수행하는 것이 좋을까 Golang을 처음 접한 지 벌 써 반년이 넘은 것 같다. 처음 한 4개월 가량은 거의 머릿 속에 아른거리는 물음표 같은 내용이었는데, 꾸준히 공부하다보니 조금 알 것 같다. 이번 글에서는 그러한 내용을 추상적인 말보단 예시와 코드로 명확히 정리해보고자 한다.\nConcurrency is not parallelism 위의 문장은 동시성과 병렬성에 대해 알아보려하면 매번 보게 되는 문장인다. 그렇다. 동시성은 병렬성과 다르다. 이에 대해 설명할 때 영어권에서는 아래의 두 문장이 자주 등장하는 듯 하다.\nConcurrency is about dealing with lots of things at once.\nParallelism is about doing lots of things at once.\n한 번에 dealing with하는 것과 한 번에 doing하는 것이라고 한다. 하지만 잘 와닿지 않는다\u0026hellip; 아마 저 글을 보고 이해할 수 있는 한국인은 극히 드물지 않을까 싶다.\n예시로 보는 동시성 vs 병렬성 따라서 좀 더 쉽게 예시로 설명해보려 한다. 예를 들어 사회적 거리 두기로 인해 우리 Gopher(고퍼)가 친구들과 떠나려는 예약했던 여행이 취소되었고, 항공권, 호텔, 렌트카 예약을 모두 취소해야하는 상황이라고 해보자.\n동시성(Concurrent)의 예시 - 만약 모든 취소 작업이 고객센터 1대1 채팅으로 이루어지는 경우. Gopher1 혼자 모든 취소 작업을 동시적으로 수행 가능 Gopher1 혼자 항공권 취소 신청, 호텔, 렌트카의 고객센터 1대1 채팅에 예약 취소 신청을 함 상담사분이 답장을 주는 대로 작업을 진행 Gopher1 혼자서 세 개의 취소 작업을 동시에 진행하는 셈이다. 병렬성(Parallelism)의 예시 - 만약 모든 취소 작업이 고객센터 전화 상담으로 이루어 지는 경우. Gopher1 혼자 모든 취소 작업을 동시에 진행할 수는 없고, Gopher 1, 2, 3이 병렬적으로 진행할 순 있음. 뒤에서 설명하긴 하겠지만 전자의 경우에만 동시성이 일어날 수 있는 이유는 Gopher가 1대1 채팅으로 작업을 진행할 때는 한가한 반면(CPU 점유율이 낮고, 상담사의 메시지를 기다리느라 많은 블락킹이 존재) 전화로 작업을 진행할 때에는 여러 명과 전화할 수 없기 때문이다.(CPU 점유율이 높음.)\n항공권 취소 전화 도중 잠시 지연되는 시간에 전화를 끊고 호텔 고객 센터에 전화를 거는 방식으로 Gopher1 혼자 Concurrent하게 작업을 진행할 수 있겠지만 이 경우 계속해서 전화를 끊고 다시 걸고 상황 설명을 하는 동안에 지연시간이 발생해서 효율이 좋지 않다. 이 지연 시간을 항공권 취소 전화 =\u0026gt; 호텔 예약 취소 전화로의 context switch penalty로 볼 수 있다.\n프로그램으로 알아보는 Concurrency vs Parallelism 동시성과 병렬성을 비교하는 한글 자료는 찾기 힘든 편이고, 영어로 된 자료는 꽤나 많이 찾아볼 수 있지만, 그래서 \u0026ldquo;언제\u0026rdquo;, \u0026ldquo;무엇을\u0026rdquo;, \u0026ldquo;왜\u0026rdquo; 써야하는 지에 대한 내용은 잘 찾아보기 힘들다. 게다가 간혹 Concurrent programming의 장점이라면서 parallelism을 설명하는 경우도 있어 혼란스러웠다.\nconcurrency-vs-parallelism.jpg 동시성이란 한 작업이 완료된 뒤 다음 작업이 수행되는 것이 아니라 계속해서 적절히 switch 되며 동시에 진행되어나가는 것을 의미한다. 그 결과 일정 기간동안 수행한 작업들에 대해 말할 때 그 작업들은 동시에 수행됐다고 할 수 있게 된다. 한 순간에는 엄밀히 따지면 한 작업이 수행된다.\n병렬성이란 여러 코어에서 작업이 병렬적으로 진행되는 것을 의미한다. 그 결과 일정한 기간이 아닌 순간을 놓고 봤을 때에도 여러 작업이 병렬적으로 동시에 수행되게 된다.\n코어 수 보다 많은 스레드가 이용될 경우는 동시성과 병렬성이 함께 이용된다고 볼 수 있다. 즉 여러 스레드에 걸쳐 여러 작업이 완료 되지 않았지만 적절히 switch되며 진행된다는 의미이다.\n프로그래밍적으로 봤을 때 같이 동시성은 블락이 많이 걸리는 작업에 유리하고, 병렬성은 CPU bound한 작업에 유리하다고 생각된다. 그럼 좀 더 자세히 동시성과 병렬성이 \u0026ldquo;언제\u0026rdquo; \u0026ldquo;왜\u0026rdquo; 유리한지 알아보자.\nConcurrency - Block이 많은 작업에 유리 예상: 블락이 많은 작업의 경우 Core 숫자에 상관 없이 대체로 많은 thread(혹은 goroutine)을 생성할 수록 작업이 빨라질 것이다.\nblock이 많은 작업이 뭐가 있을까? 네트워크 IO 블락이 많이 걸릴 작업으로서 https://example.com 에 요청/응답을 얻는 작업을 64번 수행해보겠다. core의 숫자를 1개로 제한하여 concurrency의 효과를 관찰해보기 위해 4개의 코어로 8개의 스레드를 하이퍼스레딩하는 내 랩탑이 아닌 AWS EC2 t2.micro에서 벤치마크를 수행했다.\n프로그램 코드 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/stretchr/testify/assert\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;testing\u0026#34; ) func Benchmark동시성(b *testing.B) { for _, goroutineNum := range []int{1,4,8,16,32,64}{ b.Run(fmt.Sprintf(\u0026#34;%d개의 goroutine\u0026#34;, goroutineNum), func(b *testing.B) { for i := 0; i \u0026lt; b.N; i++{ do동시성(b, goroutineNum) } }) } } func do동시성(b *testing.B, goroutineNum int){ wg := \u0026amp;sync.WaitGroup{} totalRep := 64 for n := 0; n \u0026lt; goroutineNum; n++{ wg.Add(1) go func(num int) { for j := 0; j \u0026lt; totalRep / goroutineNum; j++{ resp, err := http.Get(\u0026#34;https://example.com\u0026#34;) assert.NoError(b, err) assert.Equal(b, 200, resp.StatusCode) } wg.Done() }(n) } wg.Wait() } Core 1개의 AWS EC2 t2.micro에서의 벤치마킹 ubuntu@ip-172-31-12-2:~/concurrency$ CGO_ENABLED=0 go test ./concurrency_test.go -bench=. -benchtime=1x goos: linux goarch: amd64 Benchmark동시성/1개의_goroutine 1 9426630119 ns/op Benchmark동시성/4개의_goroutine 1 2354737545 ns/op Benchmark동시성/8개의_goroutine 1 1192283559 ns/op Benchmark동시성/16개의_goroutine 1 604589767 ns/op Benchmark동시성/32개의_goroutine 1 313062480 ns/op Benchmark동시성/64개의_goroutine 1 182304298 ns/op PASS ok command-line-arguments 28.809s 64번 요청/응답 작업을 수행할 때 맨 위의 경우는 1개의 goroutine이 64번, 맨 밑의 경우에는 64개의 goroutine이 각각 1번씩 수행하는 식으로 진행했다. 1개의 코어를 갖는 t2.micro에서도 64개의 goroutine으로 concurrent하게 작업을 진행하는 것이 더 속도가 빨랐다. 네트워크 IO로 CPU가 놀고있는 시간이 많았기 때문에 하나의 코어로도 concurrent하게 수 많은 작업을 효율적으로 진행할 수 있었다.\nParallelism - CPU bound한 작업에 유리 예상: CPU bound한 작업의 경우 코어의 개수까지는 goroutine이 늘어날 수록 성능이 좋을 것이다.\n(아직 코어와 프로세서, vCPU의 차이는 명확히는 모르겠다)\nCPU Bound한 작업의 경우는 정말 동시성보단 병렬성을 이용할 때 더 좋은 성능을 보일까? 일종의 제곱 연산으로 CPU를 혹사시키는 작업을 정의했다. 병렬성의 경우 어떤 경우까지가 병렬성이고, 어떤 경우까지가 동시성 + 병렬성인지 헷갈릴 수 있기에 정리해본다.\n코어 1개에 여러 혹은 goroutine =\u0026gt; 당연히 병렬성이 일어날 코어들이 없으니 동시성 코어 N개에 N개 이하의 여러 goroutine =\u0026gt; 병렬성 코어 N개에 N개 이상의 여러 goroutine =\u0026gt; 동시성 + 병렬성. 하지만 CPU Bound 한 작업의 경우 동시성은 별 이점을 가져다 주지 않는다. 프로그램 코드 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;testing\u0026#34; ) func Benchmark병렬성(b *testing.B) { for _, goroutineNum := range []int{1,4,8,16,32,64}{ b.Run(fmt.Sprintf(\u0026#34;%d개의 goroutine\u0026#34;, goroutineNum), func(b *testing.B) { for i := 0; i \u0026lt; b.N; i++{ do병렬성(b, goroutineNum) } }) } } func do병렬성(b *testing.B, goroutineNum int){ wg := \u0026amp;sync.WaitGroup{} totalRep := 1024 * 1024 dummyNumber := 12345 for n := 0; n \u0026lt; goroutineNum; n++{ wg.Add(1) go func(num int) { for j := 0; j \u0026lt; totalRep / goroutineNum; j++{ dummyNumber = (dummyNumber * dummyNumber) % 100000 } wg.Done() }(n) } wg.Wait() } Core 1개의 AWS t2.micro에서의 벤치마킹 ubuntu@ip-172-31-12-2:~/concurrency$ CGO_ENABLED=0 go test ./parallelism_test.go -bench=. -benchtime=10x goos: linux goarch: amd64 Benchmark병렬성/1개의_goroutine 10 15228154 ns/op Benchmark병렬성/4개의_goroutine 10 14048765 ns/op Benchmark병렬성/8개의_goroutine 10 14038413 ns/op Benchmark병렬성/16개의_goroutine 10 13916091 ns/op Benchmark병렬성/32개의_goroutine 10 13877358 ns/op Benchmark병렬성/64개의_goroutine 10 14082735 ns/op PASS ok command-line-arguments 0.946s t2.micro의 경우 코어가 1개이기 때문에 병렬성이 존재할 것도 없이 동시성만이 존재한다고 했듯이 goroutine이 많아져도 성능이 좋아지지 않는다. 되려 불필요한 goroutine을 관리하기 위한 코스트와 미미하겠지만 존재할 goroutine에 대한 context switch로 인해 작업이 더 더뎌지기도 하는 결과를 볼 수 있다.\nCore 4개로 8개의 vCPU를 가진 나의 랩탑에서의 벤치마킹 $ go test ./parallelism_test.go -bench=. -benchtime=10x goos: linux goarch: amd64 Benchmark병렬성/1개의_goroutine-8 10 10263785 ns/op Benchmark병렬성/4개의_goroutine-8 10 7342331 ns/op Benchmark병렬성/8개의_goroutine-8 10 4872064 ns/op Benchmark병렬성/16개의_goroutine-8 10 5000452 ns/op Benchmark병렬성/32개의_goroutine-8 10 4830541 ns/op Benchmark병렬성/64개의_goroutine-8 10 4968710 ns/op PASS ok command-line-arguments 0.419s 내 랩탑은 실제 core는 4개지만 8개의 스레드까지 병렬적으로 처리가 가능하므로 8개라고 볼 수 있다.\n따라서 8개의 goroutine까지는 병렬성에 의해 성능이 점점 좋아지고, 8개 초과의 goroutine부터는 병렬성과 동시성이 모두 적용되겠지만 현재의 작업은 CPU bound하기 때문에 동시성은 그닥 이점을 가져다주지 못하고 EC2의 경우와 마찬가지로 과잉되는 goroutine으로 인해 오히려 성능을 저하시킬 수 있다. 코드에 첨부는 하지 않았지만 극단적으로 과잉되는 goroutine은 아래와 같은 성능 저하를 야기하기도 했다.\nBenchmark병렬성/8개의_goroutine-8 10 580169905 ns/op Benchmark병렬성/16개의_goroutine-8 10 602391376 ns/op Benchmark병렬성/1024*1024개의_goroutine-8 10 879079810 ns/op (한편으론 1024*1024개의 goroutine을 관리하는 데에도 성능 저하가 저정도 뿐이라니 대단하단 생각도 든다\u0026hellip;!)\n마치며 동시성과 병렬성은 Go를 공부하면서 컴퓨터 구조를 공부하면서 너무도 많이 파고들었던 내용이라 술술 적힐 줄 알았는데 시간이 좀 지나서인지 가물가물한 내용도 좀 있었고 내용도 쉽지 않았으며 좋은 예시를 어떻게 들어야할 지 많이 고민이 됐던 것 같다. 이런 저런 생각들로 인해 설명이나 진행이 그렇게 깔끔히 된 것 같진 않지만 그래도 결론이 깔끔하게 나와서 다행이다. 사실 이런 코어나 고루틴, 스레드의 개수에 따라 병렬성이나 동시성을 구분 짓기보다 \u0026lsquo;동시성은 주로 구조와 관련되고 병렬성은 실행과 관련된다.\u0026lsquo;와 같은 철학적인 내용도 간간히 나오긴 하지만 몇 달 째 그닥 와닿지 않고, 간간히 등장할 뿐 오피셜한 내용은 아닌 듯하여 생략했다.\n그리고 이번엔 벤치마크 테스트에 한글 이름을 도입해봤는데 읽기도 편하고 나름 귀여운 것 같다. ㅎㅎ 종종 개발하면서 테스트 코드 짤 때에도 한글 테스트 케이스를 사용해보려한다.\n참고 Concurrency is not parallelism https://blog.golang.org/waza-talk\nConcurrency is not parallelism slide - Rob Pike https://talks.golang.org/2012/waza.slide#1\nBack to the Essence - Concurrency vs Parallelism https://homoefficio.github.io/2019/02/02/Back-to-the-Essence-Concurrency-vs-Parallelism/\n[ 제 4회 파이썬 격월 세미나 ] 동시성과 병렬성 - 이찬형 https://youtu.be/Iv3e9Dxt9WY\n","date":"2021-02-04T12:46:54+09:00","image":"https://umi0410.github.io/blog/golang/go-concurrency-vs-parallelism/concurrency-vs-parallelism_hu86023f6654c19803b1eea4e88bc97fcd_191924_120x120_fill_q75_box_smart1.jpg","permalink":"https://umi0410.github.io/blog/golang/go-concurrency-vs-parallelism/","title":"Go 언어로 적용해보는 Computer Science - Concurrency vs Parallelism"},{"content":"시작하며 개발 공부를 처음 시작한 지 언제 안 됐을 무렵, 의경 복무를 하며 자바로 TCP Socket을 이용해 옆 컴퓨터와 채팅을 하는 프로그램을 만들어 보는 것을 시작으로 docker나 mysql과 같은 다양한 오픈소스들을 이용해보면서나 네트워크를 공부하면서 다양하게 들어왔던 소켓이지만 정확히 어떤 역할을 하는지 어떤 종류가 있는지 어떻게 동작하는지 알지 못했다.\n오늘은 그렇게 알쏭달쏭한 존재였던 소켓을 크게 Unix Domain Socket와 Network Socket 두 가지로 나눠 정리해보고 Unix Domain Socket과 TCP를 사용하는 Network Socket을 벤치마크해보고 어떤 차이가 있는지 확인해보도록 하겠다. 주로 내가 소켓을 접했던 자료나 이슈 상황에서는 UDP보다는 TCP가 많이 등장했었기 때문에 UDP에 대한 내용은 거의 제외할 것이므로 대부분이 Network socket과 Unix domain socket 각각의 stream(network socket에선 tcp를 이용하는 경우에 해당) type socket에 관련한 내용일 것이다.\nSocket 이란 소켓은 어떠한 통신에서의 Endpoint(끝점) 역할을 한다. 끝점이 없으면 어디와 어디가 통신하는지 어디에 데이터를 써야하는지 알 수 없다.\n소켓을 마치 전구 소켓과 같이 소켓이라고 부르는 이유는 소켓에 올바르게 데이터를 적기만 하면 통신을 위한 세부적인 동작은 소켓이 알아서 수행하기 때문이다. 따라서 우리는 통신을 하기 위한 Socket을 올바르게 생성하고, 그 곳에 데이터를 올바르게 적거나 그곳의 데이터를 올바르게 읽기만 하면 된다. 실제 데이터 전송은 소켓이 알아서 수행해준다.\n소켓은 같은 호스트 내에서 IPC를 위해 사용되는 Unix domain socket과 네트워크 통신을 하기 위해 사용되는 Network socket으로 분류할 수 있다. 간혹 unix domain socket과 TCP를 이용하는 Network socket, UDP를 이용하는 Network socket 이렇게 세 가지로 분류하는 경우를 본 것 같은데 이는 잘못된 분류라고 생각한다. Network socket 뿐만 아니라 Unix domain socket 또한 stream(Network socket의 경우 TCP를 이용하는 경우에 해당) 타입과 datagram 타입(Network socket의 경우 UDP를 이용하는 경우에 해당)으로 사용될 수 있기 때문이다.\n서버 소켓과 클라이언트 소켓 server-client-socket.jpeg 소켓을 역할의 측면에서 분류한다면 서버 소켓과 클라이언트 소켓으로 나눌 수 있다. 동일한 구조의 소켓이지만 생성되는 시기나 역할이 다를 뿐이다.\n서버 소켓 - 클라이언트 소켓의 연결 요청을 받아들이기만 할 뿐. 실제 서버 측에서의 데이터의 송수신은 서버 소켓이 클라이언트 소켓의 연결 요청을 수락하면서 새로 만들어지는 소켓을 통해 수행. 클라이언트 소켓 - 클라이언트가 서버와 통신하고자 할 때 생성하는 소켓. 클라이언트는 실제 데이터 송수신도 이 소켓을 통해 수행. 서버 소켓과 클라이언트 소켓이 단순한게 1:N으로만 통신하면 이렇게 서버 소켓이 클라이언트 소켓의 요청을 수락한 뒤 새로 소켓을 만들 필요 없지 않을까싶지만 그렇게 되면 서버가 각 클라이언트와 통신할 때 하나의 소켓을 이용하므로 올바르게 원하는 클라이언트와 통신할 수 없을테니 좋은 방식이 아닐 것이다.\n그렇다면 과연 정말 서버에서는 연결을 accept 한 뒤 소켓을 새로 생성할까? 확인해보자.\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;net\u0026#34; \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; ) var ( network = \u0026#34;tcp\u0026#34; address = \u0026#34;0.0.0.0:8080\u0026#34; ) func main() { os.Remove(address) socket, err := net.Listen(network, address) if err != nil { log.Fatal(err) } for { connectedSocket, err := socket.Accept() if err != nil{ log.Fatal(err) } go func() { for i := 0; i \u0026lt;= 10; i++{ connectedSocket.Write([]byte(\u0026#34;pong\\n\u0026#34;)) time.Sleep(time.Second) } connectedSocket.Close() }() } } 간단하게 TCP 통신프로그램을 하나 만들어봤다. 단순히 1초 간격으로 pong을 5번 출력한 뒤 연결을 끊는 프로그램이다.\n요청 전\n$ netstat --tcp Active Internet connections (w/o servers) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 ip-172-31-34-13.ap-:ssh 124.50.93.166:60082 ESTABLISHED tcp 0 0 ip-172-31-34-13.ap-:ssh 124.50.93.166:60084 ESTABLISHED tcp 0 0 ip-172-31-34-13.ap-:ssh 124.50.93.166:60078 ESTABLISHED 요청 수락 후 통신 중 - 소켓이 하나 생성되어 ESTABLISHED 상태\n$ netstat --tcp Active Internet connections (w/o servers) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 ip-172-31-34-13.ap-:ssh 124.50.93.166:60082 ESTABLISHED tcp 0 0 ip-172-31-34-13.ap-:ssh 124.50.93.166:60084 ESTABLISHED tcp 0 0 ip-172-31-34-13.ap-:ssh 124.50.93.166:60078 ESTABLISHED tcp6 0 0 ip-172-31-34-1:http-alt 124.50.93.166:42434 ESTABLISHED 통신 종료 - 생성되었던 소켓이 TIME_WAIT 상태. 잠시 후 사라진다.\n$ netstat --tcp Active Internet connections (w/o servers) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 ip-172-31-34-13.ap-:ssh 124.50.93.166:60082 ESTABLISHED tcp 0 0 ip-172-31-34-13.ap-:ssh 124.50.93.166:60084 ESTABLISHED tcp 0 0 ip-172-31-34-13.ap-:ssh 124.50.93.166:60078 ESTABLISHED tcp6 0 0 ip-172-31-34-1:http-alt 124.50.93.166:42434 TIME_WAIT 깔끔한 네트워크 정보를 보기 위해 순수한 AWS EC2 t2.micro 인스턴스에서 작업해보았다. 맨 위 3줄에 나온 소켓 정보는 ssh 접속으로 인한 소켓 정보이다.\n또한 서버 소켓은 출력되지 않았는데 이는 우리는 평소에 listen 중인 포트나 서버 소켓을 보기 위해 netstat에 -l 옵션을 전달한 반면 이번엔 \u0026ndash;tcp 옵션을 통해 listen 중인 서버 소켓이 아닌 클라이언트의 요청을 수락한 뒤 생성되는 소켓을 보려하고있기 때문이다.\n4번 째 줄의 소켓 정보가 바로 우리가 생각하는 서버 소켓이 client의 연결 요청을 accept 후 생성하는 socket이다. 클라이언트의 요청을 accept 후에 새로운 소켓이 생성되며 통신 종료 후 잠시 기다린 뒤 사라진다.\nNetwork Socket Network socket은 네트워크 통신이 필요한 작업을 수행할 때 이용하는 소켓의 한 종류로 다시 동작 방식에 따라 TCP 프로토콜을 이용하는 stream socket과 UDP 프로토콜을 이용하는 datagram socket으로 구분할 수 있다. 사용자는 socket에 데이터를 적기만 하면 네트워크와 관련된 작업은 socket이 알아서 수행해준다. 읽을 때에도 마찬가지이다.\nTCP socket이라고 부르는 사람도 있고 stream socket, TCP/IP socket이라 부르는 사람도 있는 것 같다. 정확한 명칭은 모르겠지만 사용하거나 이해하는 데에는 무리가 없을 것 같다. TCP를 이용하는 stream type의 Network socket과 stream type의 Unix domain socket은 사용 방법이 매우 유사하다. 둘 다 stream type이고, 소켓에 데이터를 적은 뒤의 작업은 소켓이 알아서 수행해주기 때문이다. 각각의 소켓을 이용해 서버를 띄우는 작업은 인자의 값만 조금 달라질 뿐이다. 이는 글의 하부의 코드에서 확인해볼 수 있다.\nSocket과 Port Network socket에 대해서는 socket과 port의 구분이나 역할이 애매하게 느껴질 수 있다. 통신을 할 때 IP 주소를 이용해 목적지인 Host를 찾을 수는 있지만 그 Host의 어떤 프로세스과 통신하려는 것인지는 알 수 없다. 올바른 프로세스를 찾을 수 있도록 프로세스와 어떠한 숫자를 매핑시키는데 이 숫자를 Port 번호라고 한다. 예를 들어 123.123.123.123:8080으로 요청을 보내는 것은 123.123.123.123의 IP 주소를 갖는 Host의 8080번 포트에 맵핑된 프로세스에 요청을 보내는 것이다.\n이 때 Port와 프로세스를 그냥 연결할 수는 없고 Socket이라는 녀석이 필요하다. Socket은 실질적으로 어떤 프로세스를 어떤 포트에 맵핑시킬지에 대한 정보가 필요하고 네트워크 작업을 알아서 수행한다.\n조금 비유를 해보자면 회사내에 어떤 부서가 있고 외부에서 해당 부서와 작업하기 위해선 어떠한 고유한 부서 번호가 필요하고, 이때 외부와 해당 부서간에 오가는 통신을 담당하는 담당자가 있어야하는 경우에 비유해 볼 수 있다. \u0026ldquo;어떤 부서\u0026quot;는 프로세스이고 \u0026ldquo;고유한 부서 번호\u0026quot;는 포트 번호, 통신을 담당하는 담당자는 소켓에 해당한다.\nUnix Domain Socket Unix Domain Socket은 IPC(Inter-Process Communication, 프로세스 간 통신)의 여러 방법 중 가장 자유로우면서 사용하는 데에 있어 제한이 별로 없는 방법이다. 네트워크 소켓과 달리 같은 호스트 내의 프로세스 간 통신을 담당하기 때문에 아무런 네트워크 작업이 필요 없다. 하지만 TCP나 UDP를 이용하는 Network socket을 이용할 때와 인자 값만 조금 바꾸어 동일한 방식으로 사용이 가능하다.\n🌈 상상의 나래 - 우리가 알게 모르게 겪었던 Unix domain socket의 Permission오류에 대해 $ mysql stat /var/lib/mysql/mysql.sock stat: cannot stat \u0026#39;/var/lib/mysql/mysql.sock\u0026#39;: Permission denied $ docker docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock 주로 우리가 MySQL이나 Docker를 localhost에서 사용할 때 unix socket에 대한 permission 에러를 겪은 적이 있을 것이다. MySQL이나 Docker를 비롯한 많은 오픈소스들이 아마 성능상의 이점을 위해 localhost에서 서비스를 이용할 경우 Network socket이 아닌 Unix domain socket을 많이들 이용하는 것으로 알고있다. 그렇기 때문에 주로 MySQL 설치 이후 Remote에서는 접속이 되는데 localhost에서는 권한 문제로 접속이 안된다거나 Docker 설치 이후 사용자를 docker group에 넣어주는 경우가 많이 있다.\n아마도 네트워크 통신을 통해 서버 소켓에 접근하는 네트워크 소켓과 달리 unix domain socket은 client process가 직접 server의 socket file에 접근하기 때문에 이때 쓰기 권한 때문에 그런 권한 오류가 생기는 것이 아닐까라는 상상의 나래를 펼쳐본다.\n# ubuntu 사용자로 unix domain socket server 프로세스를 띄운 경우 # ubuntu 사용자로는 잘 접속이 된다. ubuntu@ec2 $ go test -v === RUN TestSocket === RUN TestSocket/tcp 2021/02/13 12:59:06 pong === RUN TestSocket/unix 2021/02/13 12:59:06 pong --- PASS: TestSocket (0.00s) --- PASS: TestSocket/tcp (0.00s) --- PASS: TestSocket/unix (0.00s) PASS ok uds\t0.004s # server가 생성한 unix domain socket에 write 권한이 없는 # guest1이라는 사용자로는 server와 unix domain socket으로는 통신할 수 없고, # 앞서 오픈소스를 이용하며 겪었던 에러와 마찬가지로 권한 이슈가 발생한다. # 하지만 TCP 소켓을 이용한 경우는 원활히 ping-pong test가 성공한 것을 볼 수 있다. guest1@ec2 $ go test -v === RUN TestSocket === RUN TestSocket/tcp 2021/02/13 12:59:01 pong === RUN TestSocket/unix main_test.go:21: Error Trace:\tmain_test.go:21 Error: Received unexpected error: dial unix jinsu.sock: connect: permission denied Test: TestSocket/unix --- FAIL: TestSocket (0.00s) --- PASS: TestSocket/tcp (0.00s) --- FAIL: TestSocket/unix (0.00s) # 하지만 재미있게도 guest1도 socket에 write할 수 있도록 권한을 수정해주니 # guest1도 unix domain socket으로 무리 없이 통신이 가능했다! guest1@ec2 $ sudo chmod 777 jinsu.sock guest1@ec2 $ go test -v === RUN TestSocket === RUN TestSocket/tcp 2021/02/13 12:59:06 pong === RUN TestSocket/unix 2021/02/13 12:59:06 pong --- PASS: TestSocket (0.00s) --- PASS: TestSocket/tcp (0.00s) --- PASS: TestSocket/unix (0.00s) PASS ok uds\t0.004s 따라서 ubuntu 사용자로 TCP socket과 Unix domain socket 두 가지 방법으로 서버 역할을 할 수 있는 프로세스를 띄운 뒤 ubuntu 사용자와 guest1 사용자로 통신 테스트를 진행해보았다.\n서버가 생성한 unix domain socket은 srwxrwxr-x 의 형식과 권한을 갖고 있기 때문에 guest1은 이 소켓에 대해 read와 execute 권한 뿐이고, write는 불가능하기에 unix domain socket을 이용해서는 통신할 수 없다. 따라서 우리가 평소에 오픈소스를 localhost에서 이용하면서 종종 맞이했던 소켓에 대한 permission error을 만나볼 수 있었다! 반면 socket에 대한 접근 권한이 필요 없는 TCP로는 통신이 가능했고, 놀랍게도 guest1에게 소켓 파일에 대한 write 권한을 부여해주자 Unix domain socket으로도 통신이 가능해진 것을 볼 수 있다.\n상상의 나래 정리: unix domain socket을 통해 접속을 시도할 때에는 unix domain socket file에 대한 접근을 하는 프로세스가 해당 socket file에 대한 적절한 permission을 갖고 있어야한다.\n또한 구글링 도중 보았던 재미있는 예시는 database를 통해 authentication/authorization을 수행하는 일반적인 서비스와 달리 Unix domain socket을 이용하는 경우에는 linux user 시스템을 이용해서도 권한/인증 관리를 수행하는 경우도 존재할 수 있다는 것이었다.\n벤치마킹 Unix domain socket vs Network socket Unix domain socket과 Network socket의 성능을 비교하는 벤치마크를 작성했다. 두 경우 모두 Stream type의 socket을 이용하도록 했고, Network socket의 경우는 특히나 이 경우 TCP 프로토콜을 이용하게 된다.\n// main.go // 한 프로세스 내에서 TCP Socket과 Unix Domain Socket 두 가지를 이용해 // 통신할 수 있는 프로그램 package main import ( \u0026#34;errors\u0026#34; \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net\u0026#34; \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; ) func main() { go RunTCPSocketServer() go RunUnixDomainSocketServer() for { log.Println(\u0026#34;Dummy waiting...\u0026#34;) time.Sleep(time.Minute) } } func RunTCPSocketServer(){ log.Println(\u0026#34;Run TCPSocketServer...\u0026#34;) run(\u0026#34;tcp\u0026#34;, \u0026#34;0.0.0.0:8080\u0026#34;) } func RunUnixDomainSocketServer(){ log.Println(\u0026#34;Run UnixDomainSocketServer...\u0026#34;) serverSocketName := \u0026#34;jinsu.sock\u0026#34; os.Remove(serverSocketName) run(\u0026#34;unix\u0026#34;, serverSocketName) } // TCP socket을 이용해 서버를 띄우든 Unix Domain Socket을 이용해 서버를 띄우든 // 간단히 인자 값만 변경해서 이용할 수 있다! func run(network, address string){ buf := make([]byte, 1024) socket, err := net.Listen(network, address) if err != nil { log.Fatal(err) } for { connectedSocket, err := socket.Accept() if err != nil{ log.Fatal(err) } go func() { for{ n, err := connectedSocket.Read(buf) if err != nil{ if !errors.Is(io.EOF, err){ log.Fatal(err) } else{ log.Print(\u0026#34;[Error]\u0026#34;, err) break } } log.Println(network, \u0026#34;Client sent:\u0026#34;, n) connectedSocket.Write([]byte(\u0026#34;pong\u0026#34;)) } connectedSocket.Close() }() } } // main_test.go // tcp socket과 unix domain socket을 이용해 성공적으로 서버와 통신이 // 이루어지는지 테스트하는 테스트 코드 // tcp와 unix domain socket의 성능을 비교하는 벤치마크 코드 package main import ( \u0026#34;errors\u0026#34; \u0026#34;github.com/stretchr/testify/assert\u0026#34; \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net\u0026#34; \u0026#34;testing\u0026#34; ) func TestSocket(t *testing.T){ args := [][]string{ []string{\u0026#34;tcp\u0026#34;, \u0026#34;0.0.0.0:8080\u0026#34;}, []string{\u0026#34;unix\u0026#34;, \u0026#34;jinsu.sock\u0026#34;}, } for _, arg := range args{ t.Run(arg[0], func(t *testing.T) { conn, err := net.Dial(arg[0], arg[1]) assert.NoError(t, err) buf := make([]byte, 1024) _, err = conn.Write([]byte(\u0026#34;ping\u0026#34;)) if err != nil{ log.Fatal(err) } n, err := conn.Read(buf) if err != nil{ if ! errors.Is(io.EOF, err){ log.Println(err) } } log.Println(string(buf[:n])) }) } } func BenchmarkSocket(b *testing.B) { args := [][]string{ []string{\u0026#34;tcp\u0026#34;, \u0026#34;0.0.0.0:8080\u0026#34;}, []string{\u0026#34;unix\u0026#34;, \u0026#34;jinsu.sock\u0026#34;}, } for _, arg := range args{ b.Run(arg[0], func(b *testing.B) { //log.Println(\u0026#34;Out\u0026#34;) // b.N과 상관 없이 args의 길이에 따라 출력됨. conn, err := net.Dial(arg[0], arg[1]) assert.NoError(b, err) buf := make([]byte, 1024) for i := 0; i \u0026lt; b.N; i++{ //log.Println(\u0026#34;In\u0026#34;) // b.N만큼 수행됨 _, err := conn.Write([]byte(\u0026#34;ping\u0026#34;)) assert.NoError(b, err) _, err = conn.Read(buf) if err != nil{ if !errors.Is(io.EOF, err){ b.Fail() } } } }) } } $ go test -bench=. -benchtime=100000x goos: linux goarch: amd64 pkg: uds BenchmarkSocket/tcp 100000\t104537 ns/op BenchmarkSocket/unix 100000\t83456 ns/op PASS ok uds\t18.808s 10000번의 ping-pong을 수행하는 벤치마크를 수행했다. 그 결과 TCP socket을 이용할 경우는 한 번의 ping-pong에 약 104537ns, unix domain socket을 이용할 경우는 한 번의 ping-pong에 약 83456ns가 소요된 것을 보아 예상대로 네트워크 통신이 일절 필요하지 않은 unix domain socket이 좀 더 성능이 좋은 것으로 보여졌다. 사전에 자료 조사를 할 때에는 unix domain socket이 tcp socket을 이용할 때보다 약 2배가량 성능이 우월할 것이라고 들었는데, 그렇게 많은 차이가 나는 것 같지는 않다. 하지만 어느 정도 데이터의 크기나 버퍼의 크기에 따라 달라질 수도 있을 것 같고 더 깊은 원리들이 존재할 것 같아 더 자세히는 측정해보지 못했다.\n마치며 사실 이번에 다룬 소켓이라는 주제는 Go로 적용해보는 Computer Science라는 이 시리즈를 정리해나가던 초기에 2번 째 글로 시도했던 주제였는데 당시엔 Go benchmark도 처음 사용해봤었고 내용이 어려웠던 터라 벤치마크도 제대로 되지 않고 정리도 하기 힘들어서 중단했던 주제이다. 하지만 이번엔 같은 내용에 대해 두 번째 정리해서인지 전보다 내용도 잘 이해됐고, 벤치마킹 코드와 결과도 깔끔하게 나왔던 것 같아 뿌듯하다.\n그리고 그 동안 도커나 MySQL 같은 오픈소스들을 사용하면서 간혹 소켓에 대한 오류를 접할 때나 이런 저런 글에서 소켓 관련한 내용이 등장할 때 정확히 어떤 역할인지 이유가 뭔지 자세히 알지 못했는데 이번 기회덕에 앞으로는 좀 더 잘 이해해볼 수 있을 것 같다.\n참고 소켓 http://www.ktword.co.kr/abbr_view.php?nav=\u0026m_temp1=280\u0026id=742\n네트워크 소켓 https://ko.wikipedia.org/wiki/%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC_%EC%86%8C%EC%BC%93\n소켓 프로그래밍 https://recipes4dev.tistory.com/153\nUDS (Unix Domain Socket) http://www.dreamy.pe.kr/zbxe/CodeClip/119393\n[Linux/UDS/Unix Domain Socket] UDS https://yaaam.tistory.com/entry/LinuxUDSUnix-Domain-Socket-UDS\nUnix file type https://en.wikipedia.org/wiki/Unix_file_types\nC# - .NET Core Unix Domain Socket 사용 예제 https://www.sysnet.pe.kr/2/0/11963\nUnix Domain Socket https://www.joinc.co.kr/w/Site/system_programing/IPC/Unix_Domain_Socket\nUnix Domain Socket - UDP https://www.joinc.co.kr/w/Site/system_programing/IPC/Unix_Domain_Socket_UDP\nWhat is the difference between Unix sockets and TCP/IP sockets? https://serverfault.com/questions/124517/what-is-the-difference-between-unix-sockets-and-tcp-ip-sockets\n","date":"2021-02-13T14:10:54+09:00","image":"https://umi0410.github.io/blog/golang/go-socket/server-client-socket_huc5be35f5857cc34672b838b13057c03e_282874_120x120_fill_q75_box_smart1.jpeg","permalink":"https://umi0410.github.io/blog/golang/go-socket/","title":"Go 언어로 적용해보는 Computer Science - Socket (Unix Domain Socket, Network/TCP/UDP Socket)"},{"content":" preview.png 사진 출처: https://tv.naver.com/v/16972079\n시작하며 원래 간간히 재미삼아 개발 컨퍼런스들을 찾아보곤 해왔는데 그냥 쓰윽 듣기만 하니까 머리에 남는 게 별로 없는 것 같아 가끔은 이렇게 내용을 정리해보는 글을 써볼까한다. 나는 학교 시험과는 그닥 맞지 않는 스타일 같아서 시험 기간이 되면 더 이런 실무적인 내용들이 괜히 더 끌리는 것 같다. 소프트웨어 공학 수업을 듣다가 지루해져서 네이버 d2를 돌아다니던 중 네이버 데뷰 2020에 Golang 관련 영상이 있길래 \u0026ldquo;오잉?! 네이버 채용에 가끔 Golang이 뜨긴 하던데 Golang 어떻게 쓰려나?!\u0026rdquo; 싶은 마음에 영상을 시청해봤다.\n내용을 간단히 요약하자면 기존에는 C++로 검색 엔진 라이브러리를 개발해왔고 해당 라이브러리를 이용하는 C++ 검색 서비스를 개발해왔다고 하는데, 이를 Golang으로 마이그레이션하면서 성능은 유지하면서 Golang 특유의 간결하고 강력한 기능들(unit test, benchmark, fmt, build 등등)과 함께 데브옵스 문화에 잘 녹아들 수 있었고 팀원들도 편리함을 느끼며 생산성까지 잡을 수 있었다는 이야기이다.\n본 글은 https://tv.naver.com/v/16972079 영상을 바탕으로 해당 내용을 정리해본 글입니다.\n검색 서비스의 특징 자세히 검색 엔진이 어떻게 구성되어 있는지, 내부적으로 데이터를 어떻게 저장하고 질의하는지, 검색 서비스는 검색 라이브러리를 어떻게 사용하는지 등등 이런 내용들을 잘 몰라서인지 자세히 검색 엔진이나 검색 서비스가 어떤 식으로 동작하는지, 어떤 형태로 개발되는지에 대해서는 감이 잘 잡히지 않는다. 혹시 자세히 코드를 볼 수 있었다면 좋았겠지만, 아무래도 그러기는 힘들겠지\u0026hellip;ㅜㅜ\n아무튼 검색 서비스는 단언 성능이 중요했으며 주로 질의를 분석하고, 검색 결과를 정렬하는 방식, 랭킹에 대한 모델링 등의 로직이 구현되어 있다고 한다.\n각 서비스들은 자신들의 고유한 검색 로직을 구현하고, 실제 검색을 수행하는 것은 검색 엔진 라이브러리라고 한다. 점점 검색 엔진의 규모와 신규 검색 서비스들이 증가하게 되면서 관리가 어려워졌다. 서비스들의 크기와 수가 증가하면서 높은 생산성을 유지하기 위해 개발 프로세스를 표준화하고 자동화했지만 추가적으로 개발 언어적인 관점에서의 고민이 들기 시작했다고 한다.\n개인적으로는 아무래도 서비스가 많아질 수록 데브옵스 문화가 많이 녹아들어야한다고 생각하는데 아무래도 많은 것들을 자동화하기 위해서는 언어 차원에서 강력하고 편리한 CLI 기능들일 잘 지원되어야한다고 생각하고, 지속적인 통합을 위해서도 편리한 테스트가 지원되어야한다고 생각한다. C++을 많이 해보진 않아 잘 모르겠지만, 아무래도 Go가 간단한 명령어들을 통해 IaC도 곁들이며 많은 작업들을 자동화하기 더 편리하고, unit test를 작성해가며 TDD식으로 개발하기에도 편리하지 않을까 싶다.\n개인적인 의견이었고, 어쨌든 이 팀에서는 결국 효율적으로 DevOps 문화에 녹일 수 있으면서 어느 정도 높은 성능이 보장되고 생산성을 유지하기도 쉬운 언어를 생각하게 되었고 Go가 적합하다고 판단했다고 한다.\nGo의 특징 golang-features.png (이해를 돕고자 발표 PPT에서 하나 캡쳐해왔습니다.)\n성능이 높으면서 GC를 지원한다.\ndiscord-golang.png 출처 - 디스코드 블로그(https://blog.discord.com/why-discord-is-switching-from-go-to-rust-a190bbca2b1f)\n과거 디스코드에서는 Go의 GC로 인해 간혹 CPU가 Spike치는 경험을 했고 Rust로 옮겨갔다는 이야기가 있는데, 이에 대해서는 Go community에서도 다양한 의견이 있었던 것 같다. 최근 버젼의 Go에서는 해당 이슈가 픽스되었다는 의견도 있고, 그 정도 Spike가 있어도 GC가 가져다 주는 생산성이 더 크기 때문에 감수해야한다는 의견도 있었던 것 같다. 하지만 여태까지는 네이버에서 Golang을 이용하면서 모니터링해봤을 때는 GC로 인한 말썽은 없었다는 것 같다! Pointer의 개념이 여전히 존재하지만 배우기 쉽다.\n이 부분 같은 경우는 내가 Docker를 처음 배우던 시절에 느꼈던 것과 유사한 것 같다. 사용법이 어려운 게 아니라 그게 왜 필요한 지 이해하고 잘 사용하는 게 어려운 느낌이다. 근데 Go가 어려운 것은 보통 \u0026ldquo;어떻게 해야 Concurrency programming의 장점을 잘 이끌어 낼 수 있을까?\u0026rdquo;, \u0026ldquo;channel을 통한 communication, synchronization을 어떻게 해야 잘 이용할까?\u0026rdquo; 와 같은 내용들인데 물론 Go가 그런 부분에서 장점을 가진 것은 맞지만 이런 부분을 꼭 이용하지 않더라도 이제는 Go가 충분히 그 외의 매력도 많다는 것이 점점 인정받고 있는 것 같아 다양하게 사용되고 있는 것 같다. 예를 들어 prometheus나 docker cli 같은 Go로 작성된 유명한 오픈소스들의 코드를 까봐도 그닥 \u0026lsquo;와 이걸 동시성을 이용해서 풀어냈네..!\u0026rsquo;, \u0026lsquo;와 이걸 channel들로 pipeline을 만들어서 진행하네?!\u0026rsquo; 이런 부분은 찾기 힘들었던 것 같다. 패키지 관리가 쉽다.\n따로 npm이나 pypi 같은 곳에 업로드 할 필요도 없고 Git을 기반으로 해서 release나 commit 단위로도 패키지의 버젼을 지정할 수 있어 개인적으로도 아주 맘에 든다! 패키지를 설치할 때에도 $ go get ... 명령어를 이용하면 되지만 보통은 그냥 Jetbrains사의 Goland IDE에서 그냥 Alt + Enter를 연타해서 이용 중이다 ㅎㅅㅎ Unit test를 기본적으로 지원해주고 테스트 하기가 아주 편하다.\n내가 처음 TDD를 시작하고 Unit test 작성을 시작했던 것도 Go를 공부하면서 였다! 몇 년 전 node.js를 막 공부했을 때 Test code를 짜볼까싶어서 test framework들에 대해 알아봤는데 그냥 테스트 프레임워크를 고르는 것부터가 힘들어서 \u0026lsquo;에잇\u0026hellip;ㅎㅎ 테스트는 다음에..ㅎㅎ\u0026rsquo; 했던 기억이 난다. 요즘은 주로 Spring으로 Test code를 짜고 있다. 너무나도 편리하다. 하지만 Spring은 그런 편리함을 누리기 위해 알아야할 내용이 너무도 많은 게 사실이다. test시에는 어떤 profile을 이용할지, Configuration은 어떤 걸 사용할 것인지, Bean을 어떻게 초기화 할 것인지 등등을 설정해줘야하고 어떻게 동작하는지 알아야한다. 그리고 솔직히\u0026hellip; 한 번 그렇게 공부하고 난 뒤에 테스트를 짜다가 다른 Layer의 코드에 대한 테스트를 작성하려면 다시 \u0026lsquo;음..? 어떻게 동작하더라\u0026hellip;..\u0026rsquo; 하고는 다시 내용들을 찾아봐야하는 경우도 많았던 것 같다. 반면 Go는 간결하고 직관적으로 쉽게 쉽게 테스트 할 수 있다. 별로 고민할 것들도 없다. 다만 초큼 아쉬운 건 mocking을 하려면 다 interface로 선언해야한다는 것과 mocking type을 자동으로 관리하기 힘들고 mocking library를 선택하고 익혀야한다는 것..? SWIG를 도입했다. C/C++ 등의 라이브러리를 Go(혹은 그 외의 다양한 언어)에서 사용할 수 있도록 추상화 시켜주는 오픈소스\nSWIG에 대한 설명을 하긴 하지만 결국 요점은 C++ ⇒ Go 로 넘어갈 때 기존 코드를 모두 버린 것이 아니라 다른 방법은 없을까 알아보고 적절한 기술을 빠르게 채택했다는 점이 중요한 점 같다.\n만약 C++⇒ Golang을 고려중인 기업이나 개발자분이 계시다면 좋은 참고자료가 될 것 같긴하다.\n나에게는 별로 필요 없으니 패스하겠다~!\nSWIG를 이용했지만 그럼에도 존재했던 Go와 C++의 언어적 차이 Go에서 C++로 마이그레이션하면서 목표는 \u0026ldquo;Go는 Go답게!\u0026rdquo; 이용하는 것으로 정했다고 한다. 이 파트도 거의 SWIG 특화 내용이라 패스하도록 하겠다.\n다만 무슨 작업을 할 때 저런 식으로 슬로건이나 목표를 정해놓고 나아가는 것은 좋은 방향인 것 같았다.\n가끔 작업을 하거나 프로젝트를 진행하다보면 일관성이 흐려지는 경우가 있는 것 같다. 예를 들면 얼마전 토스의 컨퍼런스에서도 결제 SDK, API를 만들면서 API Naming 같은 것을 할 때의 팀 내 룰을 설명해줬던 것 같다. 그 정도로 협업을 할 때 일정한 룰을 정해놓고 그것을 준수하는 것은 참 중요할 것 같다는 게 다시 한 번 와닿았다!\nShared memory를 이용해 멀티 프로세스로 작업하던 기존 방식 기존의 Apache MPM Prefork를 이용하던 방식에서는 멀티 프로세싱을 통해 병렬성, 동시성을 이용했다고한다. 이때 여러 프로세스가 공유하는 Shared cache를 이용했고 이 캐시는 Key-value 형식의 Map이었다고 한다.\n하지만 Go는 멀티프로세싱 방식보다는 Goroutine을 이용한 동시성 프로그래밍과 channel을 이용한 goroutine간의 communication을 권장한다.\n기존에는 multi-process에서 shared memory 형태의 shared cache를 이용했는데 이것을 유사하게 이용할 방법을 잘 찾지 못했다고 한다. 그 이유는 Goroutine간의 데이터 공유는 직접 공유가 아닌 channel을 이용한 communication 방식을 이용할 것이 대체로 권장(즉 기존의 방식과 너무 다른 방식)되었고, 그 이외의 방법들은 서버/클라이언트 방식이나 gRPC 방식이었기 때문이라고 한다. 그리고 결과적으로는 이런 방식들보다는 Memory상의 SQLite를 이용하기로 했다고 한다.\n아마 여러 Goroutine이 동시적으로 접근해도 SQLite도 나름의 DB라서 여러 goroutine이 동시적으로 동작해도 동시성 문제가 없을 것이고, 드라이브가 아닌 메모리를 바탕으로 하도록 설정도 가능해 속도도 빠르기 때문이 아닐까 싶다. 하지만 개인적으로는 여기서 이해되지 않는 내용이 꽤 많이 존재한다. Go는 goroutine간의 데이터 공유를 주로 communication을 이용할 것이 권장된다고 하셨는데 그런 경우는 다음과 같지 않을까 싶다.\ngoroutine이 서로간에 데이터를 ping pong하며 주고 받으며 동작하는 경우 여러 스레드(혹은 고루틴 혹은 프로세스)가 동시에 이용 가능하도록 동기화가 지원되는 큐처럼 이용할 경우 semaphore처럼 작업량을 조절하려는 경우 하지만 이런 communicate 방식 말고도 Go에도 충분히 Mutex가 제공되고 게다가 Read/Write에 대해 다르게 Lock을 걸 수 있는 기능도 있는데, 굳이 SQLite3를 사용할 필요가 있었을까 싶기도하다. 아니면 다른 관점에서 봤을 땐 Go의 Mutex 기능을 이용하는 것이 그리 쉬운 편은 아니라는 점에서 너무 Go스러운 방식을 적극 활용하는 쪽 보다는 그냥 일반적인 개발자들이 친숙한 SQLite와 Database적인 특징을 이용하는 쪽이 협업하기에 더 좋을 수도 있을 것 같다.\n성능 Go가 C++ 보다 나은 성능을 보여주냐보다는 Go가 C++만큼의 성능을 낼 수 있는가의 관점에서 평가했다고한다. 그리고 Go는 기존의 C++ 검색엔진 만큼의 작업 처리량을 보여줬다고한다.\n기본적으로 Go Binary가 큰 편이고 Shared memory를 이용하던 것을 SQLite3를 쓴 점에서 메모리의 차가 좀 크게 난 것 같다고 한다.\n생산성이 정말 좋아졌는가? 4명이서 시작한 킥오프, 5개월만에 검색엔진을 Go기반의 모듈로 전환 완료\nGo는 C++에 비해 러닝 커브가 낮다고 생각 테스트를 짜는 것도 돌리는 것도 쉬움 go build, go test, go test -bench, godoc 등의 기본 명령어를 통해 빌드, 테스트, 벤치마크, 문서화등을 이용가능 gofmt나 goimport 통일된 기능과 같은 깐깐한 Compiler로 인해 깔끔한 코드 유지 가능. 패키징이 아주 간단하다. go.mod에 한 줄만 추가하면 됨. 소스도 까보기 쉬움. C++ ⇒ Golang으로 마이그레이션 한 뒤 고객의 소리 (검색 엔진 라이브러리를 이용한 타 부서 사람들의 의견) 로직 파악이 쉽고, 다른 개발자의 코드를 Import 하기 쉬웠다. TDD를 적용해볼 수 있었다. 표준 라이브러리(marshaling 등등)이 파워풀했다. 단순하고 러닝 커브가 적어서 좋았다. 개인적으로는 확실히 Go 코드가가 엄청 읽기 쉽긴하다. 불필요한 코드는 애초에 컴파일러가 허가해주지 않으니 읽는 사람 입장에서는 모든 코드가 필요한 내용들로 구성되어있으니 간결하다. 또한 강한 Type 강제 언어답게 어떤 메소가드가 어떤 것을 리턴하는지, 어떤 인자를 필요로 하는지가 분명하다. 게다가 Type 언어이면서도 Java와 달리 앞서 말했듯이 간결하면서도 강력하고 통일된 기능들을 제공한다. 약간 자바는 기능도 엄청 많은 대신 설정해야하는 것도 너무 많아 번거로울 때가 있는 반면 Go는 \u0026ldquo;우리가 표준으로 정의한 대로 해!\u0026ldquo;라고 강제하는 느낌이 있는데 그 표준이 꽤나 깔끔해서 반박의 여지가 적은 것 같다. 마치 CISC(Complex Instruction Set Architecture)의 형태로 발전하다 \u0026ldquo;아 몰라! 너무 복잡해. 다시 간단하게 해!!!\u0026rdquo; 라며 RISC의 형태로 발전된 느낌이랄까\u0026hellip;ㅎㅎ\n총평 및 의견 실제로 기존 C++ 라이브러리를 Go로 마이그레이션 해보았으며, Go를 모르던 타부서 개발자들도 Go로 마이그레이션된 라이브러리를 무리 없이 이용했다는 썰이 흥미로웠고, 당근마켓이 그러했듯 네이버에도 어쩌면 Go 붐이 이를 수 있지 않을까싶다. 만약 그렇게 된다면 나중에 네이버에 지원하는 신입 중엔 꽤 Go를 잘 이해하고 잘 사용할 자신이 있을 듯 하다..!\n성능이 C++보다 잘 나오느냐의 관점보다는 C++만큼의 작업 처리량을 낼 수 있느냐는 점도 좀 재미있었다. 이제는 Go도 그 매력을 인정받으며 점점 대중화되고 있기 때문에 \u0026ldquo;꼭 Go를 써야할 절대적인 이유가 있어? 다른 언어들도 그런 이유는 어떻게든 커버 가능하잖아.\u0026rdquo; 의 관점에서 바라보지 않는다는 것이다. Go가 다방면에서 좋은 장점이 있기 때문에 \u0026ldquo;꼭 Go만의 절대적인, 유일한 장점, 기능을 써먹는 케이스가 아니더라도 기존 작업량을 따라 갈 수만 있으면 성능도 괜찮고, 관리도 편리하고, 러닝 커브도 적고, 테스트도 쉬운 좋은 언어인데 도입해볼만하지 않아?\u0026rdquo; 라는 관점에서 접근하게 된 것이다.\n실제로 Go를 적용해봤는데 발표자분이 속한 팀에서도 또 그 외의 부서에서도 사람들이 잘 따라온다고하니 역시 Go가 러닝 커브는 높지 않고 간결한 언어인 것 같구나 싶기도 하고, 사실 한 언어를 잘 하는 것도 힘들 수 있는데 곧잘 다들 Go라는 언어를 익히신다니 역시 Naver의 개발자들이구나 싶은 생각도 들었다.\n","date":"2021-06-03T15:46:54+09:00","image":"https://umi0410.github.io/blog/conferences/naver-deview-2020-cpp-to-golang/preview_hu5e425158693deacd0d5ef6841733ed22_379145_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/conferences/naver-deview-2020-cpp-to-golang/","title":"Naver Deview 2020 C++ to Golang 리뷰"},{"content":"시작하며 이 글은 Go 1.15 버전을 바탕으로 개발하며 겪은 이슈에 대해 설명하고 있으며 Go 1.16에서는 해당 이슈가 개선될 것이라고 합니다.\nmemory-leak-graph.png 진행 중인 쿠뮤라는 프로젝트에서 Go를 이용해 이미지에 대한 url 해싱, 리사이징, 섬네일, 센터 크롭 작업을 하는 이미지 프로세싱 마이크로서비스를 개발하고있었다. 1차적으로 어느 정도 개발이 끝난 뒤 벤치마크 겸 부하 테스트 겸해서 얼마나 해당 마이크로서비스가 잘 버티면서 작업을 수행하는지 확인해보려 했으나 Memory가 한 번 치솟게되면 어느 정도 이하로 떨어지지 않는 이슈가 발견되었다.\n허용할 수 있는 양보다 많은 요청을 보낼 경우 애플리케이션 레벨 이전에서 요청을 차단해주지 않으면 애플리케이션 단에서는 터지거나 문제가 생기는 것은 당연하겠지만 사진처럼 작업을 다 수행한 뒤에도 메모리가 제대로 해제되지 않는 것이 이슈였다.\n이슈의 원인을 파헤치기 위해 했던 노력들\u0026hellip; 이 이슈를 잡아보려 여러 가지 디버깅 작업을 해봤으나 계속해서 메모리 이슈가 발생했고, 각종 커뮤니티에서 도움을 구해보고자했다. Go 오픈 카톡방, Reddit, Slack 등에서 의견을 구해보았다. 재미있는 경험이었으며 덕분에 원인을 파악할 수 있던 것 같다.\nslack-thread.png reddit.png 어떤 상황에 Memory Leak이 발생한 것일까? 그럼 자세히 어떤 이슈가 있었고, 어떻게 그 상황을 분석할 수 있었으며 어떻게 해결할 수 있을지 알아보도록하겠다.\ntl;dr - 자세한 묘사보다는 그냥 딱 원인/결과만 궁금하신 분들을 위한 요약\n원인 - 힙 메모리에 메모리를 할당받았는데 혹시 재사용할 수도 있어 OS에게 바로 메모리를 반환하지 않음. 결과 - OS가 메모리 부족하니 달라고 하면 그때 힙 메모리 사이즈를 실제로 줄인다. (하지만 go 1.16 버전부터는 개선될 예정이다.) 정확히 이러한 것을 메모리 누수라고 하는지는 명확하지 않다. Go 런타임이 놀고 있는 메모리를 갖고는 있지만 언제든 OS에게 반환해줄 수는 있기 때문이다. (하지만 매끄럽게 OS에게 반환해주는 느낌은 아니긴했다.) package main import ( \u0026#34;time\u0026#34; ) func main(){ for routine := 0; routine \u0026lt; 10; routine ++{ DoFloat() } for { // dummy waiting.. time.Sleep(3*time.Second) } } func DoFloat(){ var tmp [400000000]float64 tmp[0] = 0 // tmp에 접근하지 않으면 unused variable이 되기 때문에 dummy한 access 작업 수행 } 처음엔 이미지 리사이징 시 메모리가 제대로 해제되지않는 것을 보고 이슈를 발견했지만, 사람들에게 도움을 구하고자 할 때, 이슈에 대해 설명할 때 전체 프로그램 코드를 첨부할 수는 없었기에 문제 상황을 간단하게 표현할 수 있는 코드를 짜보고자했다. 이리 저리 프로그램을 간소화하면서 커다란 배열 생성시에도 같은 메모리 이슈가 발생한다는 것을 알게되었다.\n그래서 아주 간단한 배열 생성 예시를 통해 사람들에게 이 이슈에 대해 설명해보고자했다. 이 예시에서는 8바이트의 float64로 이루어진 400000000칸의 배열 tmp를 선언한다.\nunit-conversion.png 8바이트가 400000000칸이면 400000000 * 8 / 1024(KB) / 1024(MB) / 1024(GB) = 약 2.9GB 혹은 그대로 1000단위 씩으로 나눠 3.2GB을 할당하는 것이다.\n원래는 스택 메모리에 할당된 뒤 다른 곳에서 이 녀석을 참조하는 일이 없기 때문에 바로 release되어 스택 메모리에서 점유가 해제되어야한다. 하지만 몇몇 경우에 스택이 아닌 힙에 데이터가 저장될 수 있다고 하는데, 이 경우에는 너무 큰 값을 선언하여 스택이 아닌 힙에 데이터가 저장되었고, 힙이 할당받은 메모리를 해제해주지 않아 생기는 문제였다. 사실 이런 현상도 정확히 메모리 누수 혹은 Memory Leak라고 하는지는 잘 모르겠다. 왜냐하면 메모리에서 해제할 수 없는 수준으로 그 값의 주소를 잃어버려 실제로 그 공간이 누수가 되는 것이 아니라 아직 딱히 OS가 부담을 느끼지 않기 때문에 Go Runtime의 Heap에서 해당 주소는 비워뒀지만 OS에게 반납은 안 한 상태인 것이기 때문이다.\ntop-memory-leak.png 실행해보면 10번의 DoFloat() 후에 그냥 for loop에서 time.Sleep 중이기에 CPU를 거의 사용하지 않고 있는 반면 내 Laptop의 16GB의 Memory 중 20.5%인 3.2GB를 사용 중인 것을 확인할 수 있었다.\n해결 해보기 Go runtime은 OS에게 더 이상 이 메모리가 필요하지 않다고 알려주기만 하지 그 Memory를 실제로 회수할 지 말지는 OS에게 달려있다. The Go runtime only advises the OS when it no longer needs memory and it is up to the OS to reclaim it - @justinisrael\n내가 Reddit에서 사람들께 여쭤봤던 글에 담긴 한 댓글을 인용해보았다. 과연 저 말이 사실일까? 프로세스를 여러번 띄워보았다.\ntop-multiple-process.png 프로세스를 하나 띄웠을 때에는 아까는 Dummy waiting 중이면서도 Memory를 20% 가까이 점유하고 있었는데 여러 프로세스를 띄우면서 메모리가 부족해지자 놀고있던 Heap memory를 반환하여 거의 약 0.6%의 메모리만 점유 중인 것을 볼 수 있다.\n마지막 프로세스는 아직 DoFloat() 작업을 진행 중이므로 여전히 19.8%의 메모리를 점유 중이고, 작업이 완료된 뒤에도 OS가 Reclaim(다시 메모리를 가져가는 것)하기 전까지는 약 20%대를 유지하는 것으로 보여졌다.\n하지만 좀 더 자세히 기록해보고싶었다. Go의 내장 패키지 runtime의 Memory 관련 기능을 이용하면 될 것 같았다. runtime.ReadMemStats(*runtime.MemStats)를 이용하면 런타임 도중 자신의 런타임 상황을 알아볼 수 있다. 자세한 사용법은 구글링을 통해 쉽게 얻을 수 있으니 지면 관계상 생략한다.\nruntime-memory-leak.png 즉 힙 메모리가 확보간 공간은 거대한 배열을 생성하는 함수인 DoFloat을 진행하는 동안은 실제로 Allocate 할당하여 사용하는 반면 DoFloat을 모두 마친 뒤에는 힙 메모리는 공간을 확보하고는 있지만 Idle(놀고 있는) 상태로 존재하는 것이었다.\n나는 이러한 경우를 처음 맞이했지만, 각종 가비지 컬렉터가 있는 언어에서 각각의 가비지 컬렉터나 런타임을 구현하는 방법에 따라 이런 식으로 힙 메모리를 재사용하는 경우를 대비해 한 번 할당받은 메모리를 완전히 OS에게 반환하지 않는 경우가 있다고 한다.\nGo 1.16 버전부터는 메모리 계산 방식이 바뀔 것이다. (그래서 괜찮을 것이다.) \u0026ldquo;There was a change to how memory is calculated on 1.16\u0026rdquo; - @gopj\n과연 사실일지 확인해보자. 간단히 Docker를 통해 나의 Local 환경에서 별 다른 세팅 없이 Go 버전을 다르게 하여 실행할 수 있었다. 실제로 이 이슈를 커뮤니티에 제기하기 전에도 Docker를 이용해 1.2, 1.3, 1.4 등의 버전에서도 실행해보았다. 나의 랩탑은 1.5 버전을 사용 중이었다. 1.16 버전은 아직 rc는 Release Candidate의 줄임말로 보통 배포 후보 버전을 의미한다.\n# Dockerfile FROM golang:1.16-rc WORKDIR /app COPY main.go main.go ENTRYPOINT [\u0026#34;go\u0026#34;] CMD [\u0026#34;run\u0026#34;, \u0026#34;./main.go\u0026#34;] # 이미지 빌드 후 컨테이너 실행 $ docker build . -t tmp \u0026amp;\u0026amp; docker run --rm --it tmp go-1.16-memory-not-leaking.png 1.16-rc 버전을 사용하자 결과적으로 다른 메모리 부하가 심한 프로세스를 실행시키지 않았지만 수십 초 이내에 사용하지 않는 힙 메모리가 OS에게 반환되었다!!!\n하지만 여전히 runtime.MemStats에서는 HeapMemory에 Idle한 메모리 크기가 크게 잡혀있었는데, 이 부분은 rc 버전이기 때문에 runtime까지 완전히 기능이 개발되지 않아서인지 이전에도 메모리는 반환했지만 메모리를 표시하기 위한 계산의 문제만 개선이 된 것인지는 확실하진 않지만 여튼 이슈에 대해서는 파악할 수 있었다!\n마치며 개발이나 운영을 하면서 이런 저런 이슈들이 있었지만 이번 경우처럼 로우 레벨스럽게 들어가서 런타임 동안 메모리 관리가 어떻게 되는지까지 탐구해본 적은 드물었던 것 같다. 도저히 원인을 모르겠어서 \u0026lsquo;하 결국 포기하고 이 마이크로서비스는 Lambda로 돌려야하나\u0026hellip;\u0026rsquo; 싶었다. 하지만 몇 시간을 고생하고 직접 의견을 구하러 다니면서 새로 배우게 된 내용도 많았고, 외국에 계신 몇몇 개발자 분들과도 이렇게 소통할 수 있다는 것이 신기했다. (그리고 참 세세한 지식까지 겸비한 분들이 많다는 것이 놀라웠다\u0026hellip;)\n결과적으로 해당 이슈는 아마 Go의 1.16 버전이 패치되면 완전히 해결 가능할 것 같고 그 이전에도 사실 Host에서 메모리 부담을 느끼면 Go runtime이 안 쓰고 있는 힙 메모리를 반환해준다고 하니 큰 문제는 없을 것 같다. 하지만 실제로 Host가 부담을 느끼는 선 이전에 메모리를 반환받고 싶다면 Pod level에서 메모리 리소스를 제한해볼 수는 있을 것 같다.\n혹시 다음에도 이런 흔치 않은 고된 이슈를 맞이한다면 그 원인과 해법을 이렇게 또 기록할 수 있기를 바래본다.\n","date":"2021-01-27T15:25:54+09:00","image":"https://umi0410.github.io/blog/golang/go-memory-leak-issue/memory-leak-graph_hu1f014e9e399a42abc11cd4e02aac56df_8940_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/golang/go-memory-leak-issue/","title":"개발 썰 - Go Memory Leak(메모리 누수) 관련 이슈"},{"content":" mzc_logo 안녕하세요. 이번에 메가존 클라우드의 클라우드 원 팀에서 데브옵스 인턴으로 근무를 하게되었던 박진수입니다. 너무 좋은 팀원들과 많은 경험을 하며 단시간에 성장할 수 있었고, 일하는 동안 매 순간 순간이 너무 행복했었기에 이렇게 인턴 후기를 작성해봅니다. 한 학기를 쉬고 2020.04.13~2020.08.31 까지 인턴으로 근무 했고, 다시 2020-2학기부터는 학교 복학을 하게되었습니다. 기술적인 얘기는 이곳 저곳에 많으니 항상 제가 가장 중요시하는 \u0026ldquo;느낀 점\u0026rdquo; 과 \u0026ldquo;배운 점\u0026rdquo; 을 위주로 적어보겠습니다!! 글은 위의 목록의 순서로 진행해보겠습니다.\n지원동기 저는 AWS와 배포에 관해 흥미가 있어 종종 AWSKRUG 라는 그룹의 소모임에 가서 핸즈온이나 세미나에 참여하곤 했는데요. 그곳에서 저희 팀원들을 만나게 되었고, 그것이 인연이 되어 채용으로까지 이어질 수 있었습니다.\n평소 AUSG 라는 대학생 AWS 사용자 모임의 일원으로서 참여하며 AWS 를 이용한 클라우드 인프라, CI/CD 파이프라인 구축을 통한 자동화, 컨테이너 등에 관심이 많았는데, 마침 메가존 클라우드의 저희 CloudOne 팀에서는 팀의 모든 마이크로서비스 및 기타 서비스들을 EKS라는 AWS의 Managed Kubernetes Cluster Service 위에 자동화시켜 배포를 하고있었고, 개발 인프라를 이전하려던 참이었기에 저의 관심사와 향후 목적에 잘 부합했습니다.\nMegazone Cloud: CloudOne Team? Megazone Cloud: 국내 최초 \u0026amp; 최대 AWS 프리미어 컨설팅 파트너, AWS 컨설팅, 구축, 운영 및 빌링 서비스 제공. 메가존클라우드는 2009년부터 클라우드를 차세대 핵심 사업으로 성장시키며 ‘클라우드 이노베이터(Cloud Innovator)’로서 고객님들의 클라우드 전환의 과정마다 최선의 선택을 하실 수 있도록 다양한 서비스를 제공하고 있습니다. - 출처(https://www.megazone.com/)\nspaceone_preview_2 AWS EKS를 이용한 Kubernetes 환경, 마이크로서비스 아키텍쳐 Composition API를 이용하는 최신 Vue 기술 Terraform을 통한 IaC 자동화된 CI/CD, 다양한 배포 전략, 인프라 모니터링 HTTP2를 이용한 gRPC API 진행했던 업무들 링크 형식이므로 링크를 눌러 읽어주시면 됩니다.\n2020.04: Stargate 라는 개발 인프라를 구축 terraform 을 통한 개발 인프라 구축 EKS 클러스터에 jenkins, spinnaker, grafana 등등의 개발 도구들을 배포함 종종 발생하는 장애 상황을 멘토님과 트러블슈팅함 2020.05: 개발 CI/CD 파이프라인 구축 Experiment 환경에 배포 =\u0026gt; Test, CI =\u0026gt; Development 환경에 배포를 자동화 Spinnaker와 Jenkins, Github Action을 이용한 자동화 파이프라인 구축 2020.06: Argo Project들을 PoC argo cd, argo(workflow), argo-event 등의 다양한 프로젝트에 대한 PoC를 진행 2020.07: spaceone-helm Helm3 Chart 개발 개별로 배포되고 운영되던 우리 팀의 서비스인 SpaceONE을 패키지로 배포할 수 있게해주는 Helm Chart를 개발 2020.08: spacectl 설계, 개발 참여 우리 팀의 서비스인 SpaceONE에 대한 API 작업을 수행하는 CLI 도구에 대한 설계와 개발에 참여함. 느낀 점 그 동안 혼자 개발 및 평소 관심분야였던 클라우드, 컨테이너 등등의 주제로 공부해왔었는데, 과연 이 내용들이 정말 실무에 도움이 될 지, 제가 잘 나아가고 있는 건지 확신이 들지 않았습니다. 하지만 CloudOne팀에서 다양한 경험을 하면서 \u0026lsquo;제가 공부해온 길이 틀리지만은 않았구나\u0026rsquo;라는 느낌을 받을 수 있었고, 보완해야할 부분들은 보완하면서 불확실한 자세가 아닌 확신과 열정을 가진 자세로 좀 더 몰두할 수 있을 것 같습니다!\n배우고 느낀 내용이 너무 많아 글이 다소 길어졌습니다. 기술을 거부감 없이 접하되 기술이 다가 아닌, 팀원들을 위해 솔선수범하는 개발자, 팀원들의 생각을 읽어줄 수 있고, 잘 이해해줄 수 있는 개발자가 되기 위해 노력해야겠단 생각이 듭니다. 쉽진 않겠지만, 나아가고 싶은 방향은 정해진 것 같아 다행입니다!\n","date":"2020-09-04T12:46:54+09:00","image":"https://umi0410.github.io/blog/megazone-cloud/index/mzcloud-logo_huaed2850ad2750bd21c0ea378f4b3d958_6575_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/megazone-cloud/index/","title":"메가존 클라우드 데브옵스 인턴 후기"},{"content":"Stargate라는 개발 인프라 구축기 저희 팀의 개발 인프라는 위와 같습니다. 마침 제가 입사할 쯤이 기존에 존재하던 인프라를 새로운 환경으로 이전해야할 시점이었습니다. 덕분에 저는 저희 개발 환경을 처음부터 구축하고, 우리의 서비스를 배포해보고, 그 후 운영하면서 여러 경험들을 할 수 있었습니다.\nTerraform으로 처음 접해 본 IaC 친구: IaC가 뭔 지 알아? 써봤어?\n본인: 뭐 인프라를 코드로 관리한다는 건데, 나도 몰라 ㅋㅋ\n본인: 아마 너무 고급 기술이라 대학 졸업할 때 까진 못 써볼 듯?\n위의 대화는 제가 입사하기 약 일주일 전에 나눴던 대화인데, 입사 후에는 어느 덧 IaC와 꽤나 친근해진 것 같아 유머삼아 유머로 가져와봤습니다. 메가존 클라우드의 클라우드 원팀에서 일하기 전까지는 IaC는 저와는 거리가 먼 토픽이었고, Kubernetes 또한 minikube로 몇 가지 Object들을 배포해본 것이 다였습니다. 하지만, 저는 저희 팀에서 근무하게 되면서 terraform을 통해 위의 차트에 그려진 모든 인프라를 구축하고 파이프라인을 구축하게 됩니다.\nterraform을 사용하며 느꼈던 점은 \u0026lsquo;장점만 존재하는 기술은 드물 것이다\u0026rsquo;라는 점입니다. 분명 대부분은 장점이 있으면 그에 따른 단점이 존재할 것이고, 개인적으로는 테라폼도 장단점이 공존하고있다는 느낌을 받았습니다. 저의 주관적인 느낌에 따른 가장 큰 장점과 단점을 몇 개만 설명해보겠습니다.\nTerraform 단점 우선 단점부터. \u0026ldquo;사소한 인프라 변경도 코드에 반영하려면 문서를 찾아봐야하고, plan 내용을 검토해야한다\u0026rdquo;.\n저희는 평소에는 EKS의 로그를 켜놓지 않았습니다. 그런데 언젠가 EKS 로그를 CloudWatch를 이용해 분석해야한 적이 있는데, AWS 콘솔에서 바로 눈 앞에 로그 설정 칸이 있었음에도 EKS 로그를 설정하는 terraform Docs를 찾아본 뒤 plan을 분석한 뒤 apply 했어야합니다. 물론 아마도 로그 설정 쯤이야 잠깐 콘솔로 설정했다가 콘솔로 해제하면 그 한 번쯤은 문제가 없었겠지만, 그런 식으로 이번 한 번만, 이것쯤이야 하면서 코드가 아닌 매뉴얼로 직접 인프라 형상을 제어하는 경험이 쌓이게되면 형상이 깨져서 다시 형상을 맞추기 힘들수도 있고, 애초에 그런 수작업이 많이 들어가야하는 경우가 있다면 오히려 IaC를 이용할 필요가 없다고 생각했기 때문에 최대한 인프라는 terraform code로만 작업한다는 저의 원칙을 지키기 위함이었습니다.\n즉 자동화보다는 수작업이 편한 경우는 굳이 IaC라는 컨셉을 이용하는 것이 더 불편한 경우도 존재할 수 있을 것 같다는 생각이 들었습니다.\nTerraform 장점 \u0026ldquo;내가 사용하고 있는 인프라 전체를 한 눈에 보기 쉽다\u0026rdquo;\n물론 클라우드 서비스 하나에 대한 정보를 보고싶으면, 웹 콘솔에 들어가서 확인하는 것이 편하겠지만, 내가 이용하고 있는 AWS내의 모든 클라우드 서비스에 대한 정보나 설정을 보기에는 terraform 코드나 output, state 등이 더 알아보기 편할 수 있습니다.\n\u0026ldquo;혹시 장애가 생긴 경우 그 원인을 추적하기 쉽다.\u0026rdquo;\n수작업으로 클라우드 인프라를 관리하는 경우에 자신이 모르는 어떤 변동사항이 있고, 그 변동사항이 어떤 버그를 야기하고 있다면, 수작업으로 작업을 진행하는 경우에는 그 변동사항을 알아채고 트러블슈팅하기 쉽지 않을 것입니다.\n하지만 테라폼 코드를 통해 인프라를 관리하면 변동사항을 테라폼이 알아서 잡아주고, 혹은 코드에 대한 커밋 내역 등을 통해 변동 사항을 체크해 볼 수도 있을 것입니다. 따라서 그 변동사항에 맞는 트러블 슈팅을 하기 쉬울 것 입니다.\n개발 관련 다양한 서비스 배포 테라폼으로 인프라를 구축한 뒤에는 위의 차트에서 오른쪽에 작게 Stargate 라는 곳에 적힌 서비스들을 배포했습니다. 이 부분에 대해서는 너무나도 하고싶은 얘기가 많지만, 지루해질 수 있으니 그 중 기억에 많이 남거나 애착이 가는 서비스들에 대한 리뷰를 간단히 적어보겠습니다.\nNginx Ingress Controller 로컬에서 minikube로만 개발하다가 처음으로 Ingress를 사용하게 되었습니다. helm을 이용한 게 아니라, helm으로 만들어진 manifest를 수동으로 하나하나 설정해서 설치하고 관리했습니다. 설정이 꽤나 복잡했었기에 설정을 많이 변경하는 경우 helm으로 설치하는 건 어떨까싶습니다\u0026hellip; 일하면서 Nginx로 L7 로드밸런싱 뿐만 아니라 L4 로드밸런싱도 몇 번 다룰 일이 있었고, 여러 Nginx Ingress Controller을 배포할 일도 있었고, gRPC 통신을 위한 설정을 해야할 일도 있었는데, 점점 수작업으로 하다보니 설정이 너무 복잡해져서 헷갈렸던 적이 있습니다. 뭔가 단점만 적은 것 같은데, K8s의 Ingress Controller로서 성능적인 부분은 제가 잘 모르겠지만, 크게 불편함 없이 잘 사용했습니다. ALB Ingress Controller 서비스에 대해 직접 L7 로드밸런싱을 수행할 경우 이용합니다. 헷갈렸던 점은 보통은 Nginx Ingress Controller는 사실 Ingress는 설정이고, 컨트롤러가 실제로 로드밸런싱을 수행하는데, ALB는 ALB Ingress Controller가 Ingress를 통해 ALB를 만들고 실제 로드밸런싱은 ALB Ingress Controller가 아니라 ALB가 한다는 점이었습니다. 설정에 따라 다르지만 기본적으로는 노출시키고자 하는 서비스는 NodePort급 이상으로 서비스가 열려있어야하는데, 실수로 ClusterIP로 노출시켜 ALB가 서비스를 제대로 찾지 못한 적이 종종 있었습니다. 주의..! Cert Manager 자동으로 cert를 발급해주고 갱신해주는 서비스입니다. Let\u0026rsquo;s Encrypt를 이용했고 무료입니다. 정말 정말 편리했습니다! DNS, HTTP Challenge, TLS에 대해 많이 배울 수 있었습니다. TLS 통신 과정은 공부해도해도 정확한 순서는 까먹게 돼서\u0026hellip; 다시 공부해봐야겠습니다. Jenkins 편한데, 관리하기 귀찮을 수 있을 것 같습니다. 결정적으로 코드로 관리할 수도 없는데, UI도 그렇게 직관적인지는 잘 모르겠습니다. Spinnaker 정말 편리합니다. Jenkins test 결과를 CD에 이용할 수도 있고, 편리하고 다양한 문법, 파이프라인 종류, 직관적인 UI/UX. 사실 Argo를 도입하고 싶었지만, Argo는 약간 가벼운 쿠버 환경에 대한 배포용 같은 느낌, IaC는 적극 도입되었으나 현실에 도입은 쉽지 않은 기술적 이상향 같은 느낌이 컸고, 현실적으로는 좀 더 안정적인 Spinnaker를 유지하게 됐습니다. 좀 느리다고 생각했는데, 이 글을 쓰면서 생각해보니 개발에서는 K8s Deployment의 Grace Period 때문인 것 같아 그걸 좀 줄여볼 껄 싶습니다. 즉 다시 생각해보니 별로 느리지 않은 듯합니다. 배포 자체는 안정적이고 좋은데, 설정할 때 버그가 종종 있습니다. 버그인지 제 실수인지는 모르겠지만 Pipeline stage 에서 Bake라는 모드가 \u0026ldquo;우와 신박하다!\u0026rdquo; 라고 생각했지만, 설정이 제대로 되지 않거나, OAuth login 실패에 대한 처리, Artifact 경로 설정이 너무 번거로운 점 등의 단점이 있었습니다. Keycloak 우리의 개발툴들에 대한 인증을 사내 계정으로 연동시켜주는 Single Sign On 기능을 지원했습니다. 새로운 팀원은 각 서비스에 별도의 가입 과정 없이, 앱 레벨에서 따로 권한 부여가 필요한 것이 아니라면 사내 계정으로 바로 이용이 가능했습니다. 문서가 제대로 정리된 게 다소 부족한 느낌이었어서 세밀한 설정이나 정확한 작동원리를 파악하기는 쉽지 않았던 점이 조금 아쉽습니다. 인프라 구축에 대한 정리 내용이 너무 길어지면 읽기 힘들 것 같아 최대한 느낀 점 위주로 간단히 정리해보려고 노력해보았습니다. 인턴으로 일을 하기 전에 가벼운 마음 속 목표가 하나 있었습니다.\n\u0026lsquo;어떤 걸 어느정도 사용해보면 그것에 대한 주관적인 평가를 내릴 수 있는 사람이 되고싶다.\u0026rsquo;\n누군가 \u0026ldquo;도커가 설치하기 정말 쉽더라구요.\u0026rdquo;, \u0026ldquo;minikube 써보니 정말 편리하더라구요.\u0026rdquo;, \u0026ldquo;Jenkins 보다는 travis가 편리더라구요.(혹은 그 반대)\u0026rdquo; 이런 얘기가 나와도 과거에는 공감할 수도, 제 주관적인 평가를 내릴 수도 없었습니다. 저의 경험으로 구성된 모집단이 없었기 때문입니다. 하지만 저희 클라우드 원 팀에서 근무하면서 어떻게 보면 깊이는 다소 얕았을 지 몰라도 정말 다양한 서비스를 접하고 다채로운 경험을 할 수 있었던 것 같습니다! 덕분에 이제는 어떤 서비스를 접하든, 새로운 언어를 접하는 저만의 느낌을 가질 수 있고, 의견을 말할 수 있을 것 같습니다!\n","date":"2020-09-04T12:46:54+09:00","image":"https://umi0410.github.io/blog/megazone-cloud/stargate-infra/dev-architecture.svg","permalink":"https://umi0410.github.io/blog/megazone-cloud/stargate-infra/","title":"1. Stargate라는 인프라 구축기"},{"content":"Github Action을 사용하게 된 배경 저희 팀은 원래 CI용으로 Jenkins를 사용했습니다만 팀이 개발 중이던 서비스가 오픈소스가 목표인 프로젝트였고, Github Action이 빠르게 발전해나가면서 비용도 무료가 되었고, 좋은 Action들이 많이 생겨나고 있었기에 어느 정도 프로젝트 구조가 잡힌 뒤에는 Github의 Public Repository로 프로젝트를 관리하고 Github Action을 CI 도구로 채택하게되었습니다. Integration Test를 제외한 모든 빌드 및 일부 배포를 Github Action을 이용하게되었고, 대부분의 배포에는 사용하던대로 Spinnaker을 이용했습니다.\nGithub Action vs Jenkins 이 부분 역시 느낀 점 위주로 요약해보겠습니다.\nGithub Action Jenkins 내가 서버를 관리할 필요가 없다. 내가 직접 master을 띄우고, slave를 띄워우고, 관리해줘야한다. VM이 배치되어 제공되는 데에 좀 시간이 든다. 내 커스텀 이미지를 사용할 수가 없다보니 반복되는 패키지 설치나 환경 설정을 매번 해야해서 좀 느리다. 내가 필요한 Plugin을 설치해놓거나 설정을 입력해 놓으면 매번 빌드할 때 따로 제공할 필요 없다. Code로 관리가 가능하다! 처음엔 조금 어려울 수 있지만, 알고 나면 쓰기 너무 쉽다. 처음 접한 사람이 사용하기에는 Github Action보다 편리할 수 있지만, 그렇다고 훌륭한 UI/UX는 아닌 듯 하다. 요즘 핫하고, 빠르게 발전 중이다. 구식이다. 비용이 저렴하게 풀리고 있고(퍼블릭의 경우 아마 무조건 무제한 공짜), 원래 지원하지 않던 매뉴얼 트리거가 2020.07부터 제공되기 시작했다는 점, 누구든 오픈소스로 Github Action에서 남들이 사용할 수 있는 action 을 만들 수 있다는 점등을 보고 Github Action이 빠르게 발전 중이라는 생각이 들었습니다.\n그래서 어떤 CI/CD를 자동화하였나요? ci-cd-pipeline.png 세로로 길어서 좀 보기 불편하실 수도 있는데, 위의 차트가 저희의 깃헙액션에 대한 차트입니다. 레포지토리마다 조금씩 다른 부분이 있고, 프론트엔드의 경우 파이프라인이 다양했는데, 우선 백엔드의 깃헙액션 및 CI/CD 진행 방식에 대해 요약해보겠습니다. (Chart를 작성한 지가 좀 돼서 설명과 조금 다른 부분이 있을 수도 있습니다.)\n개발 스프린트 진행 시에는 아래와 같이 진행되었습니다. (\u0026lt;상황 설명\u0026gt; =\u0026gt; \u0026lt;Github Action 수행 내용\u0026gt; 형식으로 나했습니다.)\n개별 Fork 후 Master Branch에 Pull Request =\u0026gt; lint, basic unit test 진행. 통과된 PR만 Merge열 Master에 Merge 혹은 Commit이 Push됨 =\u0026gt; CI Action이 실행됩니다. 개발용 docker registry 에 업로드 해당 registry에 업로드 된 것을 감지하고 Spinnaker가 Experiment 환경에 배포 Experimental 환경을 이용해 Jenkins가 Integration Test를 진행 Integration Test 성공 시에 Dev 환경에 배포. 이후 스프린트 막바지 QA 기간에는 Experiment, Dev 환경이 주로 QA 환경으로 사용되었고, 파이프라인은 다음과 같았습니다.\n검증이 어느 정도 끝난 커밋에 대해 Git Tag를 {{VERSION}}-rc{{RC_NUMBER}} 형태로 달아 푸시 - Github Release가 생김 =\u0026gt; 도커 이미지 빌드 후 Production Docker Registry에 이미지 업로드 =\u0026gt; tag가 달린 커밋을 기점으로 자동으로 버전 명의 브랜치를 만듦. Production Docker Registry의 업로드를 감지하고 Dev에서 손수 QA 진행 문제가 있을 경우 rc number를 올려서 다시 태그를 달고 업로드 후 재차 QA 문제가 없을 경우 해당 태그를 바탕으로 rc를 지우고 실제 버전으로서 태그를 달아 푸시 =\u0026gt; 다시 업로드 Production은 매뉴얼 배포. Github Action을 다뤄보면서 느낀 점 처음에는 Github Action에 그렇게 만족을 하지 못했습니다. 초기에는 Manual Trigger가 지원되지 않았던 데다가, 가뜩이나 Github Action을 잘 몰랐기에 한 번 Github Action을 수정하여 테스트 하고싶을 때 마다 커밋을 하나씩 날려야했던 게 불편했고, 브랜치를 따로 만들거나 fork를 떠서 Github Action 테스트 후 해당 workflow만 마스터에 머지하는 방식 등등 다양한 방식을 사용했었는데, 어느 정도 익숙해지고 Github Action에 Manual Trigger도 등장하게 되면서 꽤나 만족도가 높아졌습니다.\nJenkins와 달리 제가 서버를 이용하지 않아도 된다는 점도 맘에 들긴했는데, 종종 Github 서버가 죽는 일이 발생해서 난감했던 적이 있긴합니다.\n하나 재미있었던 점은 Github Action을 이용하면서 저희의 CI/CD 전략이 꽤나 고도화되었는데, 그 과정에서 팀원들과 자유롭게 의사소통하는 과정이 재미있었고, 저 또한 자유롭게 의견을 나눌 수 있었던 경험을 할 수 있었다는 것입니다.\n후에 저희 SpaceONE의 CLI API 클라이언트인 spacectl의 설계에도 Github Action의 구조를 모티브삼았는데 이때에도 Github Action에 대한 지식이 많은 도움이 되었고, 퇴사 후에도 개인적인 Github 활동을 하면서 자유롭게 Github Action을 사용할 수 있었기에 든든한 개발 도구를 얻은 느낌입니다. 코드로 제가 하고싶은 것을 뭐든 정의할 수 있고, 만들어져 있는 작업은 편하게 가져다 쓰면 되기 때문에 빌드나 배포에 관해 재미있는 번뜩이는 아이디어가 있을 때 바로 바로 적용할 수 있고, 실제로 현재의 Github Page도 Github Action을 통해 다양한 트릭을 이용할 수도 있었고, 빌드 후 배포 또한 자동화 되어있습니다!\n","date":"2020-09-04T12:46:54+09:00","image":"https://umi0410.github.io/blog/megazone-cloud/ci-cd-pipeline/ci-cd-pipeline_hu2d78a6f76281dd93e8329c8b5a44cef9_282268_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/megazone-cloud/ci-cd-pipeline/","title":"2. Github Action, Spinnaker을 이용한 CI/CD 파이프라인 구축기"},{"content":"Argo PoC를 진행하게 된 배경 입사 초기부터 Argo에 대해 간간히 이야기를 들어왔습니다. 저희는 원래 Spinnaker을 사용했는데, 쿠버네티스 환경에 좀 더 친화적이라는 Argo를 도입해보는 것은 어떨까에 대한 얘기였는데요. 어느정도 개발 인프라 구축이 완료되고 한가해지자 잠깐이나마 Argo를 사용해볼 수 있었습니다. 결과적으로 Argo 도입은 적합하지 않다고 판단이 되었고 따라서 충분히 써볼 수는 없었기에 기록용으로만 간단히 적어보겠습니다.\nArgo Project 구조 Argo가 워낙 빠르게 변화하는 서비스다보니 지금은 구조가 많이 변경되었을 수도 있습니다!\nargo_1 argo_2 간편하게 사용할 수 있는 순서는 argo, argo(workflow), argo event 순이라고 생각되는데, 실질적인 파이프라인 구축은 argo event, argo (workflow), argo 순으로 이뤄지는 셈이라 후자의 순서에 맞춰 설명해보겠습니다.\nArgo Event (Official site) argo-events-top-level.png 셋 중 가장 베이비 프로젝트입니다. 빠르게 개발되고 있고, 변화하고있기에 제가 argo project들을 만났던 시절과 많이 구조가 달라져있습니다. 간단하게 말하자면 이벤트를 감지하여 어떤 작업을 수행할 수 있습니다. argo-events의 공식 홈페이지에서 제공되는 위의 이미지와 같이 다양한 이벤트 소스를 이용해 다양한 이벤트를 트리거할 수 있습니다. 예를 들어 AWS SQS에 메시지가 생기면, 그 메시지를 가져와서 어떤 작업을 수행할 수 있습니다. 혹은 웹훅 서버를 돌려서 웹훅 요청이 오면, 어떤 작업을 트리거할 수 있습니다.\n제가 구상한 파이프라인에서는 이벤트를 감지하는 역할로서 Argo Events를 앞에 두고, Argo Events가 Workflow를 생성는 작업을 트리거하는 방식으로 CI/CD에 이용할 수 있습니다.\nArgo, Argo Workflow (Official site) 셋 중 가장 오래된 프로젝트이고, 별도 많습니다. Argo 라는 이름을 가졌고 실질적으로는 Workflow 관련 프로젝트입니다. Workflow란 컨테이너를 이용해 진행되는 일련의 step들을 정의하는 CRD(K8s Custom Resource Definition 입니다.)\n처음엔 \u0026lsquo;굳이 Workflow가 필요할까\u0026rsquo; 싶었지만, 쿠버네티스 상에서 일련의 Job을 연속적으로 수행할 수 있는 방법이 현재까지는 없는 것으로 알고 있습니다. K8s Job의 Container에 대한 InitContainer을 지정함으로써 한 Job에 대한 두 Container의 순서를 명시할 수는 있지만, Workflow처럼 다양하게 일련의 Job을 연속적으로 수행하기는 힘든 것으로 알고 있습니다.\nArgoCD (Official site) ArgoCD만 보면 가장 간단하게 실제 CD에 적용할 수 있는 프로젝트 중 하나가 아닐까싶습니다. ArgoCD 자체에 대한 설정, 소스가 될 Repository에 대한 설정, Project, Application에 대한 설정 등등 모든 것이 Code IaC에 특화된 독특한 프로젝트입니다.\nUI가 직관적이고 알아보기 쉽지만, 반대로 대규모 애플리케이션이 될 경우 너무도 배포 현황을 보여주는 맵이 커지기 때문에 알아보기 쉽지 않을 수 있을 것 같습니다. GitOps(\u0026ldquo;GitOps의 핵심은 Git 저장소에 저장된 쿠버네티스 매니페스트 같은 파일을 이용하여, 배포를 선언적으로 한다는 것입니다. 출처\u0026rdquo;) 라는 말에 아마 빠지지 않고 등장하는 CD 도구인듯합니다. Git에 올라가는 Manifest가 그대로 K8s 클러스터에 적용됩니다. 마치 Github Page나 S3(웹호스팅 설정이 된 S3 Bucket)에 올린 파일들이 바로 하나의 웹 애플리케이션처럼 동작하는 것과 비슷한 느낌입니다. 간단하게 사용하기에는 ArgoCD가 참 좋아보였지만, 실제로 업무적으로 사용하지는 못하겠다고 판단한 이유가 몇 가지 있습니다.\n애플리케이션 하나에 대한 배포는 쉽지만, 연속적인 배포나 다양한 배포 파이프라인을 구성하기는 어려웠다. Github Push 시에 webhook을 설정해서 ArgoCD의 배포를 트리거하도록 했는데 이 부분이 최적화가 덜 되었는지 어떤 repository든 하나의 ArgoCD 환경에서는 Git platform 당 하나의 secret만 설정이 가능했고, 한 repository가 push되면 다른 repository도 배포가 다 같이 이루어져버렸습니다. (이 부분은 패치됐을 수도 있습니다.) 소규모 애플리케이션은 배포 상황을 한 눈에 보기 쉽지만, K8s Object가 많아지면 많아질수록 점점 하나하나 눈에 보이지 않고, 잡다한 Object들(ConfigMap, Secret, \u0026hellip;)로 인해 인식하기 힘듭니다. Argo Project를 조합한 CI/CD 파이프라인 구축기 파이프라인 수행 과정 (Argo Event) Github Push를 감지하고 Argo Workflow CRD를 생성 Argo workflow를 통해 다양한 step들을 이용하기 위해 바로 ArgoCD가 Github Push를 받지 않고, Argo Event가 받도록 했습니다. Argo Workflow의 step들 수행 Slack에 workflow 시작 알림 Experiment 환경에 배포 (Argo CD 이용) Interation Test 실행 트리거 (Jenkins 이용) Integration Test 결과가 성공이면 Dev 환경에 배포 (Argo CD 이용) 어느 step에서든 실패 시 Slack에 실패 알림 이 정도 Pipeline을 짜면 사실 사용할 정도는 될 수 있겠지만, \u0026ldquo;일\u0026quot;로서 본다면 굳이 현재 사용 중인 Spinnaker에 비해 장점이 뚜렷하게 느껴지지 않는 Argo를 위해 파이프라인을 이전하기에는 역부족이라는 판단이 들었습니다. 예를 들어 과정 목록에서는 간단히 묘사했지만, Spinnaker에서는 UI에서 알아서 처리되던 부분들을 하나 하나 webhook을 걸어주거나, 따로 bash script를 짜야하는 경우들이 많았습니다.\nArgo Project들에 대한 PoC를 진행하며 느낀 점 개인적으로는 많이 애정이 갔던 프로젝트들이고 신기했던 배포방식에 대한 소개였으며, 이 프로젝트들을 공부해보면서 다양한 아키텍쳐에 대해 경험해볼 수 있었던 것 같지만, 현실적인 벽에 부딪혀 도입을 할 수 없었던 아쉬움 컸습니다. 마치 좋아하는 분야가 있었지만, 현실의 벽에 부딪혀 꿈을 접고, 어떠한 현실적인 진로 나아가는 것과 같았달까요?\n그리고 개인적인 아쉬움은 위의 차트를 손수 그리며 PoC 문서를 작성했었지만, 결국은 PoC를 진행한 담당자인 저조차 \u0026ldquo;음\u0026hellip; 실사용은 힘들 것 같은데요\u0026quot;라는 의견을 냈던 터라 바쁜 일정 속에서 reject할 개념 증명 리뷰에 많은 시간을 할애할 수 없었기에, 팀원들과 이 내용을 공유할 수 없었던 점에 조금 아쉬움이 남습니다..! 만약 정말 Argo를 사용하는 방식이 뛰어난 방식이었다면, 저도 적극 추천하며 함께 리뷰해주길 기대했겠지만, 현실적으로 저희 팀의 배포 방식과는 맞지 않았다고 판단해서 그랬던 것이기에 괜찮습니다~!\n","date":"2020-09-04T12:46:54+09:00","image":"https://umi0410.github.io/blog/megazone-cloud/argo-poc/argo-events-top-level_hu911ca173f7b6efecb06ab37a91a70151_401443_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/megazone-cloud/argo-poc/","title":"3. Argo Project들에 대한 PoC(개념 증명) 진행"},{"content":" spaceone-helm-preivew.png SpaceONE Helm Chart란? spaceone-helm 은 저희 CloudOne 팀이 개발하는 서비스인 SpaceONE을 helm chart를 이용해 패키지화하는 프로젝트입니다. 원래의 저희 환경은 MicroService들을 개별 배포하고있었지만 오픈소스로 개발되는 저희 서비스를 저희 팀원들 뿐만아니라 다른 개발자들이 쉽게 개발할 수 있고, SpaceONE을 모르던 사용자들도 쉽게 SpaceONE을 구축해볼 수 있도록 하기 위해 패키지화도 진행하게되었습니다.\n일반적으로 만들어진 Helm Chart를 이용해보기만했지 직접 Chart를 만드는 것은 처음 해본 일이기도 했고, Chart를 개발하면서 새로운 환경에 저희 서비스를 배포해보다보니 삽질하며 고생도 꽤 했고, 무엇보다 프로젝트의 시작부터 퇴사 전까지의 작업들을 거의 제가 도맡아한 프로젝트였기에 개인적으로 애정이 많이 갔습니다 ^_^! 그리고 저도 이제는 저희 Chart를 이용해 SpaceONE을 구축형으로 손쉽게 이용할 수 있었습니다.\nspaceone-helm 설계 일단은 심플하게 사이드카 없이 저희의 마이크로서비스들만을 배포하고, 그 외에 필요한 서비스들도 최대한 Cloud Service에 의존하지 않고 그때 그때 손쉽게 서비스를 내렸다 올렸다 할 수 있도록 K8s 클러스터 위에 함께 배포하는 형식으로 Chart를 설계했습니다.\n무엇보다 chart 개발의 목적은 아래 두 가지 사항이 컸기 때문에, minikube 혹은 EKS 띄운 마이크로서비스들과 로컬에서 통신이 가능하도록 해야했고, 사용하기 편리한 구조를 만들기 위해 고심했습니다.\n팀원이 아닌 개발자들도 자신의 부가적인 마이크로서비스를 로컬에 띄워 개발할 수 있도록 지원. 로컬에서도 쿠버네티스 클러스터 내의 서비스에 접속이 가능해야하고, 경우에 따라 클러스터에서도 본인의 로컬 서비스로 접속을 할 수 있어야함. 오픈소스로서 임의의 사용자가 서비스를 구축해보고자 시도할 때 손 쉽게 구축할 수 있도록 지원. 마치 유저들의 사용자 경험을 중요시해서 디자인, 기획을 하듯 Chart의 사용자들이 직관적이고 편리하게 구축할 수 있도록 Configuration values(values.yaml in Helm Chart)를 설계함. spaceone-helm chart 구조 templates/ ├── backend │ ├── config │ │ ├── config-conf.yml │ │ ├── config-deployment.yml │ │ └── config-svc.yml │ ├── identity │ │ ├── identity-conf.yml │ │ ├── identity-deployment.yml │ │ └── identity-svc.yml │ ├── inventory │ │ ├── inventory-conf.yml │ │ ├── inventory-deployment.yml │ │ └── inventory-svc.yml │ ├── inventory-scheduler │ │ ├── inventory-scheduler-conf.yml │ │ ├── inventory-scheduler-deployment.yml │ │ └── inventory-scheduler-svc.yml │ ├── inventory-worker │ │ ├── inventory-worker-conf.yml │ │ ├── inventory-worker-deployment.yml │ │ └── inventory-worker-svc.yml #... Backend 생략 ├── consul, mongo, redis # 디테일한 파일구조는 생략 ├── frontend │ ├── console # 디테일한 파일구조는 생략 │ └── console-api # 디테일한 파일구조는 생략 ├── ingress │ └── ingress.yaml ├── initializer │ ├── initialize-spaceone-conf.yml │ ├── initialize-spaceone-job.yml │ ├── spacectl-apply-conf.yml │ └── spacectl-conf.yml └── supervisor └── supervisor ├── supervisor-conf.yml ├── supervisor-deployment.yml └── supervisor-roles.yml 저희 spaceone-helm chart는 꽤나 구조가 간단한 편은 아니라고 생각됩니다. 웹페이지 환경상 너무 부수적인 부분은 tree에서 생략하기도 했습니다. 꽤나 복잡한 구조를 가졌기에 패키지화하는 부분이 쉽지 않았지만, 그런 과정 속에서 많이 트러블 슈팅을 경험하고 성장할 수 있었던 것 같습니다.\n그리고 반대로 생각하면, 서비스가 패키지화하기도 쉽지 않을만큼 복잡한 구조를 가졌다면, 당연히 손수 배포하는 것은 그것보다 몇 배는 어려울 것이므로 누군가가 저희 서비스를 구축형으로 이용해주기 위해서는 패키지화가 필수라는 생각이 들었습니다.\n마이크로서비스들의 버전 관리를 시작 Helm Chart를 개발하고, CI가 고도화되기 전까지는 단순히 팀 내에서 개발을 진행하면서 개발환경에 대한 배포는 latest tag만을 이용해 자동으로 진행하고, QA를 진행하면서 커밋을 멈추고, 그 시점에 빌드된 Docker image의 Tag를 바탕으로 상용 환경에도 배포를 하곤했습니다. 하지만, Chart에서도 tag를 latest로 유지하거나 가독성이 좋지 않은 임의의 tag를 이용해 이미지를 제공하기는 힘들었고, 맞는 방향이 아니라고 생각했습니다.\n따라서 저희는 꼭 Helm을 통한 패키지 뿐만 아니라 롤백에 대한 안정성, 버전 간의 Update 내역 관리 등을 위해 {{MAJOR}}.{{MINOR}}.{{SPRINT_NUMBER}}-{{EXTRA_TAGS}} 형태를 통해 버전을 관리하고자했습니다.\nHelm Chart를 통해 패키지화하며 느낀 점 Helm Chart를 통해 저희 서비스를 패키지화하기 전까지는 마이크로서비스 형태로 관리되는 서비스를 새로운 환경에 완전하게 구축한다는 것은 쉽지 않았습니다. Container라는 것이 ReadOnly 레이어로 이미 구성이 완료된 이미지를 바탕으로 생성되기 때문에 언제 어디서든 구동이 가능하다는 것이 장점이겠지만, 현실적으로는 환경에 따라 이것 저것 설정해줘야하는 것이 있었기때문입니다.\n이러한 난관들은 어떠한 변수처리와 자동화를 통해 해결할 수 있을텐데, 그러한 작업을 해준 녀석이 바로 Helm이었고, 그때 그때 설정을 바꿈으로써 커스터마이징할 수 있다는 점이 알고는 있었습니다만 막상 저희 서비스에 도입해보니 꽤나 만족스러웠습니다.\n일화로 사내 네트워크가 막힌 상황에서도 받아놓은 이미지들만 있으면 네트워크 접속을 할 필요 없이 minikube와 로컬 서버를 이용해 minikube의 클러스터와 통신하면서 개발을 진행할 수도 있었습니다.\n누군가 제가 만든 서비스를 사용해준다는 것은 참 뿌듯한 일이었고, 앞으로도 저희 helm chart가 잘 발전되어 더욱 더 편리하게 구축할 수 있는 형태로 제공될 수 있기를 기대해봅니다!\n","date":"2020-09-04T12:46:54+09:00","permalink":"https://umi0410.github.io/blog/megazone-cloud/spaceone-helm/","title":"4. SpaceONE Helm Chart 개발"},{"content":"spacectl이란? 소개 spacectl 은 저희팀이 개발하는 서비스인 SpaceONE의 gRPC API request를 CLI로 손쉽게 수행할 수 있도록 해주는 도구입니다. 파이썬을 통해 개발했고 Click 이라는 모듈로 CLI 환경을 손쉽게 사용할 수 있었고, Jinja2를 통해 상세한 Manifest 들에서 변수 치환, 분기 등을 수행할 수 있었습니다.\n사용 예시 A simple example 간단하게 spacectl이 어떤 식으로 이용되는 도구인지 예시를 보여드리겠습니다. 아래의 커맨드를 이용해 손쉽게 SpaceONE의 다양한 마이크로서비스들의 API를 이용할 수 있습니다.\n$ spacectl list domain domain_id | name | state | plugin_id ... domain-abc123abc | umi0410| ENABLED | ... $ spacectl list server -p domain_id=domain-abc123abc server_id | name | provider ... server-abc123abc | foo | aws ... apply command spacectl apply command는 kubectl의 apply와 유사하게 없으면 만들고, 있으면 업데이트하고 혹은 단순히 어떤 API를 Execute하는 Task들의 플로우를 관리해주는 커맨드 입니다.\n아쉽게도 퇴사 전에 마무리를 짓지는 못했습니다. ㅜ.ㅜ 퇴사 전까지 진행한 작업은 일부 리소스에 대한 CRU(create, read, update), 대부분의 리소스에 대한 Excute(execute할 API를 설정)까지입니다.\n# main.yaml import: - mongo.yaml # 개별 yaml file에서는 terraform/ansible과 같이 수행할 Task들을 정의 - root_domain.yaml - repository.yaml var: domain_name: root domain_owner: admin admin_username: admin admin_password: admin $ spacectl apply main.yaml 설계를 하며 느낀 점 이 프로젝트에 대한 실제 개발 업무 이전에는 꽤나 설계 업무가 많았습니다. 저는 그 동안은 혼자 주로 개발을 해왔고, 실행력 좋게 시작은 하지만 설계는 충분하지 않은 채 성급하게 실행에 옮겼던 경험이 많습니다. 또한 학생이었고, 개발 경력이 길지 않았기에 사실상 \u0026ldquo;개발 = 그때 그때 새로운 내용 공부\u0026quot;와 같은 느낌이었기에 애초에 설계를 하려해도 \u0026lsquo;뭐가 필요하고 뭐가 가능할 것이고 뭐가 힘들 것인가\u0026rsquo; 를 판단하기 어려웠습니다.\n하지만 팀원들과 함께 개발하면서 밥 먹을 때, 회의 할 때 틈틈히 설계 방식과 요령에 대해 상의했고, 처음으로 설계를 어느 정도 굳힌 뒤 개발에 들어들어갔던 경험이었습니다.\n인턴 기간 막바지에 이 설계에 참여하게 된 것은 정말 값진 경험이었다고 생각했습니다. 사실 단순히 데브옵스로서 일할 때는 남들과 의사소통할 일이 그리 많진 않았는데, 이 설계를 맡게 되면서 많은 회의와 대화를 하게되었습니다. 선배 개발자분들과 설계에 대해 잦은 회의를 하면서 제가 어떤 개발자가 되고싶은지 직접 느낄 수 있었던 것 같습니다. 그 배경에는 두 가지 충격이 있었습니다.\n\u0026lsquo;내가 남의 생각을 잘 읽는 편은 아니었나보군\u0026hellip;?\u0026rsquo; \u0026lsquo;선배 개발자분은 내가 개판으로 설명해도 어떻게 귀신같이 나보다 내 생각을 잘 읽으시지?\u0026rsquo; =\u0026gt; 마치 축구할 때 노련한 축구선수와 함께 뛰면서 저 행동을 귀신같이 예측하고서는 너무나도 잘 밀어주는 느낌을 받았습니다\u0026hellip; 제가 평소에 말을 잘하는 편이라고 생각했는데, 남의 생각을 이해하고 읽어내는 능력은 그리 뛰어나지많은 않구나라는 생각을 하게됐습니다. 설계 내용이 꽤나 추상적으로 구두로 진행되었기에 그랬을 수도 있겠지만, 큰 충격은 선배 개발자분의 노련함이었습니다.\n후에 누군가 어떤 개발자가 되고싶냐, 협업할 때 어떤 노하우가 있느냐 이런 내용을 물어보면 자신있게 남의 생각을 잘 이해하고, 알아주는 사람과 관련해 대답할 수 있는 사람이 될 수 있었으면 좋겠습니다.\n또한 설계 깊게 진행하기 전에 침착하게 자료 조사를 잘 해야한다는 것을 느꼈습니다.\n하나 일화로 원래는 설정 파일에서 ${{ tasks.umi0410.output }} 이런 식의 변수를 이용한 설정을 치환한 뒤 API를 수행해야할 때, 아마 template 언어들을 제가 원하는 대로 사용하기 힘들 것이라 생각하고, 하나하나 함수와 클래스를 만들어가곤했는데, 개발이 거의 완료되어갈쯤 Jinja2의 사용법을 다시 읽다보니 spacectl에 Jinja2를 적절히 사용할 수 있을 것 같았고, 정신이 번뜩 들어 몇 시간만에 수작업으로 짠 코드들을 덜어내고 Jinja2를 이용해 좀 더 깔끔하게 변수 치환 및 추가로 Jinja2의 built-in filter들을 이용할 수 있었습니다!\n개발하면서 느낀 점 역시 개발은 남이 만든 패키지를 잘 사용해야한다는 것을 느꼈고, 그냥 복사 붙여넣기만 잘하면 된다는 의미가 아니라, 그런 것들을 빠르게 가져와서 적용시키고 부분 부분 커스터마이징하기 위해서는 기본기가 탄탄해야한다고 느꼈습니다. 물론 수작업으로 만드는 것도 좋을 수 있겠지만, 다양한 엣지케이스가 존재할 수 있고, 그 모든 작업들을 문서로 상세히 설명하는 것이 아니라면, 제작자인 제가 아닌 누군가가 그 기능을 이용하기는 힘들 것입니다. Jinja2를 이용한 템플릿 기능 제공이 이와 관련된 경험이 될 수 있겠습니다.\n또한 남의 코드를 자세히 읽어보는 게 처음이었는데, 덕분에 파이썬 프로젝트를 수행할 때 어떤 식으로 디렉토리 스트럭쳐를 짜면 좋을 지 생각해볼 수 있었던 계기였던 것 같습니다. 제가 설계하고, 개발한 내용을 짧게나마 발표한 뒤 리뷰를 받고 수정을 하면 한층 더 코드의 구조와 사용이 간결하고 직관적으로 보인다는 느낌을 받을 수 있었습니다. 이렇게 다양한 가르침을 주신 저희 팀의 선배 개발자분들께 항상 감사드립니다.\nMegazone CloudOne 팀의 DevOps 인턴으로서 근무했던 내용에 대한 후기가 거의 끝났습니다. 끝으로 인턴 활동에 대한 종합적인 느낀점을 이어서 보시거나 다시 목차를 보고싶으신 분은 여기를 클릭해주세요.\n","date":"2020-09-04T12:46:54+09:00","permalink":"https://umi0410.github.io/blog/megazone-cloud/spacectl/","title":"5. SpaceONE CLI Client인 spacectl 설계 및 개발"},{"content":"🐶 시작하며 본 게시글은 AWS 대학생 유저그룹인 AUSG의 활동 중 하나로서 본인(박진수)이 작성한 게시물을 포워딩한 것입니다.\npreview.png 데브옵스 인턴으로 근무한 지가 벌써 두 달이 되어갑니다. 이것 저것 배운 것이 많았던 시간이었는데, 그 중 꽤나 삽질을 했던 Kubernetes 와 ELB를 이용하는 부분에 대해 정리를 해볼까합니다. jenkins, spinnaker, argo, terraform, ansible, github action, \u0026hellip; 등등 다양한 내용을 경험할 수 있던 시간이었지만, 그 중 kubernetes에서 무슨 작업을 하던 빼놓을 수 없으면서 어딘가 깔끔히 그 흐름이 정리된 곳을 보기 힘들었던 service를 ELB에 연결하기에 대한 내용을 정리해보겠습니다.\n본 포스트는 EKS를 통해 K8s를 이용할 때를 기준으로 설명합니다.\n💁🏻‍♂️ EKS 에서 ELB를 사용해 서비스를 노출킬 때 유의사항들 🧐 : \u0026quot; ELB, NLB, ALB 대체 뭐가 다른 거야..?ㅜㅜ 쿠버네티스를 쓸 때는 어떻게 얘네를 지정하는 거지..? kubectl expose deploy {{deployment_name}} --type=LoadBalancer 하면 그냥 작동은 하던데\u0026hellip;\u0026quot;\nEKS에서 주로 사용하는 ELB는 L4의 NLB와 L7의 ALB 입니다. ALB가 L7에 대한 좀 더 다양한 설정이 가능하기 때문에 조건이 많기도 하고, AWS의 ALB만을 위한 alb-ingress-controller라는 녀석이 직접 Ingress의 설정들을 관리해주기 때문에 설정할 수 있는 옵션도 많습니다. 좋게 보면 많은 설정을 할 수 있고, 나쁘게 보면 초보자에겐 귀찮을 수 있습니다. NLB는 비교적 설정이 적고 따라서 설정해줄 수 있는 항목도 적습니다.\n쿠버네티스에서 다양한 작업을 하면서 다양한 controller을 접하게 되고, 그렇게 될 수록 annotation으로 많은 설정을 하게 됩니다. k8s를 처음 접할 때에는 annotation에 대한 정의로서 아래와 같은 문장을 접할 수 있고, 마치 기능과 크게 상관이 없을 것처럼 느껴지기도 하지만 사실 EKS를 비롯한 여러 서비스에서는 annotation을 이용해 중요한 설정 등을 기입할 수 있기 때문에 잘 설정해주어야합니다. ELB또한 모든 설정이 annotation으로 동작한다.\n\u0026quot; Label을 사용하여 오브젝트를 선택하고, 특정 조건을 만족하는 오브젝트 컬렉션을 찾을 수 있다. 반면에, annotation은 오브젝트를 식별하고 선택하는데 사용되지 않는다. 어노테이션의 메타데이터는 작거나 크고, 구조적이거나 구조적이지 않을 수 있으며, 레이블에서 허용되지 않는 문자를 포함할 수 있다.\u0026quot;\n⚠️ ALB를 사용할 때 유의할 점 어떤 옵션들이 있고, 기본적으로는 어떻게 설정되는 지에 대한 이해가 있어야 오류 과정을 추적하기 쉬우므로 기본적으로 ALB를 AWS Console에서 사용해본 뒤에 설정할 것을 추천합니다.\nalb ingress controller가 생성할 ALB가 사용할 서브넷을 discover하기 위해서는 올바른 태그가 달린 subnet이 존재해야한다. node 혹은 alb ingress controller에 연결된 service account가 alb를 제어하기 위한 iam permission이 부여되어야한다. internet facing한 alb를 만들지 internal한 alb를 만들지 고민해봐야한다. alb ingress controller의 log를 통해 작업에 대한 log를 볼 수 있다. ⚠️ NLB, CLB를 사용할 때 유의할 점 https://kubernetes.io/ko/docs/concepts/services-networking/service/#aws-nlb-support\nhttps://docs.aws.amazon.com/ko_kr/eks/latest/userguide/load-balancing.html\nNLB, CLB가 사용할 서브넷을 설정하기 위해서는 올바른 태그가 달린 subnet이 존재해야한다. 어느 부분에선가 NLB, CLB를 제어하기 위한 iam permission이 부여되어야한다. (어느 부분인지 확실히는 모르겠음. 따로 설정안해도 동작하는 것을 보아 worker node가 갖는 iam role에 permission이 붙어있을 것으로 예상됨) 🌎 ALB를 사용해 서비스를 노출시키는 방법 😊 ALB는 K8s에 친숙하지 않으신 분들께는 다소 진입장벽이 있을 수 있습니다. 그냥 서비스를 노출시킬 때는 굳이 사용할 필요 없는 Ingress 라는 오브젝트도 관리해야하고, alb-ingress-contoller라는 녀석도 배포해야하며 설정이 다양하기 때문이죠! 💦\nK8s에서 EKS를 사용해 ALB를 이용하고싶은 경우 alb-ingress-controller을 배포한 뒤, Ingress를 통해 사용할 alb에 대한 rule을 설정을 해주어야합니다.\nhttps://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/controller/setup/ 의 내용을 버릴 부분이 하나도 없습니다. 위 링크를 통해 alb-ingress-controller에 대한 개념을 잡고 배포해봅니다. alb-ingress-controller.yaml의 인자를 적절히 수정해주어야합니다.\nALB가 아닌 k8s cluster 상에서 L7 LoadBalancer를 이용하는 경우에는 nginx ingress controller등을 이용하며 nginx 에 적용할 rule을 Ingress라는 K8s Object를 통해 설정합니다. ingress controller의 설정에서 자신의 class name을 적어주고, Ingress에서는 어떤 class name의 ingress controller에서 자신(Ingress이자 Rule)을 적용하도록 할 지를 annotation을 통해 설정하거나 ingressClassName이라는 필드를 통해 설정합니다.(ingress 설정에 대한 참고 - https://kubernetes.io/ko/docs/concepts/services-networking/ingress/#인그레스-클래스)\n이와 같은 경우에는 ingress controller가 직접 ingress에 명시된 rule을 이용했지만, alb-ingress-controller의 경우는 alb-ingress-controller가 nginx-ingress-controller처럼 직접 웹서버의 역할을 하는 것이 아닌, ingress에 명시된 rule을 이용하는 ALB를 생성하고 관리하는 역할을 한다는 것입니다. 이 부분이 처음에는 다소 헷갈리게 느껴질 수 있기에 길게 서술해보았습니다.\n실제로 ingress를 생성한 뒤 앞에서 배포한 alb-ingress-controller의 log를 보면 alb를 관리하기위한 여러 작업을 수행중인 모습을 볼 수 있습니다.\n그럼 이제 실제로 alb ingress controller을 통해 alb를 이용해보겠습니다.\nALB를 원활히 제어하기 위한 permission 부여 ALB iam 정책 참고 https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/alb-ingress.html https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.2/docs/examples/iam-policy.json ALB를 제어하기 위해서는 aws의 리소스에 대한 어떠한 permission이 필요합니다. node에 부여할 수도 있고, IAM User에 부여한 뒤 alb ingress controller의 설정에서 해당 IAM User의 Key를 부여할 수도 있고, Service account와 IAM Role을 OIDC(OpenID Connect를 이용해 Service account와 IAM Role을 연결시키는 작업)를 이용해 엮은 뒤, alb ingress controller pod에 해당 Service Account를 부여할 수도 있지만 뒤의 방법들은 좀 튜토리얼치고 투머치한 감이 있기때문에, 간단히 node에 Permission을 부여하도록하겠습니다.\niam.png worker node들이 이용하는 IAM Role에 Policy를 추가한 모습.\nalb가 사용할 subnet에 적절한 태그 달기 https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/alb-ingress.html 에 나와있듯이 ELB가 이용하는 서브넷을 자동으로 설정되도록 하기 위해서는 사용하고자 하는 서브넷에 아래와 같은 태그들을 달아주어야한다.\nkubernetes.io/cluster/\u0026lt;cluster-name\u0026gt; = shared | owned # Required kubernetes.io/role/internal-elb = 1 | \u0026#34;\u0026#34; # Optional, for internal alb kubernetes.io/role/elb = 1 | \u0026#34;\u0026#34; # Optional, for internet-facing alb a | b와 같은 표현은 a 나 b중 한 값을 가져야한다는 의미로 표현한 것입니다.\ntag.png internet facing ALB만 이용할 것이기 때문에 kubernetes.io/role/internal-elb tag는 생략하고 태그를 달아준 모습.\nsubnet에 ALB를 사용하기 위한 태그를 제대로 달아주지 않을 경우 alb-ingress-controller 에서 아래와 같은 로그를 보게 됩니다. ALB가 생성되지도 않습니다.\ncontroller.go:217] kubebuilder/controller \u0026#34;msg\u0026#34;=\u0026#34;Reconciler error\u0026#34; \u0026#34;error\u0026#34;=\u0026#34;failed to build LoadBalancer configuration due to failed to resolve 2 qualified subnet for ALB. Subnets must contains these tags: \u0026#39;kubernetes.io/cluster/umi-dev\u0026#39;: [\u0026#39;shared\u0026#39; or \u0026#39;owned\u0026#39;] and \u0026#39;kubernetes.io/role/internal-elb\u0026#39;: [\u0026#39;\u0026#39; or \u0026#39;1\u0026#39;] alb ingress controller 배포하기. 슬슬 읽기 귀찮아질 타이밍입니다. \u0026lsquo;요놈이 IAM policy도 만들고, 서브넷에 엄한 태그를 달더니 이제는 하,,, 뭘 또 배포하라고 하는구나 아이고 내 눈아,,,\u0026rsquo; 싶겠지만, 좀 더 힘을 내어봅시다 🍻\nhttps://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/controller/setup/ 를 참고하여 Deployment 내의 container의 args를 자신의 상황에 맞게 수정한 뒤 배포해줍니다.\n... args: - --ingress-class=alb # ingress의 annotation에 명시할 class name - --cluster-name=umi-dev # eks cluster name - --aws-region=ap-northeast-2 - --aws-api-debug=true 저는 위와 같은 식으로 설정해주었고, 잘 배포되었는지 확인해봅니다.\n$ kubectl get po -A | grep alb kube-system alb-ingress-controller-594f84b465-q4qjb 1/1 Running 0 106m 노출시킬 서비스 배포하기 간단하게 Nginx를 배포해보록하겠습니다.\napiVersion: v1 kind: Service metadata: name: ingress-test spec: selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80 nodePort: 30010 type: NodePort --- apiVersion: apps/v1 kind: Deployment metadata: name: ingress-test labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 ALB는 기본적으로 node의 Port를 AWS 상의 target group으로서 이용하기 때문에, ingress를 통해 노출시켜줄 서비스는 적어도 NodePort 타입으로 노출되어있어야 ALB ingress controller가 해당 서비스를 노출시킬 수 있습니다.(target type을 기본값인 instance가 아니라 IP로 설정하면, Pod의 IP로 트래픽이 흘러가게 할 수는 있습니다.)\n작동방식을 설명해보자면 ingress 는 service name과 service port를 설정으로 받습니다. alb-ingress-controller는 그러면 해당 service name, service port와 연결된 NodePort를 찾아서 ALB의 target group으로 등록시킵니다.\nIngress 배포하기 apiVersion: extensions/v1beta1 kind: Ingress metadata: name: \u0026#34;ingress\u0026#34; annotations: kubernetes.io/ingress.class: alb # the value we set in alb-ingress-controller alb.ingress.kubernetes.io/scheme: internet-facing spec: rules: - http: paths: - path: /* backend: serviceName: \u0026#34;ingress-test\u0026#34; servicePort: 80 https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/ingress/annotation/ 을 참고하여 Ingress를 작성해줍니다. [kubernetes.io/ingress.class는](http://kubernetes.io/ingress.class는) alb-ingress-controller에서 설정한 ingress.class를 적어주고, alb.ingress.kubernetes.io/scheme는 용도에 따라 internet-facing 혹은 internal을 적어줍니다. ingress 를 생성하기 전에 kubectl logs -f {alb-ingress-controller pod name}을 한 창에 띄워놓으면 ALB 생성 관련 로그를 쭈루룩 볼 수 있습니다.\nelb.png 잘 설정되었다면 위와 같이 ALB가 생성될 것 입니다.\n$ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE ingress * bf1e76be-default-ingress-e8c7-1351183883.ap-northeast-2.elb.amazonaws.com 80 30s 인증서 설정을 통해 HTTPS 까지?! ❤️ AWS Certificate Manager와 Ingress에 대한 annotation을 이용해 간단하게 HTTPS를 이용할 수도 있습니다! 직접하려면 도메인 소유 인증과 인증서, 비밀키 등을 모두 관리해야했는데 말이지요! 🐥\ncert.png 이런식으로 AWS의 Certificate Manager을 통해 발급받은 인증서가 있다면 이를 alb 에서 ingress에 annotation을 설정함으로써 사용할 수 있습니다.\nIngress의 annotation에 HTTPS및 인증서 관련 설정 추가해주기\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: \u0026#34;ingress\u0026#34; annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTP\u0026#34;: 80}, {\u0026#34;HTTPS\u0026#34;: 443}]\u0026#39; alb.ingress.kubernetes.io/actions.redirect-to-https: \u0026gt; {\u0026#34;Type\u0026#34;:\u0026#34;redirect\u0026#34;,\u0026#34;RedirectConfig\u0026#34;:{\u0026#34;Port\u0026#34;:\u0026#34;443\u0026#34;,\u0026#34;Protocol\u0026#34;:\u0026#34;HTTPS\u0026#34;,\u0026#34;StatusCode\u0026#34;:\u0026#34;HTTP_302\u0026#34;}} alb.ingress.kubernetes.io/certificate-arn: {Certificate Manger의 인증서 arn} ... http redirect 및 actions에 대한 내용은 저의 애교입니다. 궁금하신 분들은 한 번 적용해보시거나 알아보시면 어렵지 않게 알아내실 수 있을 겁니다! 😆\nRoute53에 ALB 추가해주기\nEKS에서 ALB를 사용하는 등의 작업을 하시는 분은 어느 정도 aws에 대한 이해가 있으리라 생각하고, ALB를 Route53을 통해 레코드로 추가하는 작업에 대한 설명은 생략하겠습니다.\ncert2.png alb-ingress-controller로 AWS Certificate Manager의 인증서까지 사용한 모습.\n🌎 NLB를 사용해 서비스를 노출시키는 방법 CLB는 Deprecate 대상이라고 들었기도 하고, 굳이 써본 적이 없어 NLB로만 설명합니다. NLB는 ALB에 비해 사용이 간단합니다.\nIngress와 alb-ingress-controller를 사용했던 ALB와 달리 NLB는 서비스를 직접 노출시킵니다. 주로 Nginx Ingress Controller을 NLB에 연결해서 사용했던 기억이 납니다. NLB는 L4 LB로, Nginx를 주로 L7 LB로 사용하는 경우 이렇게 NLB를 사용합니다. ALB와 Nginx 모두 L7 LB로서 역할을 하기때문에 굳이 ALB를 사용할 필요가 없는 경우가 많았습니다.\nNLB를 통해 서비스를 노출시키기 위해선 annotaion중에서도 [service.beta.kubernetes.io](http://service.beta.kubernetes.io/)...형태의 annotation을 이용합니다. 사실 실제로 실무해서 사용해본 annotation은 거의 [service.beta.kubernetes.io/aws-load-balancer-type:](http://service.beta.kubernetes.io/aws-load-balancer-type:) \u0026quot;nlb\u0026quot; 뿐입니다. (이를 사용하지 않을 경우 디폴트가 CLB이기 때문에\u0026hellip;)\nNLB가 사용할 subnet에 적절한 태그 달기 ALB를 사용했을 때와 마찬가지로 NLB가 사용할 서브넷에 필수 태그를 달아줍니다. 기억이 안난 다면 글의 상단 ALB 파트를 참고!\nNLB로 노출할 서비스 생성하기 apiVersion: v1 kind: Service metadata: name: nginx-nlb annotations: service.beta.kubernetes.io/aws-load-balancer-type: \u0026#34;nlb\u0026#34; spec: selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80 nodePort: 30011 type: LoadBalancer terminal.png zzal.png 간단히 type을 LoadBalancer로 바꾸어주고, annotation에 어떤 ELB를 사용할지( NLB/CLB )만 적어주면 아래 그림처럼 손쉽게 NLB로 서비스를 노출 시킬 수 있습니다.\n인증서 설정을 통해 HTTPS까지?! 🌋: \u0026ldquo;그만\u0026hellip;. 아무도 안 궁금해\u0026hellip;\u0026rdquo; - 하지만 마지막까지 힘을 내서 EKS에서의 ELB를 정복해봅시다!\napiVersion: v1 kind: Service metadata: name: nginx-nlb annotations: service.beta.kubernetes.io/aws-load-balancer-type: \u0026#34;nlb\u0026#34; service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp service.beta.kubernetes.io/aws-load-balancer-ssl-ports: \u0026#34;443\u0026#34; service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:ap-northeast-2:{{root}}:certificate/{{arn}} spec: selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80 nodePort: 30011 - protocol: TCP port: 443 targetPort: 80 nodePort: 30012 type: LoadBalancer backend-protocol은 tcp|tls 혹은 https|http로 설정이 돠는 듯합니다. 예를 들어 backend-protocol 로 tcp를 설정한 뒤 ssl-port로 443을 설정, 서비스의 spec에서의 포트로는 80과 443을 설정하면, 자동적으로 80은 tcp, 443은 tls를 이용하는 NLB listener로 설정되게 되는데 http,https도 마찬가지로 tcp,tls로 적절히 설정이 됩니다. 다만 http,https를 설정할 경우 X-Forwarded-For 헤더가 삽입된다고 합니다. (정확하지는 않아요\u0026hellip; 딱히 NLB의 backend protocol을 L7으로 설정하는 것이 NLB의 원래 스펙이 아니었던 점도 있고, L7을 이용하고 싶으면, ALB를 이용하는 것이 더 편하다고 생각이 들어서 따로 검증해본 적이 없기 때문에\u0026hellip; )\n⚠️단점이 하나 있다면 아직 http⇒https redirect가 불가능하다는 것인데, 이는 애초에 L4 LB를 이용하는 것과 L7 LB를 이용하는 쓰임에 대한 차이라고 생각을 하기 때문에 감수를 해야할 것 같습니다. 예를 들어 L4 NLB에 L7 nginx-ingress-controller을 연결하여 redirect는 nginx가 담당하도록하는 방식을 많이 이용하는 것 같습니다. NLB에서는 L4 의 뭔가 SSL/TLS한 작업을 하기 위함이고, L7의 https 작업이 주가 되는 것은 아니므로\u0026hellip;? 사실 이 부분은 잘 모르겠습니다\u0026hellip;💦💦 잘 아시는 분이 계시다면 알려주시면 감사하겠습니다. ㅜㅜㅜ( NLB에서 HTTP, HTTPS redirect가 안되는 이유 참고 - https://aws.amazon.com/premiumsupport/knowledge-center/redirect-http-https-elb/ )\n⭕️ NLB를 통한 HTTPS 서버 구축 결과 어쨌든 위의 annotation을 통해 올바르게 svc를 설정한다면\n$ kubectl get svc nginx-nlb LoadBalancer 10.100.180.174 ae27784521c4f4bcd96b22f2cca2358b-4bffb166fafa47f2.elb.ap-northeast-2.amazonaws.com 443:30014/TCP 37m Route53 설정은 추가로 해주어야함. - 예시 nlb.umidev.net - ALIAS ae2778452xxxxxxxxxxxxxx.elb.ap-northeast-2.amazonaws.com 이렇게 service가 생성될 것이고, (service의 port로서 사용할 80,443 등은 service.spec에서 명시) 생성 후 A Record Alias로서 NLB의 DNS name을 넣어주면 사진과 같이 HTTPS 접속이 가능합니다!\ncert3.png 🐳 마치며 어차피 한 번의 검색이면 정보를 얻을 수 있는 모든 annotation이나 기타 설정에 대한 내용을 다루기 보단 나름 제가 실제로 쿠버네티스를 관리하는 데브옵스 인턴로서 일을 하면서 헷갈렸던 내용과 EKS에서의 ELB 관리에 대한 흐름을 위주로 설명하려 노력했고, 저의 삽질이 깃든 내용들입니다 ㅎㅎㅎ\n아는 범위 + 좀 더 조사하여 열심히 정리해보았지만, 부족한 부분이 있을 수도 있고 틀린 부분이 있을 수도 있을텐데, 보완해주실 내용이 있다면 말씀해주시면 열심히 검토해보겠습니다~! 감사합니다.\n🧙‍♂️글쓴이 박진수 - 👨‍👩‍👧‍👧AUSG (AWS University Student Group) 3기로 활동 중 관심사 Docker, Kubernetes 등의 컨테이너 기술 Argo, Spinnaker, Github action 등의 CI/CD 툴 Terraform, AWS를 통한 클라우드 인프라 구축 Blog - https://senticoding.tistory.com Email - bo314@naver.com 📚 참고 (References) EKS의 Required subnet tags https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/load-balancing.html https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/alb-ingress.html Kubernetes Cloud provider aws - https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#aws ALB ingress controller install - https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/controller/setup/ ALB ingress Annotation https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/ingress/annotation/ NLB의 HTTPS redirect가 불가능한 이유 -https://aws.amazon.com/premiumsupport/knowledge-center/redirect-http-https-elb/ NLB service Annotations - https://kubernetes.io/ko/docs/concepts/services-networking/service/#aws-nlb-support Ingress의 class에 대해 - https://kubernetes.io/ko/docs/concepts/services-networking/ingress/#인그레스-클래스 ","date":"2020-09-06T19:11:07+09:00","image":"https://umi0410.github.io/blog/aws/aws_eks_elb/preview_huc38b74e19f60c2848e88837c4c6920fa_1709468_120x120_fill_box_smart1_3.png","permalink":"https://umi0410.github.io/blog/aws/aws_eks_elb/","title":"EKS K8s에서 ELB(ALB, NLB) 제대로 설정하며 사용하기"}]