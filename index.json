[{"uri":"https://umi0410.github.io/experiences/megazone-cloud/","title":"메가존 클라우드 데브옵스 인턴 후","tags":[],"description":"안녕하세요. 이번에 메가존 클라우드에서 데브옵스 인턴으로 근무를 하게되었던 박진수입니다.
너무 좋은 팀원들과 많은 경험을 하며 단시간에 성장할 수 있었고, 일하는 동안 매 순간 순간이 너무
행복했었기에 이렇게 인턴 후기를 작성해봅니다.
","content":" Table of Contents  지원동기 Megazone Cloud: CloudOne Team? 진행했던 업무들 느낀 점     안녕하세요. 이번에 메가존 클라우드에서 데브옵스 인턴으로 근무를 하게되었던 박진수입니다. 너무 좋은 팀원들과 많은 경험을 하며 단시간에 성장할 수 있었고, 일하는 동안 매 순간 순간이 너무 행복했었기에 이렇게 인턴 후기를 작성해봅니다. 한 학기를 쉬고 2020.04.13~2020.08.31 까지 인턴으로 근무 했고, 다시 2020-2학기부터는 학교 복학을 하게되었습니다. 기술적인 얘기는 이곳 저곳에 많으니 항상 제가 가장 중요시하는 \u0026ldquo;느낀 점\u0026rdquo; 과 \u0026ldquo;배운 점\u0026rdquo; 을 위주로 적어보겠습니다!! 글은 위의 목록의 순서로 진행해보겠습니다.\n지원동기 저는 AWS와 배포에 관해 흥미가 있어 종종 AWSKRUG 라는 그룹의 소모임에 가서 핸즈온이나 세미나에 참여하곤 했는데요. 그곳에서 저희 팀원들을 만나게 되었고, 그것이 인연이 되어 채용으로까지 이어질 수 있었습니다.\n평소 AUSG 라는 대학생 AWS 사용자 모임의 일원으로서 참여하며 AWS 를 이용한 클라우드 인프라, CI/CD 파이프라인 구축을 통한 자동화, 컨테이너 등에 관심이 많았는데, 마침 메가존 클라우드의 저희 CloudOne 팀에서는 팀의 모든 마이크로서비스 및 기타 서비스들을 EKS라는 AWS의 Managed Kubernetes Cluster Service 위에 자동화시켜 배포를 하고있었고, 개발 인프라를 이전하려던 참이었기에 저의 관심사와 향후 목적에 잘 부합했습니다.\nMegazone Cloud: CloudOne Team?  Megazone Cloud: 국내 최초 \u0026amp; 최대 AWS 프리미어 컨설팅 파트너, AWS 컨설팅, 구축, 운영 및 빌링 서비스 제공. 메가존클라우드는 2009년부터 클라우드를 차세대 핵심 사업으로 성장시키며 ‘클라우드 이노베이터(Cloud Innovator)’로서 고객님들의 클라우드 전환의 과정마다 최선의 선택을 하실 수 있도록 다양한 서비스를 제공하고 있습니다. - 출처(https://www.megazone.com/)\n SpaceONE Preview\n메가존 클라우드에 대한 설명은 위와 같고, 제가 근무했던 CloudOne Team에 대해 간단히 소개해보겠습니다. CloudOne 팀은 SpaceONE 이라는 Multi Cloud 에 대한 오픈소스 CMP(Cloud Management Platform)를 개발하는 팀입니다. AWS, Azure, GCP, IDC에 대한 다양한 관리, 모니터링을 지원하는 것이 목표이고, 빠르게 발전되어나가고있습니다! 주요 내용들은 아래와 같은 키워드를 갖고 있습니다.  AWS EKS를 이용한 Kubernetes 환경, 마이크로서비스 아키텍쳐 Composition API를 이용하는 최신 Vue 기술 Terraform을 통한 IaC 자동화된 CI/CD, 다양한 배포 전략, 인프라 모니터링 HTTP2를 이용한 gRPC API  진행했던 업무들  링크 형식이므로 링크를 눌러 읽어주시면 됩니다.\n  2020.04: Stargate 라는 개발 인프라를 구축  terraform 을 통한 개발 인프라 구축 EKS 클러스터에 jenkins, spinnaker, grafana 등등의 개발 도구들을 배포함 종종 발생하는 장애 상황을 멘토님과 트러블슈팅함   2020.05: 개발 CI/CD 파이프라인 구축  Experiment 환경에 배포 =\u0026gt; Test, CI =\u0026gt; Development 환경에 배포를 자동화 Spinnaker와 Jenkins, Github Action을 이용한 자동화 파이프라인 구축   2020.06: Argo Project들을 PoC  argo cd, argo(workflow), argo-event 등의 다양한 프로젝트에 대한 PoC를 진행   2020.07: spaceone-helm Helm3 Chart 개발  개별로 배포되고 운영되던 우리 팀의 서비스인 SpaceONE을 패키지로 배포할 수 있게해주는 Helm Chart를 개발   2020.08: spacectl 설계, 개발 참여  우리 팀의 서비스인 SpaceONE에 대한 API 작업을 수행하는 CLI 도구에 대한 설계와 개발에 참여함.    느낀 점 그 동안 혼자 개발 및 평소 관심분야였던 클라우드, 컨테이너 등등의 주제로 공부해왔었는데, 과연 이 내용들이 정말 실무에 도움이 될 지, 제가 잘 나아가고 있는 건지 확신이 들지 않았습니다. 하지만 CloudOne팀에서 다양한 경험을 하면서 \u0026lsquo;제가 공부해온 길이 틀리지만은 않았구나\u0026rsquo;라는 느낌을 받을 수 있었고, 보완해야할 부분들은 보완하면서 불확실한 마음이 아닌 확신과 열정을 가진 자세로 좀 더 몰두할 수 있을 것 같습니다!\n배우고 느낀 내용이 너무 많아 글이 다소 길어졌습니다. 기술을 거부감 없이 접하되 기술이 다가 아닌, 팀원들을 위해 솔선수범하는 개발자, 팀원들의 생각을 읽어줄 수 있고, 잘 이해해줄 수 있는 개발자가 되기 위해 노력해야겠단 생각이 듭니다. 쉽진 않겠지만, 나아가고 싶은 방향은 정해진 것 같아 다행입니다!\n"},{"uri":"https://umi0410.github.io/","title":"Portfolio","tags":[],"description":"","content":"😄 소개  이름: 박진수 학력: 경희대학교 컴퓨터공학과 재학 관심사  깔끔한 백엔드 구조 및 설계 컨테이너 오케스트레이션 클라우드 컴퓨팅 Golang 오픈소스   자기소개  언제 어디서는 배울 자세를 가진, 느끼고 판단할 줄 아는 개발자가 되고싶습니다.\n컨테이너와 자동화, 깔끔한 아키텍쳐 설계 및 구축에 관심이 많습니다.\n   🥅 활동 내용  [2020.04~2020.08] Megazone Cloud DevOps 인턴 [2020.08~2020.09] 오픈스택 컨트리뷰톤 [2020.01~2020.03] 위시오피스 AWS 서버 관리자 [2019.06~] AUSG 3기 [2019.11] 경희대학교 소프트웨어 페스티발 웹/앱 최우수상 수상  To be updated soon\u0026hellip;\u0026hellip;..\n "},{"uri":"https://umi0410.github.io/portfolio/","title":"Portfolio","tags":[],"description":"","content":"😄 소개  이름: 박진수 학력: 경희대학교 컴퓨터공학과 재학 관심사  깔끔한 백엔드 구조 및 설계 컨테이너 오케스트레이션 클라우드 컴퓨팅 Golang 오픈소스   자기소개  언제 어디서는 배울 자세를 가진, 느끼고 판단할 줄 아는 개발자가 되고싶습니다.\n컨테이너와 자동화, 깔끔한 아키텍쳐 설계 및 구축에 관심이 많습니다.\n   🥅 활동 내용  [2020.04~2020.08] Megazone Cloud DevOps 인턴 [2020.08~2020.09] 오픈스택 컨트리뷰톤 [2020.01~2020.03] 위시오피스 AWS 서버 관리자 [2019.06~] AUSG 3기 [2019.11] 경희대학교 소프트웨어 페스티발 웹/앱 최우수상 수상  To be updated soon\u0026hellip;\u0026hellip;..\n "},{"uri":"https://umi0410.github.io/experiences/open-source/","title":"Open Source 기여 활동","tags":[],"description":" ","content":"Open Source 기여 활동 내용.\n"},{"uri":"https://umi0410.github.io/experiences/","title":"Experiences","tags":[],"description":"","content":"주요 활동 내역입니다.  메가존 클라우드 데브옵스 인턴 후  안녕하세요. 이번에 메가존 클라우드에서 데브옵스 인턴으로 근무를 하게되었던 박진수입니다. 너무 좋은 팀원들과 많은 경험을 하며 단시간에 성장할 수 있었고, 일하는 동안 매 순간 순간이 너무 행복했었기에 이렇게 인턴 후기를 작성해봅니다.  Open Source 기여 활동    Kyunghee Univ  경희대학교에서 참여했던 활동들....\n "},{"uri":"https://umi0410.github.io/blog/","title":"Blog","tags":[],"description":"","content":"자유로운 주제로 작성한 블로그 게시물들입니다. Github.io  Category: AWS  EKS K8s에서 ELB(ALB, NLB) 제대로 설정하며 사용하기 🐶 시작하며 본 게시글은 AWS 대학생 유저그룹인 AUSG의 활동 중 하나로서 본인(박진수)이 작성한 게시물을 포워딩한 것입니다. 데브옵스 인턴으로 근무한 지가 벌써 두 달이 되어갑니다. 이것 저것 배운 것이 많았던 시간이었는데, 그 중 꽤나 삽질을 했던 Kubernetes 와 ELB를 이용하는 부분에 대해 정리를 해볼까합니다. jenkins, spinnaker, argo, terraform, ansible, github action, \u0026hellip; 등등 다양한 내용을 경험할 수 있던 시간이었지만, 그 중 kubernetes에서 무슨 작업을 하던 빼놓을 수 없으면서 어딘가 깔끔히 그 흐름이 정리된 곳을 보기 힘들었던 service를 ELB에 연결하기에 대한 내용을 정리해보겠습니다.\n 그 외의 블로그 플랫폼 "},{"uri":"https://umi0410.github.io/experiences/kyunghee-univ/","title":"Kyunghee Univ","tags":[],"description":"경희대학교에서 참여했던 활동들....","content":" 경희대학교에서 참여했던 활동들\u0026hellip;.\n Head 1 foo\nHead 1 inner inner bar\nHead 2 foobar\n"},{"uri":"https://umi0410.github.io/blog/aws/aws_eks_elb/","title":"EKS K8s에서 ELB(ALB, NLB) 제대로 설정하며 사용하기","tags":[],"description":"","content":"🐶 시작하며  본 게시글은 AWS 대학생 유저그룹인 AUSG의 활동 중 하나로서 본인(박진수)이 작성한 게시물을 포워딩한 것입니다.\n 데브옵스 인턴으로 근무한 지가 벌써 두 달이 되어갑니다. 이것 저것 배운 것이 많았던 시간이었는데, 그 중 꽤나 삽질을 했던 Kubernetes 와 ELB를 이용하는 부분에 대해 정리를 해볼까합니다. jenkins, spinnaker, argo, terraform, ansible, github action, \u0026hellip; 등등 다양한 내용을 경험할 수 있던 시간이었지만, 그 중 kubernetes에서 무슨 작업을 하던 빼놓을 수 없으면서 어딘가 깔끔히 그 흐름이 정리된 곳을 보기 힘들었던 service를 ELB에 연결하기에 대한 내용을 정리해보겠습니다.\n본 포스트는 EKS를 통해 K8s를 이용할 때를 기준으로 설명합니다.\n💁🏻‍♂️ EKS 에서 ELB를 사용해 서비스를 노출킬 때 마음에 새길  🧐 : \u0026quot; ELB, NLB, ALB 대체 뭐가 다른 거야..?ㅜㅜ 쿠버네티스를 쓸 때는 어떻게 얘네를 지정하는 거지..? kubectl expose deploy {{deployment_name}} --type=LoadBalancer 하면 그냥 작동은 하던데\u0026hellip;\u0026rdquo;\n EKS에서 주로 사용하는 ELB는 L4의 NLB와 L7의 ALB 입니다. ALB가 L7에 대한 좀 더 다양한 설정이 가능하기 때문에 조건이 많기도 하고, AWS의 ALB만을 위한 alb-ingress-controller라는 녀석이 직접 Ingress의 설정들을 관리해주기 때문에 설정할 수 있는 옵션도 많습니다. 좋게 보면 많은 설정을 할 수 있고, 나쁘게 보면 초보자에겐 귀찮을 수 있습니다. NLB는 비교적 설정이 적고 따라서 설정해줄 수 있는 항목도 적습니다.\n쿠버네티스에서 다양한 작업을 하면서 다양한 controller을 접하게 되고, 그렇게 될 수록 annotation으로 많은 설정을 하게 됩니다. k8s를 처음 접할 때에는 annotation에 대한 정의로서 아래와 같은 문장을 접할 수 있고, 마치 기능과 크게 상관이 없을 것처럼 느껴지기도 하지만 사실 EKS를 비롯한 여러 서비스에서는 annotation을 이용해 중요한 설정 등을 기입할 수 있기 때문에 잘 설정해주어야합니다. ELB또한 모든 설정이 annotation으로 동작한다.\n \u0026quot; Label을 사용하여 오브젝트를 선택하고, 특정 조건을 만족하는 오브젝트 컬렉션을 찾을 수 있다. 반면에, annotation은 오브젝트를 식별하고 선택하는데 사용되지 않는다. 어노테이션의 메타데이터는 작거나 크고, 구조적이거나 구조적이지 않을 수 있으며, 레이블에서 허용되지 않는 문자를 포함할 수 있다.\u0026rdquo;\n ⚠️ ALB를 사용할 때 마음에 새길 점  어떤 옵션들이 있고, 기본적으로는 어떻게 설정되는 지에 대한 이해가 있어야 오류 과정을 추적하기 쉬우므로 기본적으로 ALB를 AWS Console에서 사용해본 뒤에 설정할 것을 추천합니다.\n  alb ingress controller가 생성할 ALB가 사용할 서브넷을 discover하기 위해서는 올바른 태그가 달린 subnet이 존재해야한다. node 혹은 alb ingress controller에 연결된 service account가 alb를 제어하기 위한 iam permission이 부여되어야한다. internet facing한 alb를 만들지 internal한 alb를 만들지 고민해봐야한다. alb ingress controller의 log를 통해 작업에 대한 log를 볼 수 있다.  ⚠️ NLB, CLB를 사용할 때 마음에 새길 점 https://kubernetes.io/ko/docs/concepts/services-networking/service/#aws-nlb-support\nhttps://docs.aws.amazon.com/ko_kr/eks/latest/userguide/load-balancing.html\n NLB, CLB가 사용할 서브넷을 설정하기 위해서는 올바른 태그가 달린 subnet이 존재해야한다. 어느 부분에선가 NLB, CLB를 제어하기 위한 iam permission이 부여되어야한다. (어느 부분인지 확실히는 모르겠음. 따로 설정안해도 동작하는 것을 보아 worker node가 갖는 iam role에 permission이 붙어있을 것으로 예상됨)  🌎 ALB를 사용해 서비스를 노출시키는 방법  😊 ALB는 K8s에 친숙하지 않으신 분들께는 다소 진입장벽이 있을 수 있습니다. 그냥 서비스를 노출시킬 때는 굳이 사용할 필요 없는 Ingress 라는 오브젝트도 관리해야하고, alb-ingress-contoller라는 녀석도 배포해야하며 설정이 다양하기 때문이죠! 💦\n K8s에서 EKS를 사용해 ALB를 이용하고싶은 경우 alb-ingress-controller을 배포한 뒤, Ingress를 통해 사용할 alb에 대한 rule을 설정을 해주어야합니다.\nhttps://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/controller/setup/ 의 내용을 버릴 부분이 하나도 없습니다. 위 링크를 통해 alb-ingress-controller에 대한 개념을 잡고 배포해봅니다. alb-ingress-controller.yaml의 인자를 적절히 수정해주어야합니다.\nALB가 아닌 k8s cluster 상에서 L7 LoadBalancer를 이용하는 경우에는 nginx ingress controller등을 이용하며 nginx 에 적용할 rule을 Ingress라는 K8s Object를 통해 설정합니다. ingress controller의 설정에서 자신의 class name을 적어주고, Ingress에서는 어떤 class name의 ingress controller에서 자신(Ingress이자 Rule)을 적용하도록 할 지를 annotation을 통해 설정하거나 ingressClassName이라는 필드를 통해 설정합니다.(ingress 설정에 대한 참고 - https://kubernetes.io/ko/docs/concepts/services-networking/ingress/#인그레스-클래스)\n이와 같은 경우에는 ingress controller가 직접 ingress에 명시된 rule을 이용했지만, alb-ingress-controller의 경우는 alb-ingress-controller가 nginx-ingress-controller처럼 직접 웹서버의 역할을 하는 것이 아닌, ingress에 명시된 rule을 이용하는 ALB를 생성하고 관리하는 역할을 한다는 것입니다. 이 부분이 처음에는 다소 헷갈리게 느껴질 수 있기에 길게 서술해보았습니다.\n실제로 ingress를 생성한 뒤 앞에서 배포한 alb-ingress-controller의 log를 보면 alb를 관리하기위한 여러 작업을 수행중인 모습을 볼 수 있습니다.\n그럼 이제 실제로 alb ingress controller을 통해 alb를 이용해보겠습니다.\nALB를 원활히 제어하기 위한 permission 부여  ALB iam 정책 참고  https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/alb-ingress.html https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.2/docs/examples/iam-policy.json    ALB를 제어하기 위해서는 aws의 리소스에 대한 어떠한 permission이 필요합니다. node에 부여할 수도 있고, IAM User에 부여한 뒤 alb ingress controller의 설정에서 해당 IAM User의 Key를 부여할 수도 있고, Service account와 IAM Role을 OIDC(OpenID Connect를 이용해 Service account와 IAM Role을 연결시키는 작업)를 이용해 엮은 뒤, alb ingress controller pod에 해당 Service Account를 부여할 수도 있지만 뒤의 방법들은 좀 튜토리얼치고 투머치한 감이 있기때문에, 간단히 node에 Permission을 부여하도록하겠습니다.\nworker node들이 이용하는 IAM Role에 Policy를 추가한 모습.\nalb가 사용할 subnet에 적절한 태그 달기 https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/alb-ingress.html 에 나와있듯이 ELB가 이용하는 서브넷을 자동으로 설정되도록 하기 위해서는 사용하고자 하는 서브넷에 아래와 같은 태그들을 달아주어야한다.\nkubernetes.io/cluster/\u0026lt;cluster-name\u0026gt; = shared | owned # Required kubernetes.io/role/internal-elb = 1 | \u0026#34;\u0026#34; # Optional, for internal alb kubernetes.io/role/elb = 1 | \u0026#34;\u0026#34; # Optional, for internet-facing alb a | b와 같은 표현은 a 나 b중 한 값을 가져야한다는 의미로 표현한 것입니다.\ninternet facing ALB만 이용할 것이기 때문에 kubernetes.io/role/internal-elb tag는 생략하고 태그를 달아준 모습.\nsubnet에 ALB를 사용하기 위한 태그를 제대로 달아주지 않을 경우 alb-ingress-controller 에서 아래와 같은 로그를 보게 됩니다. ALB가 생성되지도 않습니다.\ncontroller.go:217] kubebuilder/controller \u0026#34;msg\u0026#34;=\u0026#34;Reconciler error\u0026#34; \u0026#34;error\u0026#34;=\u0026#34;failed to build LoadBalancer configuration due to failed to resolve 2 qualified subnet for ALB. Subnets must contains these tags: \u0026#39;kubernetes.io/cluster/umi-dev\u0026#39;: [\u0026#39;shared\u0026#39; or \u0026#39;owned\u0026#39;] and \u0026#39;kubernetes.io/role/internal-elb\u0026#39;: [\u0026#39;\u0026#39; or \u0026#39;1\u0026#39;] alb ingress controller 배포하기.  슬슬 읽기 귀찮아질 타이밍입니다. \u0026lsquo;요놈이 IAM policy도 만들고, 서브넷에 엄한 태그를 달더니 이제는 하,,, 뭘 또 배포하라고 하는구나 아이고 내 눈아,,,\u0026rsquo; 싶겠지만, 좀 더 힘을 내어봅시다 🍻\n https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/controller/setup/ 를 참고하여 Deployment 내의 container의 args를 자신의 상황에 맞게 수정한 뒤 배포해줍니다.\n... args: - --ingress-class=alb # ingress의 annotation에 명시할 class name - --cluster-name=umi-dev # eks cluster name - --aws-region=ap-northeast-2 - --aws-api-debug=true 저는 위와 같은 식으로 설정해주었고, 잘 배포되었는지 확인해봅니다.\n$ kubectl get po -A | grep alb kube-system alb-ingress-controller-594f84b465-q4qjb 1/1 Running 0 106m 노출시킬 서비스 배포하기 간단하게 Nginx를 배포해보록하겠습니다.\napiVersion: v1 kind: Service metadata: name: ingress-test spec: selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80 nodePort: 30010 type: NodePort --- apiVersion: apps/v1 kind: Deployment metadata: name: ingress-test labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 ALB는 기본적으로 node의 Port를 AWS 상의 target group으로서 이용하기 때문에, ingress를 통해 노출시켜줄 서비스는 적어도 NodePort 타입으로 노출되어있어야 ALB ingress controller가 해당 서비스를 노출시킬 수 있습니다.(target type을 기본값인 instance가 아니라 IP로 설정하면, Pod의 IP로 트래픽이 흘러가게 할 수는 있습니다.)\n작동방식을 설명해보자면 ingress 는 service name과 service port를 설정으로 받습니다. alb-ingress-controller는 그러면 해당 service name, service port와 연결된 NodePort를 찾아서 ALB의 target group으로 등록시킵니다.\nIngress 배포하기 apiVersion: extensions/v1beta1 kind: Ingress metadata: name: \u0026#34;ingress\u0026#34; annotations: kubernetes.io/ingress.class: alb # the value we set in alb-ingress-controller alb.ingress.kubernetes.io/scheme: internet-facing spec: rules: - http: paths: - path: /* backend: serviceName: \u0026#34;ingress-test\u0026#34; servicePort: 80 https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/ingress/annotation/ 을 참고하여 Ingress를 작성해줍니다. [kubernetes.io/ingress.class는](http://kubernetes.io/ingress.class는) alb-ingress-controller에서 설정한 ingress.class를 적어주고, alb.ingress.kubernetes.io/scheme는 용도에 따라 internet-facing 혹은 internal을 적어줍니다. ingress 를 생성하기 전에 kubectl logs -f {alb-ingress-controller pod name}을 한 창에 띄워놓으면 ALB 생성 관련 로그를 쭈루룩 볼 수 있습니다.\n잘 설정되었다면 위와 같이 ALB가 생성될 것 입니다.\n$ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE ingress * bf1e76be-default-ingress-e8c7-1351183883.ap-northeast-2.elb.amazonaws.com 80 30s 인증서 설정을 통해 HTTPS 까지?!  ❤️ AWS Certificate Manager와 Ingress에 대한 annotation을 이용해 간단하게 HTTPS를 이용할 수도 있습니다! 직접하려면 도메인 소유 인증과 인증서, 비밀키 등을 모두 관리해야했는데 말이지요! 🐥\n 이런식으로 AWS의 Certificate Manager을 통해 발급받은 인증서가 있다면 이를 alb 에서 ingress에 annotation을 설정함으로써 사용할 수 있습니다.\nIngress의 annotation에 HTTPS및 인증서 관련 설정 추가해주기\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: \u0026#34;ingress\u0026#34; annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTP\u0026#34;: 80}, {\u0026#34;HTTPS\u0026#34;: 443}]\u0026#39; alb.ingress.kubernetes.io/actions.redirect-to-https: \u0026gt; {\u0026#34;Type\u0026#34;:\u0026#34;redirect\u0026#34;,\u0026#34;RedirectConfig\u0026#34;:{\u0026#34;Port\u0026#34;:\u0026#34;443\u0026#34;,\u0026#34;Protocol\u0026#34;:\u0026#34;HTTPS\u0026#34;,\u0026#34;StatusCode\u0026#34;:\u0026#34;HTTP_302\u0026#34;}} alb.ingress.kubernetes.io/certificate-arn: {Certificate Manger의 인증서 arn} ... http redirect 및 actions에 대한 내용은 저의 애교입니다. 궁금하신 분들은 한 번 적용해보시거나 알아보시면 어렵지 않게 알아내실 수 있을 겁니다! 😆\nRoute53에 ALB 추가해주기\nEKS에서 ALB를 사용하는 등의 작업을 하시는 분은 어느 정도 aws에 대한 이해가 있으리라 생각하고, ALB를 Route53을 통해 레코드로 추가하는 작업에 대한 설명은 생략하겠습니다.\nalb-ingress-controller로 AWS Certificate Manager의 인증서까지 사용한 모습.\n🌎 NLB를 사용해 서비스를 노출시키는 방법 CLB는 Deprecate 대상이라고 들었기도 하고, 굳이 써본 적이 없어 NLB로만 설명합니다. NLB는 ALB에 비해 사용이 간단합니다.\nIngress와 alb-ingress-controller를 사용했던 ALB와 달리 NLB는 서비스를 직접 노출시킵니다. 주로 Nginx Ingress Controller을 NLB에 연결해서 사용했던 기억이 납니다. NLB는 L4 LB로, Nginx를 주로 L7 LB로 사용하는 경우 이렇게 NLB를 사용합니다. ALB와 Nginx 모두 L7 LB로서 역할을 하기때문에 굳이 ALB를 사용할 필요가 없는 경우가 많았습니다.\nNLB를 통해 서비스를 노출시키기 위해선 annotaion중에서도 [service.beta.kubernetes.io](http://service.beta.kubernetes.io/)...형태의 annotation을 이용합니다. 사실 실제로 실무해서 사용해본 annotation은 거의 [service.beta.kubernetes.io/aws-load-balancer-type:](http://service.beta.kubernetes.io/aws-load-balancer-type:) \u0026quot;nlb\u0026quot; 뿐입니다. (이를 사용하지 않을 경우 디폴트가 CLB이기 때문에\u0026hellip;)\nNLB가 사용할 subnet에 적절한 태그 달기 ALB를 사용했을 때와 마찬가지로 NLB가 사용할 서브넷에 필수 태그를 달아줍니다. 기억이 안난 다면 글의 상단 ALB 파트를 참고!\nNLB로 노출할 서비스 생성하기 apiVersion: v1 kind: Service metadata: name: nginx-nlb annotations: service.beta.kubernetes.io/aws-load-balancer-type: \u0026#34;nlb\u0026#34; spec: selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80 nodePort: 30011 type: LoadBalancer 간단히 type을 LoadBalancer로 바꾸어주고, annotation에 어떤 ELB를 사용할지( NLB/CLB )만 적어주면 아래 그림처럼 손쉽게 NLB로 서비스를 노출 시킬 수 있습니다.\n인증서 설정을 통해 HTTPS까지?!  🌋: \u0026ldquo;그만\u0026hellip;. 아무도 안 궁금해\u0026hellip;\u0026rdquo; - 하지만 마지막까지 힘을 내서 EKS에서의 ELB를 정복해봅시다!\n apiVersion: v1 kind: Service metadata: name: nginx-nlb annotations: service.beta.kubernetes.io/aws-load-balancer-type: \u0026#34;nlb\u0026#34; service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp service.beta.kubernetes.io/aws-load-balancer-ssl-ports: \u0026#34;443\u0026#34; service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:ap-northeast-2:{{root}}:certificate/{{arn}} spec: selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80 nodePort: 30011 - protocol: TCP port: 443 targetPort: 80 nodePort: 30012 type: LoadBalancer backend-protocol은 tcp|tls 혹은 https|http로 설정이 돠는 듯합니다. 예를 들어 backend-protocol 로 tcp를 설정한 뒤 ssl-port로 443을 설정, 서비스의 spec에서의 포트로는 80과 443을 설정하면, 자동적으로 80은 tcp, 443은 tls를 이용하는 NLB listener로 설정되게 되는데 http,https도 마찬가지로 tcp,tls로 적절히 설정이 됩니다. 다만 http,https를 설정할 경우 X-Forwarded-For 헤더가 삽입된다고 합니다. (정확하지는 않아요\u0026hellip; 딱히 NLB의 backend protocol을 L7으로 설정하는 것이 NLB의 원래 스펙이 아니었던 점도 있고, L7을 이용하고 싶으면, ALB를 이용하는 것이 더 편하다고 생각이 들어서 따로 검증해본 적이 없기 때문에\u0026hellip; )\n⚠️단점이 하나 있다면 아직 http⇒https redirect가 불가능하다는 것인데, 이는 애초에 L4 LB를 이용하는 것과 L7 LB를 이용하는 쓰임에 대한 차이라고 생각을 하기 때문에 감수를 해야할 것 같습니다. 예를 들어 L4 NLB에 L7 nginx-ingress-controller을 연결하여 redirect는 nginx가 담당하도록하는 방식을 많이 이용하는 것 같습니다. NLB에서는 L4 의 뭔가 SSL/TLS한 작업을 하기 위함이고, L7의 https 작업이 주가 되는 것은 아니므로\u0026hellip;? 사실 이 부분은 잘 모르겠습니다\u0026hellip;💦💦 잘 아시는 분이 계시다면 알려주시면 감사하겠습니다. ㅜㅜㅜ( NLB에서 HTTP, HTTPS redirect가 안되는 이유 참고 - https://aws.amazon.com/premiumsupport/knowledge-center/redirect-http-https-elb/ )\n⭕️ NLB를 통한 HTTPS 서버 구축 결과 어쨌든 위의 annotation을 통해 올바르게 svc를 설정한다면\n$ kubectl get svc nginx-nlb LoadBalancer 10.100.180.174 ae27784521c4f4bcd96b22f2cca2358b-4bffb166fafa47f2.elb.ap-northeast-2.amazonaws.com 443:30014/TCP 37m  Route53 설정은 추가로 해주어야함. - 예시 nlb.umidev.net - ALIAS ae2778452xxxxxxxxxxxxxx.elb.ap-northeast-2.amazonaws.com  이렇게 service가 생성될 것이고, (service의 port로서 사용할 80,443 등은 service.spec에서 명시) 생성 후 A Record Alias로서 NLB의 DNS name을 넣어주면 사진과 같이 HTTPS 접속이 가능합니다!\n#점# 🐳 마치며\n어차피 한 번의 검색이면 정보를 얻을 수 있는 모든 annotation이나 기타 설정에 대한 내용을 다루기 보단 나름 제가 실제로 쿠버네티스를 관리하는 데브옵스 인턴로서 일을 하면서 헷갈렸던 내용과 EKS에서의 ELB 관리에 대한 흐름을 위주로 설명하려 노력했고, 저의 삽질이 깃든 내용들입니다 ㅎㅎㅎ\n아는 범위 + 좀 더 조사하여 열심히 정리해보았지만, 부족한 부분이 있을 수도 있고 틀린 부분이 있을 수도 있을텐데, 보완해주실 내용이 있다면 말씀해주시면 열심히 검토해보겠습니다~! 감사합니다.\n🧙‍♂️글쓴이  박진수 - 👨‍👩‍👧‍👧AUSG (AWS University Student Group) 3기로 활동 중 관심사  Docker, Kubernetes 등의 컨테이너 기술 Argo, Spinnaker, Github action 등의 CI/CD 툴 Terraform, AWS를 통한 클라우드 인프라 구축   Blog - https://senticoding.tistory.com Email - bo314@naver.com  📚 참고 (References)  EKS의 Required subnet tags  https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/load-balancing.html https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/alb-ingress.html   Kubernetes Cloud provider aws - https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#aws ALB ingress controller install - https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/controller/setup/ ALB ingress Annotation https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/ingress/annotation/ NLB의 HTTPS redirect가 불가능한 이유 -https://aws.amazon.com/premiumsupport/knowledge-center/redirect-http-https-elb/ NLB service Annotations - https://kubernetes.io/ko/docs/concepts/services-networking/service/#aws-nlb-support Ingress의 class에 대해 - https://kubernetes.io/ko/docs/concepts/services-networking/ingress/#인그레스-클래스  "},{"uri":"https://umi0410.github.io/experiences/megazone-cloud/stargate-infra/","title":"Stargate라는 인프라 구축기","tags":[],"description":"저희 팀의 개발 인프라는 위와 같습니다. 마침 제가 입사할 쯤이 기존에 존재하던 인프라를 새로운 환경으로
이전해야할 시점이었습니다. 덕분에 저는 저희 개발 환경을 처음부터 구축하고, 우리의 서비스를
배포해보고, 이후 운영하면서 여러 경험들을 할 수 있었습니다.
","content":" Table of Contents  Stargate라는 개발 인프라 구축기  Terraform으로 처음 접해 본 IaC Terraform 단점 Terraform 장점 개발 관련 다양한 서비스 배포   인프라 구축에 대한 정리     Stargate라는 개발 인프라 구축기 chart by Jinsu Park\n저희 팀의 개발 인프라는 위와 같습니다. 마침 제가 입사할 쯤이 기존에 존재하던 인프라를 새로운 환경으로 이해야할 시점이었습니다. 덕분에 저는 저희 개발 환경을 처음부터 구축하고, 우리의 서비스를 배포해보고, 이후 운영하면서 여러 경험들을 할 수 있었습니다.\nTerraform으로 처음 접해 본 IaC  친구: IaC가 뭔 지 알아? 써봤어?\n본인: 뭐 인프라를 코드로 관리한다는 건데, 나도 몰라 ㅋㅋ\n본인: 아마 너무 고급 기술이라 대학 졸업할 때 까진 못 써볼 듯?\n 위의 대화는 제가 입사하기 약 일주일 전에 나눴던 대화인데, 입사 후에는 어느 덧 IaC와 꽤나 친근해진 것 같아 유머삼아 유머로 가져와봤습니다. 메가존 클라우드의 클라우드원팀에서 일하기 전까지는 IaC는 저와는 거리가 먼 토픽이었고, Kubernetes 또한 minikube로 몇 가지 Object들을 배포해본 것이 다였습니다. 하지만, 저는 저희 팀에서 근무하게 되면서 terraform을 통해 위의 차트에 그려진 모든 인프라를 구축하고 파이프라인을 구축하게 됩니다.\nterraform을 사용하며 느꼈던 점은 \u0026lsquo;장점만 존재하는 기술은 드물 것이다\u0026rsquo;라는 점입니다. 분명 대부분은 장점이 있으면 그에 따른 단점이 존재할 것이고, 개인적으로는 테라폼도 장단점이 공존하고있다는 느낌을 받았습니다. 저의 주관적인 느낌에 따른 가장 큰 장점과 단점을 몇 개만 설명해보겠습니다.\nTerraform 단점 우선 단점부터. \u0026ldquo;사소한 인프라 변경도 코드에 반영하려면 문서를 찾아봐야하고, plan 내용을 검토해야한다\u0026rdquo;.\n저희는 평소에는 EKS의 로그를 켜놓지 않았습니다. 그런데 언젠가 EKS 로그를 CloudWatch를 이용해 분석해야한 적이 있는데, AWS 콘솔에서 바로 눈 앞에 로그 설정 칸이 있었음에도 EKS 로그를 설정하는 terraform Docs를 찾아본 뒤 plan을 분석한 뒤 apply 했어야합니다. 물론 아마도 로그 설정 쯤이야 잠깐 콘솔로 설정했다가 콘솔로 해제하면 그 한 번쯤은 문제가 없었겠지만, 그런 식으로 이번 한 번만, 이것쯤이야 하면서 코드가 아닌 매뉴얼로 직접 인프라 형상을 제어하는 경험이 쌓이게되면 형상이 깨져서 다시 형상을 맞추기 힘들수도 있고, 애초에 그런 수작업이 많이 들어가야하는 경우가 있다면 오히려 IaC를 이용할 필요가 없다고 생각했기 때문에 최대한 인프라는 terraform code로만 작업한다는 저의 원칙을 지키기 위함이었습니다.\n즉 자동화보다는 수작업이 편한 경우는 굳이 IaC라는 컨셉을 이용하는 것이 더 불편한 경우도 존재할 수 있을 것 같다는 생각이 들었습니다.\nTerraform 장점 \u0026ldquo;내가 사용하고 있는 인프라 전체를 한 눈에 보기 쉽다\u0026rdquo;\n물론 클라우드 서비스 하나에 대한 정보를 보고싶으면, 웹 콘솔에 들어가서 확인하는 것이 편하겠지만, 내가 이용하고 있는 AWS내의 모든 클라우드 서비스에 대한 정보나 설정을 보기에는 terraform 코드나 output, state 등이 더 알아보기 편할 수 있습니다.\n\u0026ldquo;혹시 장애가 생긴 경우 그 원인을 추적하기 쉽다.\u0026quot;\n수작업으로 클라우드 인프라를 관리하는 경우에 자신이 모르는 어떤 변동사항이 있고, 그 변동사항이 어떤 버그를 야기하고 있다면, 수작업으로 작업을 진행하는 경우에는 그 변동사항을 알아채고 트러블슈팅하기 쉽지 않을 것입니다.\n하지만 테라폼 코드를 통해 인프라를 관리하면 변동사항을 테라폼이 알아서 잡아주고, 혹은 코드에 대한 커밋 내역 등을 통해 변동 사항을 체크해 볼 수도 있을 것입니다. 따라서 그 변동사항에 맞는 트러블 슈팅을 하기 쉬울 것 입니다.\n개발 관련 다양한 서비스 배포 테라폼으로 인프라를 구축한 뒤에는 위의 차트에서 오른쪽에 작게 Stargate 라는 곳에 적힌 서비스들을 배포했습니다. 이 부분에 대해서는 너무나도 하고싶은 얘기가 많지만, 지루해질 수 있으니 그 중 기억에 많이 남거나 애착이 가는 서비스들에 대한 리뷰를 간단히 적어보겠습니다.\n Nginx Ingress Controller  로컬에서 minikube로만 개발하다가 처음으로 Ingress를 사용하게 되었습니다. helm을 이용한 게 아니라, helm으로 만들어진 manifest를 수동으로 하나하나 설정해서 설치하고 관리했습니다. 설정이 꽤나 복잡했었기에 설정을 많이 변경하는 경우 helm으로 설치하는 건 어떨까싶습니다\u0026hellip; 일하면서 Nginx로 L7 로드밸런싱 뿐만 아니라 L4 로드밸런싱도 몇 번 다룰 일이 있었고, 여러 Nginx Ingress Controller을 배포할 일도 있었고, gRPC 통신을 위한 설정을 해야할 일도 있었는데, 점점 수작업으로 하다보니 설정이 너무 복잡해져서 헷갈렸던 적이 있습니다. 뭔가 단점만 적은 것 같은데, K8s의 Ingress Controller로서 성능적인 부분은 제가 잘 모르겠지만, 크게 불편함 없이 잘 사용했습니다.   ALB Ingress Controller  서비스에 대해 직접 L7 로드밸런싱을 수행할 경우 이용합니다. 헷갈렸던 점은 보통은 Nginx Ingress Controller는 사실 Ingress는 설정이고, 컨트롤러가 실제로 로드밸런싱을 수행하는데, ALB는 ALB Ingress Controller가 Ingress를 통해 ALB를 만들고 실제 로드밸런싱은 ALB Ingress Controller가 아니라 ALB가 한다는 점이었습니다. 설정에 따라 다르지만 기본적으로는 노출시키고자 하는 서비스는 NodePort급 이상으로 서비스가 열려있어야하는데, 실수로 ClusterIP로 노출시켜 ALB가 서비스를 제대로 찾지 못한 적이 종종 있었습니다. 주의..!   Cert Manager  자동으로 cert를 발급해주고 갱신해주는 서비스입니다. Let\u0026rsquo;s Encrypt를 이용했고 무료입니다. 정말 정말 편리했습니다! DNS, HTTP Challenge, TLS에 대해 많이 배울 수 있었습니다. TLS 통신 과정은 공부해도해도 정확한 순서는 까먹게 돼서\u0026hellip; 다시 공부해봐야겠습니다.   Jenkins  편한데, 관리하기 귀찮을 수 있을 것 같습니다. 결정적으로 코드로 관리할 수도 없는데, UI도 그렇게 직관적인지는 잘 모르겠습니다.   Spinnaker  정말 편리합니다. Jenkins test 결과를 CD에 이용할 수도 있고, 편리하고 다양한 문법, 파이프라인 종류, 직관적인 UI/UX. 사실 Argo를 도입하고 싶었지만, Argo는 약간 가벼운 쿠버 환경에 대한 배포용 같은 느낌, IaC는 적극 도입되었으나 현실에 도입은 쉽지 않은 기술적 이상향 같은 느낌이 컸고, 현실적으로는 좀 더 안정적인 Spinnaker를 유지하게 됐습니다. 좀 느리다고 생각했는데, 이 글을 쓰면서 생각해보니 개발에서는 K8s Deployment의 Grace Period 때문인 것 같아 그걸 좀 줄여볼 껄 싶습니다. 즉 다시 생각해보니 별로 느리지 않은 듯합니. 배포 자체는 안정적이고 좋은데, 설정할 때 버그가 종종 있습니다. 버그인지 제 실수인지는 모르겠지만 Pipeline stage 에서 Bake라는 모드가 \u0026ldquo;우와 신박하다!\u0026rdquo; 라고 생각했지만, 설정이 제대로 되지 않거나, OAuth login 실패에 대한 처리, Artifact 경로 설정이 너무 번거로운 점 등의 단점이 있었습니다.   Keycloak  우리의 개발툴들에 대한 인증을 사내 계정으로 연동시켜주는 Single Sign On 기능을 지원했습니다. 새로운 팀원은 각 서비스에 별도의 가입 과정 없이, 앱 레벨에서 따로 권한 부여가 필요한 것이 아니라면 사내 계정으로 바로 이용이 가능했습니다. 문서가 제대로 정리된 게 다소 부족한 느낌이었어서 세밀한 설정이나 정확한 작동원리를 파악하기는 쉽지 않았던 점이 조금 아쉽습니다.    인프라 구축에 대한 정리 내용이 너무 길어지면 읽기 힘들 것 같아 최대한 느낀 점 위주로 간단히 정리해보려고 노력해보았습니다. 인턴으로 일을 하기 전에 가벼운 마음 속 목표가 하나 있었습니다.\n \u0026lsquo;어떤 걸 어느정도 사용해보면 그것에 대한 주관적인 평가를 내릴 수 있는 사람이 되고싶다.'\n 누군가 \u0026ldquo;도커가 설치하기 정말 쉽더라구요.\u0026rdquo;, \u0026ldquo;minikube가 정말 편리하더라구요.\u0026rdquo;, \u0026ldquo;Jenkins 보다는 travis가 편리더라구요.\u0026rdquo; 이런 얘기를 꺼내도 과거에는 공감할 수도, 제 주관적인 평가를 내릴 수도 없었습니다. 저의 경험으로 구성된 모집단이 없었기 때문입니다. 하지만 저희 클라우드 원 팀에서 근무하면서 어떻게 보면 깊이는 다소 얕았을 지 몰라도 정말 다양한 서비스를 접하고 다채로운 경험을 할 수 있었던 것 같습니다! 덕분에 이제는 어떤 서비스를 접하든, 새로운 언어를 접하는 저만의 느낌을 가질 수 있고, 의견을 말할 수 있을 것 같습니다!\n"},{"uri":"https://umi0410.github.io/blog/aws/","title":"Category: AWS","tags":[],"description":"","content":"  EKS K8s에서 ELB(ALB, NLB) 제대로 설정하며 사용하기  🐶 시작하며 본 게시글은 AWS 대학생 유저그룹인 AUSG의 활동 중 하나로서 본인(박진수)이 작성한 게시물을 포워딩한 것입니다. 데브옵스 인턴으로 근무한 지가 벌써 두 달이 되어갑니다. 이것 저것 배운 것이 많았던 시간이었는데, 그 중 꽤나 삽질을 했던 Kubernetes 와 ELB를 이용하는 부분에 대해 정리를 해볼까합니다. jenkins, spinnaker, argo, terraform, ansible, github action, \u0026hellip; 등등 다양한 내용을 경험할 수 있던 시간이었지만, 그 중 kubernetes에서 무슨 작업을 하던 빼놓을 수 없으면서 어딘가 깔끔히 그 흐름이 정리된 곳을 보기 힘들었던 service를 ELB에 연결하기에 대한 내용을 정리해보겠습니다.\n "},{"uri":"https://umi0410.github.io/experiences/megazone-cloud/ci-cd-pipeline/","title":"Github Action, Spinnaker을 이용한 CI/CD 파이프라기기","tags":[],"description":"설 
","content":" Table of Contents  Github Action을 사용하게 된 배경 Github Action vs Jenkins 그래서 어떤 CI/CD를 자동화하였나요? Github Action을 다뤄보면서 느낀 점     Github Action을 사용하게 된 배경 저희 팀은 원래 CI용으로 Jenkins를 사용했습니다만 팀이 개발 중이던 서비스가 오픈소스가 목표인 프로젝트였고, Github Action이 빠르게 발전해나가면서 비용도 무료가 되었고, 좋은 Action들이 많이 생겨나고 있었기에 어느 정도 프로젝트 구조가 잡힌 뒤에는 Github의 Public Repository로 프로젝트를 관리하고 Github Action을 CI 도구로 채택하게되었습니다. Integration Test를 제외한 모든 빌드 및 일부 배포를 Github Action을 이용하게되었고, 대부분의 배포에는 사용하던대로 Spinnaker을 이용했습니다.\nGithub Action vs Jenkins 이 부분 역시 느낀 점 위주로 요약해보겠습니다.\n   Github Action Jenkins     내가 서버를 관리할 필요가 없다. 내가 직접 master을 띄우고, slave를 띄워우고, 관리해줘야한다.   VM이 배치되어 제공되는 데에 좀 시간이 든다. 내 커스텀 이미지를 사용할 수가 없다보니 반복되는 패키지 설치나 환경 설정을 매번 해야해서 좀 느리다. 내가 필요한 Plugin을 설치해놓거나 설정을 입력해 놓으면 매번 빌드할 때 따로 제공할 필요 없다.   Code로 관리가 가능하다! 처음엔 조금 어려울 수 있지만, 알고 나면 쓰기 너무 쉽다. 처음 접한 사람이 사용하기에는 Github Action보다 편리할 수 있지만, 그렇다고 훌륭한 UI/UX는 아닌 듯 하다.   요즘 핫하고, 빠르게 발전 중이다. 구식이다.    비용이 저렴하게 풀리고 있고(퍼블릭의 경우 아마 무조건 무제한 공짜), 원래 지원하지 않던 매뉴얼 트리거가 2020.07부터 제공되기 시작했다는 점, 누구든 오픈소스로 Github Action에서 남들이 사용할 수 있는 action 을 만들 수 있다는 점등을 보고 Github Action이 빠르게 발전 중이라는 생각이 들었습니다.\n그래서 어떤 CI/CD를 자동화하였나요? chart by Jinsu Park\n세로로 길어서 좀 보기 불편하실 수도 있는데, 위의 차트가 저희의 깃헙액션에 대한 차트입니다. 레포지토리마다 조금씩 다른 부분이 있고, 프론트엔드의 경우 파이프라인이 다양했는데, 우선 백엔드의 깃헙액션 및 CI/CD 진행 방식에 대해 요약해보겠습니다. (Chart를 작성한 지가 좀 돼서 설명과 조금 다른 부분이 있을 수도 있습니다.)\n개발 스프린트 진행 시에는 아래와 같이 진행되었습니다. (\u0026lt;상황 설명\u0026gt; =\u0026gt; \u0026lt;Github Action 수행 내용\u0026gt; 형식으로 나했습니다.)\n 개별 Fork 후 Master Branch에 Pull Request =\u0026gt; lint, basic unit test 진행. 통과된 PR만 Merge열 Master에 Merge 혹은 Commit이 Push됨 =\u0026gt; CI Action이 실행됩니다. 개발용 docker registry 에 업로드 해당 registry에 업로드 된 것을 감지하고 Spinnaker가 Experiment 환경에 배포 Experimental 환경을 이용해 Jenkins가 Integration Test를 진행 Integration Test 성공 시에 Dev 환경에 배포.  이후 스프린트 막바지 QA 기간에는 Experiment, Dev 환경이 주로 QA 환경으로 사용되었고, 파이프라인은 다음과 같았습니다.\n 검증이 어느 정도 끝난 커밋에 대해 Git Tag를 {{VERSION}}-rc{{RC_NUMBER}} 형태로 달아 푸시 - Github Release가 생김 =\u0026gt; 도커 이미지 빌드 후 Production Docker Registry에 이미지 업로드 =\u0026gt; tag가 달린 커밋을 기점으로 자동으로 버전 명의 브랜치를 만듦. Production Docker Registry의 업로드를 감지하고 Dev에서 손수 QA 진행 문제가 있을 경우 rc number를 올려서 다시 태그를 달고 업로드 후 재차 QA 문제가 없을 경우 해당 태그를 바탕으로 rc를 지우고 실제 버전으로서 태그를 달아 푸시 =\u0026gt; 다시 업로드 Production은 매뉴얼 배포.  Github Action을 다뤄보면서 느낀 점 처음에는 Github Action에 그렇게 만족을 하지 못했습니다. 초기에는 Manual Trigger가 지원되지 않았던 데다가, 가뜩이나 Github Action을 잘 몰랐기에 한 번 Github Action을 수정하여 테스트 하고싶을 때 마다 커밋을 하나씩 날려야했던 게 불편했고, 브랜치를 따로 만들거나 fork를 떠서 Github Action 테스트 후 해당 workflow만 마스터에 머지하는 방식 등등 다양한 방식을 사용했었는데, 어느 정도 익숙해지고 Github Action에 Manual Trigger도 등장하게 되면서 꽤나 만족도가 높아졌습니다.\nJenkins와 달리 제가 서버를 이용하지 않아도 된다는 점도 맘에 들긴했는데, 종종 Github 서버가 죽는 일이 발생해서 난감했던 적이 있긴합니다.\n하나 재미있었던 점은 Github Action을 이용하면서 저의의 CI/CD 전략이 꽤나 고도화되었는데, 그 과정에서 팀원들과 자유롭게 의사소통하는 과정이 재미있었고, 저 또한 자유롭게 의견을 나눌 수 있었던 것 경험을 했다는 것입니다.\n후에 저희 SpaceONE의 CLI API 클라이언트인 spacectl의 설계에도 Github Action의 구조를 모티브삼았는데 이때에도 Github Action에 대한 지식이 많은 도움이 되었고, 퇴사 후에도 개인적인 Github 활동을 하면서 자유롭게 Github Action을 사용할 수 있었기에 든든한 개발 도구를 얻은 느낌입니다. 코드로 제가 하고싶은 것을 뭐든 정의할 수 있고, 만들어져 있는 작업은 편하게 가져다 쓰면 되기 때문에 빌드나 배포에 관해 재미있는 번뜩이는 아이디어가 있을 때 바로 바로 적용할 수 있고, 실제로 현재의 Github Page도 Github Action을 통해 다양한 트릭을 이용할 수도 있었고, 빌드 후 배포 또한 자동화 되어있습니다!\n"},{"uri":"https://umi0410.github.io/experiences/megazone-cloud/argo-poc/","title":"Argo Project들에 대한 PoC(개념 증명) 진행","tags":[],"description":"요약
","content":" Table of Contents  Argo PoC를 진행하계 된 배경 Argo Project 구조  Argo Event (Official site) Argo, Argo Workflow (Official site) ArgoCD (Official site)   Argo Project를 조합한 CI/CD 파이프라인 구축기  파이프라인 수행 과정   Argo Project들에 대한 PoC를 진행하며 느낀 점     Argo PoC를 진행하계 된 배경 입사 초기부터 Argo에 대해 간간히 이야기를 들어왔습니다. 저희는 원래 Spinnaker을 사용했는데, 쿠버네티스 환경에 좀 더 친화적이라는 Argo를 도입해보는 것은 어떨까에 대한 얘기였는데요. 어느정도 개발 인프라 구축이 완료되고 한가해지자 잠깐이나마 Argo를 사용해볼 수 있었습니다. 결과적으로 Argo 도입은 적합하지 않다고 판단이 되었고 따라서 충분히 써볼 수는 없었기에 기록용으로만 간단히 적어보겠습니다.\nArgo Project 구조   눌러서 참고용 Argo Draw.io Chart들 보기    Argo가 워낙 빠르게 변화하는 서비스다보니 지금은 구조가 많이 변경되었을 수도 있습니다!\n chart by Jinsu Park\n  간편하게 사용할 수 있는 순서는 argo, argo(workflow), argo event 순이라고 생각되는데, 실질적인 파이프라인 구축은 argo event, argo (workflow), argo 순으로 이뤄지는 셈이라 후자의 순서에 맞춰 설명해보겠습니다.\nArgo Event (Official site) image by argo-events\n셋 중 가장 베이비 프로젝트입니다. 빠르게 개발되고 있고, 변화하고있기에 제가 argo project들을 만났던 시절과 많이 구조가 달라져있습니다. 간단하게 말하자면 이벤트를 감지하여 어떤 작업을 수행할 수 있습니다. argo-events의 공식 홈페이지에서 제공되는 위의 이미지와 같이 다양한 이벤트 소스를 이용해 다양한 이벤트를 트리거할 수 있습니다. 예를 들어 AWS SQS에 메시지가 생기면, 그 메시지를 가져와서 어떤 작업을 수행할 수 있습니다. 혹은 웹훅 서버를 돌려서 웹훅 요청이 오면, 어떤 작업을 트리거할 수 있습니다.\n제가 구상한 파이프라인에서는 이벤트를 감지하는 역할로서 Argo Events를 앞에 두고, Argo Events가 Workflow를 생성는 작업을 트리거하는 방식으로 CI/CD에 이용할 수 있습니다.\nArgo, Argo Workflow (Official site) 셋 중 가장 오래된 프로젝트이고, 별도 많습니다. Argo 라는 이름을 가졌고 실질적으로는 Workflow 관련 프로젝트입니다. Workflow란 컨테이너를 이용해 진행되는 일련의 step들을 정의하는 CRD(K8s Custom Resource Definition 입니다.A)\n처음엔 \u0026lsquo;굳이 Workflow가 필요할까\u0026rsquo; 싶었지만, 쿠버네티스 상에서 일련의 Job을 연속적으로 수행할 수 있는 방법이 현재까지는 없는 것으로 알고 있습니다. K8s Job의 Container에 대한 InitContainer을 지정함으로써 한 Job에 대한 두 Container의 순서를 명시할 수는 있지만, Workflow처럼 다양하게 일련의 Job을 연속적으로 수행하기는 힘든 것으로 알고 있습니다.\nArgoCD (Official site) ArgoCD만 보면 가장 간단하게 실제 CD에 적용할 수 있는 프로젝트 중 하나가 아닐까싶습니다. ArgoCD 자체에 대한 설정, 소스가 될 Repository에 대한 설정, Project, Application에 대한 설정 등등 모든 것이 Code IaC에 특화된 독특한 프로젝트입니다.\nUI가 직관적이고 알아보기 쉽지만, 반대로 대규모 애플리케이션이 될 경우 너무도 배포 현황을 보여주는 맵이 커지기 때문에 알아보기 쉽지 않을 수 있을 것 같습니다. GitOps(\u0026ldquo;GitOps의 핵심은 Git 저장소에 저장된 쿠버네티스 매니페스트 같은 파일을 이용하여, 배포를 선언적으로 한다는 것입니다. 출처\u0026quot;) 라는 말에 아마 빠지지 않고 등장하는 CD 도구인듯합니다. Git에 올라가는 Manifest가 그대로 K8s 클러스터에 적용됩니다. 마치 Github Page나 S3(웹호스팅 설정이 된 S3 Bucket)에 올린 파일들이 바로 하나의 웹 애플리케이션처럼 동작하는 것과 비슷한 느낌입니다. 간단하게 사용하기에는 ArgoCD가 참 좋아보였지만, 실제로 업무적으로 사용하지는 못하겠다고 판단한 이유가 몇 가지 있습니다.\n 애플리케이션 하나에 대한 배포는 쉽지만, 연속적인 배포나 다양한 배포 파이프라인을 구성하기는 어려웠다. Github Push 시에 webhook을 설정해서 ArgoCD의 배포를 트리거하도록 했는데 이 부분이 최적화가 덜 되었는지 어떤 repository든 하나의 ArgoCD 환경에서는 Git platform 당 하나의 secret만 설정이 가능했고, 한 repository가 push되면 다른 repository도 배포가 다 같이 이루어져버렸습니다. (이 부분은 패치됐을 수도 있습니다.) 소규모 애플리케이션은 배포 상황을 한 눈에 보기 쉽지만, K8s Object가 많아지면 많아질수록 점점 하나하나 눈에 보이지 않고, 잡다한 Object들(ConfigMap, Secret, \u0026hellip;)로 인해 인식하기 힘듭니다.  Argo Project를 조합한 CI/CD 파이프라인 구축기 파이프라인 수행 과정  (Argo Event) Github Push를 감지하고 Argo Workflow CRD를 생성 Argo workflow를 통해 다양한 step들을 이용하기 위해 바로 ArgoCD가 Github Push를 받지 않고, Argo Event가 받도록 했습니다. Argo Workflow의 step들 수행  Slack에 workflow 시작 알림 Experiment 환경에 배포 (Argo CD 이용) Interation Test 실행 트리거 (Jenkins 이용) Integration Test 결과가 성공이면 Dev 환경에 배포 (Argo CD 이용) 어느 step에서든 실패 시 Slack에 실패 알림    이 정도 Pipeline을 짜면 사실 사용할 정도는 될 수 있겠지만, \u0026ldquo;일\u0026quot;로서 본다면 굳이 현재 사용 중인 Spinnaker에 비해 장점이 뚜렷하게 느껴지지 않는 Argo를 위해 파이프라인을 이전하기에는 역부족이라는 판단이 들었습니다. 예를 들어 과정 목록에서는 간단히 묘사했지만, Spinnaker에서는 UI에서 알아서 처리되던 부분들을 하나 하나 webhook을 걸어주거나, 따로 bash script를 짜야하는 경우들이 많았습니다.\nArgo Project들에 대한 PoC를 진행하며 느낀 점 개인적으로는 많이 애정이 갔던 프로젝트들이고 신기했던 배포방식에 대한 소개였으며, 이 프로젝트들을 공부해보면서 다양한 아키텍쳐에 대해 경험해볼 수 있었던 것 같지만, 현실적인 벽에 부딫혀 도입을 할 수 없었던 아쉬움 컸습니다. 마치 좋아하는 분야가 있었지만, 현실의 벽에 부딫혀 꿈을 접고, 어떠한 현실적인 진로 나아가는 것과 같았달까요?\n그리고 개인적인 아쉬움은 위의 차트를 손수 그리며 PoC 문서를 작성했었지만, 결국은 PoC를 진행한 담당자인 저조차 \u0026ldquo;음\u0026hellip; 실사용은 힘들 것 같은데요\u0026quot;라는 의견을 냈던 터라 바쁜 일정 속에서 reject할 개념 증명 리에 많은 시간을 할애할 수 없었기에, 팀원들과 이 내용을 공유할 수 없었던 점에 조금 아쉬움이 남습니다..! 만약 정말 Argo를 사용하는 방식이 뛰어난 방식이었다면, 저도 적극 추천하며 함께 리뷰해주길 기대했겠지만, 뷰현실적으로 저희 팀의 배포 방식과는 맞지 않았다고 판단해서 그랬던 것이기에 괜찮습니다~!\n"},{"uri":"https://umi0410.github.io/experiences/megazone-cloud/spaceone-helm/","title":"SpaceONE Helm Chart 개발","tags":[],"description":"설명
","content":" Table of Contents  SpaceONE Helm Chart란? spaceone-helm 설계 spaceone-helm chart 구조 마이크로서비스들의 버전 관리를 시작 Helm Chart를 통해 패키지화하며 느낀 점     SpaceONE Helm Chart란? spaceone-helm 은 저희 CloudOne 팀이 개발하는 서비스인 SpaceONE을 helm chart를 이용해 패키지화하는 프로젝트입니다. 원래의 저희 환경은 MicroService들을 개별 배포하고있었지만 오픈소스로 개발되는 저희 서비스를 저희 팀원들 뿐만아니라 다른 개발자들이 쉽게 개발할 수 있고, SpaceONE을 모르던 사용자들도 쉽게 SpaceONE을 구축해볼 수 있도록 하기 위해 패키지화도 진행하게되었습니다.\n일반적으로 만들어진 Helm Chart를 이용해보기만했지 직접 Chart를 만드는 것은 처음 해본 일이기도 했고, Chart를 개발하면서 새로운 환경에 저희 서비스를 배포해보다보니 삽질하며 고생도 꽤 했고, 무엇보다 프로젝트의 시작부터 퇴사 전까지의 작업들을 거의 제가 도맡아한 프로젝트였기에 개인적으로 애정이 많이 갔습니다 ^_^! 그리고 저도 이제는 저희 Chart를 이용해 SpaceONE을 구축형으로 손쉽게 이용할 수 있었습니다.\nspaceone-helm 설계 일단은 심플하게 사이드카 없이 저희의 마이크로서비스들만을 배포하고, 그 외에 필요한 서비스들도 최대한 Cloud Service에 의존하지 않고 그때 그때 손쉽게 서비스를 내렸다 올렸다 할 수 있도록 K8s 클러스터 위에 함께 배포하는 형식으로 Chart를 설계했습니다.\n무엇보다 chart 개발의 목적은 아래 두 가지 사항이 컸기 때문에, minikube 혹은 EKS 띄운 마이크로서비스들과 로컬에서 통신이 가능하도록 해야했고, 사용하기 편리한 구조를 만들기 위해 고심했습니다.\n 팀원이 아닌 개발자들도 자신의 부가적인 마이크로서비스를 로컬에 띄워 개발할 수 있도록 지원.  로컬에서도 쿠버네티스 클러스터 내의 서비스에 접속이 가능해야하고, 경우에 따라 클러스터에서도 본인의 로컬 서비스로 접속을 할 수 있어야함.   오픈소스로서 임의의 사용자가 서비스를 구축해보고자 시도할 때 손 쉽게 구축할 수 있도록 지원.  마치 유저들의 사용자 경험을 중요시해서 디자인, 기획을 하듯 Chart의 사용자들이 직관적이고 편리하게 구축할 수 있도록 Configuration values(values.yaml in Helm Chart)를 설계함.    spaceone-helm chart 구조 templates/ ├── backend │ ├── config │ │ ├── config-conf.yml │ │ ├── config-deployment.yml │ │ └── config-svc.yml │ ├── identity │ │ ├── identity-conf.yml │ │ ├── identity-deployment.yml │ │ └── identity-svc.yml │ ├── inventory │ │ ├── inventory-conf.yml │ │ ├── inventory-deployment.yml │ │ └── inventory-svc.yml │ ├── inventory-scheduler │ │ ├── inventory-scheduler-conf.yml │ │ ├── inventory-scheduler-deployment.yml │ │ └── inventory-scheduler-svc.yml │ ├── inventory-worker │ │ ├── inventory-worker-conf.yml │ │ ├── inventory-worker-deployment.yml │ │ └── inventory-worker-svc.yml #... Backend 생략 ├── consul, mongo, redis # 디테일한 파일구조는 생략 ├── frontend │ ├── console # 디테일한 파일구조는 생략 │ └── console-api # 디테일한 파일구조는 생략 ├── ingress │ └── ingress.yaml ├── initializer │ ├── initialize-spaceone-conf.yml │ ├── initialize-spaceone-job.yml │ ├── spacectl-apply-conf.yml │ └── spacectl-conf.yml └── supervisor └── supervisor ├── supervisor-conf.yml ├── supervisor-deployment.yml └── supervisor-roles.yml 저희 spaceone-helm chart는 꽤나 구조가 간단한 편은 아니라고 생각됩니다. 웹페이지 환경상 너무 부수적인 부분은 tree에서 생략하기도 했습니다. 꽤나 복잡한 구조를 가졌기에 패키지화하는 부분이 쉽지 않았지만, 그런 과정 속에서 많이 트러블 슈팅을 경험하고 성장할 수 있었던 것 같습니다.\n그리고 반대로 생각하면, 서비스가 패키지화하기도 쉽지 않을만큼 복잡한 구조를 가졌다면, 당연히 손수 배포하는 것은 그것보다 몇 배는 어려울 것이므로 누군가가 저희 서비스를 구축형으로 이용해주기 위해서는 패키지화가 필수라는 생각이 들었습니다.\n마이크로서비스들의 버전 관리를 시작 Helm Chart를 개발하고, CI가 고도화되기 전까지는 단순히 팀 내에서 개발을 진행하면서 개발환경에 대한 배포는 latest tag만을 이용해 자동으로 진행하고, QA를 진행하면서 커밋을 멈추고, 그 시점에 빌드된 Docker image의 Tag를 바탕으로 상용 환경에도 배포를 하곤했습니다. 하지만, Chart에서도 tag를 latest로 유지하거나 가독성이 좋지 않은 임의의 tag를 이용해 이미지를 제공하기는 힘들었고, 맞는 방향이 아니라고 생각했습니다.\n따라서 저희는 꼭 Helm을 통한 패키지 뿐만 아니라 롤백에 대한 안정성, 버전 간의 Update 내역 관리 등을 위해 {{MAJOR}}.{{MINOR}}.{{SPRINT_NUMBER}}-{{EXTRA_TAGS}} 형태를 통해 버전을 관리하고자했습니다.\nHelm Chart를 통해 패키지화하며 느낀 점 Helm Chart를 통해 저희 서비스를 패키지화하기 전까지는 마이크로서비스 형태로 관리되는 서비스를 새로운 환경에 완전하게 구축한다는 것은 쉽지 않았습니다. Container라는 것이 ReadOnly 레이어로 이미 구성이 완료된 이미지를 바탕으로 생성되기 때문에 언제 어디서든 구동이 가능하다는 것이 장점이겠지만, 현실적으로는 환경에 따라 이것 저것 설정해줘야하는 것이 있었기때문입니다.\n이러한 난관들은 어떠한 변수처리와 자동화를 통해 해결할 수 있을텐데, 그러한 작업을 해준 녀석이 바로 Helm이었고, 그때 그때 설정을 바꿈으로써 커스터마이징할 수 있다는 점이 알고는 있었습니다만 막상 저희 서비스에 도입해보니 꽤나 만족스러웠습니다.\n일화로 사내 네트워크가 막힌 상황에서도 받아놓은 이미지들만 있으면 네트워크 접속을 할 필요 없이 minikube와 로컬 서버를 이용해 minikube의 클러스터와 통신하면서 개발을 진행할 수도 있었습니다.\n누군가 제가 만든 서비스를 사용해준다는 것은 참 뿌듯한 일이었고, 앞으로도 저희 helm chart가 잘 발전되어 더욱 더 편리하게 구축할 수 있는 형태로 제공될 수 있기를 기대해봅니다!\n"},{"uri":"https://umi0410.github.io/experiences/megazone-cloud/spacectl/","title":"SpaceONE CLI Client인 spacectl 설계 및 개발","tags":[],"description":"설명
","content":" Table of Contents  spacectl이란?  소개 사용 예시   설계를 하며 느낀 점 개발하면서 느낀 점     spacectl이란? 소개 spacectl 은 저희팀이 개발하는 서비스인 SpaceONE의 gRPC API request를 CLI로 손쉽게 수행할 수 있도록 해주는 도구입니다. 파이썬을 통해 개발했고 Click 이라는 모듈로 CLI 환경을 손쉽게 사용할 수 있었고, Jinja2를 통해 상세한 Manifest 들에서 변수 치환, 분기 등을 수행할 수 있었습니다.\n사용 예시 A simple example 간단하게 spacectl이 어떤 식으로 이용되는 도구인지 예시를 보여드리겠습니다. 아래의 커맨드를 이용해 손쉽게 SpaceONE의 다양한 마이크로서비스들의 API를 이용할 수 있습니다.\n$ spacectl list domain domain_id | name | state | plugin_id ... domain-abc123abc | umi0410| ENABLED | ... $ spacectl list server -p domain_id=domain-abc123abc server_id | name | provider ... server-abc123abc | foo | aws ... apply command spacectl apply command는 kubectl의 apply와 유사하게 없으면 만들고, 있으면 업데이트하고 혹은 단순히 어떤 API를 Execute하는 Task들의 플로우를 관리해주는 커맨드 입니다.\n 아쉽게도 퇴사 전에 마무리를 짓지는 못했습니다. ㅜ.ㅜ 퇴사 전까지 진행한 작업은 일부 리소스에 대한 CRU(create, read, update), 대부분의 리소스에 대한 Excute(execute할 API를 설정)까지입니다.\n # main.yaml import: - mongo.yaml # 개별 yaml file에서는 terraform/ansible과 같이 수행할 Task들을 정의 - root_domain.yaml - repository.yaml var: domain_name: root domain_owner: admin admin_username: admin admin_password: admin $ spacectl apply main.yaml 설계를 하며 느낀 점 이 프로젝트에 대한 실제 개발 업무 이전에는 꽤나 설계 업무가 많았습니다. 저는 그 동안은 혼자 주로 개발을 해왔고, 실행력 좋게 시작은 하지만 설계는 충분하지 않은 채 성급하게 실행에 옮겼던 경험이 많습니다. 또한 학생이었고, 개발 경력이 길지 않았기에 사실상 \u0026ldquo;개발 = 그때 그때 새로운 내용 공부\u0026quot;와 같은 느낌이었기에 애초에 설계를 하려해도 \u0026lsquo;뭐가 필요하고 뭐가 가능할 것이고 뭐가 힘들 것인가\u0026rsquo; 를 판단하기 어려웠습니다.\n하지만 팀원들과 함께 개발하면서 밥 먹을 때, 회의 할 때 틈틈히 설계 방식과 요령에 대해 상의했고, 처음으로 설계를 어느 정도 굳힌 뒤 개발에 들어들어갔던 경험이었습니다.\n인턴 기간 막바지에 이 설계에 참여하게 된 것은 정말 값진 경험이었다고 생각했습니다. 사실 단순히 데브옵스로서 일할 때는 남들과 의사소통할 일이 그리 많진 않았는데, 이 설계를 맡게 되면서 많은 회의와 대화를 하게되었습니다. 선배 개발자분들과 설계에 대해 잦은 회의를 하면서 제가 어떤 개발자가 되고싶은지 직접 느낄 수 있었던 것 같습니다. 그 배경에는 두 가지 충격이 있었습니다.\n \u0026lsquo;내가 남의 생각을 잘 읽는 편은 아니었나보군\u0026hellip;?' \u0026lsquo;선배 개발자분은 내가 개판으로 설명해도 어떻게 귀신같이 나보다 내 생각을 잘 읽으시지?' =\u0026gt; 마치 축구할 때 노련한 축구선수와 함께 뛰면서 저 행동을 귀신같이 예측하고서는 너무나도 잘 밀어주는 느낌을 받았습니다\u0026hellip;  제가 평소에 말을 잘하는 편이라고 생각했는데, 남의 생각을 이해하고 읽어내는 능력은 그리 뛰어나지많은 않구나라는 생각을 하게됐습니다. 설계 내용이 꽤나 추상적으로 구두로 진행되었기에 그랬을 수도 있겠지만, 큰 충격은 선배 개발자분의 노련함이었습니다.\n후에 누군가 어떤 개발자가 되고싶냐, 협업할 때 어떤 노하우가 있느냐 이런 내용을 물어보면 자신있게 남의 생각을 잘 이해하고, 알아주는 사람과 관련해 대답할 수 있는 사람이 될 수 있었으면 좋겠습니다.\n또한 설계 깊게 진행하기 전에 침착하게 자료 조사를 잘 해야한다는 것을 느꼈습니다.\n하나 일화로 원래는 설정 파일에서 ${{ tasks.umi0410.output }} 이런 식의 변수를 이용한 설정을 치환한 뒤 API를 수행해야할 때, 아마 template 언어들을 제가 원하는 대로 사용하기 힘들 것이라 생각하고, 하나하나 함수와 클래스를 만들어가곤했는데, 개발이 거의 완료되어갈쯤 Jinja2의 사용법을 다시 읽다보니 spacectl에 Jinja2를 적절히 사용할 수 있을 것 같았고, 정신이 번뜩 들어 몇 시간만에 수작업으로 짠 코드들을 덜어내고 Jinja2를 이용해 좀 더 깔끔하게 변수 치환 및 추가로 Jinja2의 built-in filter들을 이용할 수 있었습니다!\n개발하면서 느낀 점 역시 개발은 남이 만든 패키지를 잘 사용해야한다는 것을 느꼈고, 그냥 복사 붙여넣기만 잘하면 된다는 의미가 아니라, 그런 것들을 빠르게 가져와서 적용시키고 부분 부분 커스터마이징하기 위해서는 기본기가 탄탄해야한다고 느꼈습니다. 물론 수작업으로 만드는 것도 좋을 수 있겠지만, 다양한 엣지케이스가 존재할 수 있고, 그 모든 작업들을 문서로 상세히 설명하는 것이 아니라면, 제작자인 제가 아닌 누군가가 그 기능을 이용하기는 힘들 것입니다. Jinja2를 이용한 템플릿 기능 제공이 이와 관련된 경험이 될 수 있겠습니다.\n또한 남의 코드를 자세히 읽어보는 게 처음이었는데, 덕분에 파이썬 프로젝트를 수행할 때 어떤 식으로 디렉토리 스트럭쳐를 짜면 좋을 지 생각해볼 수 있었던 계기였던 것 같습니다. 제가 설계하고, 개발한 내용을 짧게나마 발표한 뒤 리뷰를 받고 수정을 하면 한층 더 코드의 구조와 사용이 간결하고 직관적으로 보인다는 느낌을 받을 수 있었습니다. 이렇게 다양한 가르침을 주신 저희 팀의 선배 개발자분들께 항상 감사드립니다.\nMegazone CloudOne 팀의 DevOps 인턴으로서 근무했던 내용에 대한 후기가 거의 끝났습니다. 끝으로 인턴 활동에 대한 종합적인 느낀점을 이어서 보시거나 다시 목차를 보고싶으신 분은 여기를 클릭해주세요.\n "},{"uri":"https://umi0410.github.io/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://umi0410.github.io/tags/","title":"Tags","tags":[],"description":"","content":""}]