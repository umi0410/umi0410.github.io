[{"uri":"https://umi0410.github.io/","title":"About Jinsu Park (umi0410)","tags":[],"description":"","content":"😄 소개   이름: 박진수\n  학력: 경희대학교 컴퓨터공학과 16학번 재학\n  관심사\n 깔끔한 백엔드 구조 및 설계 컨테이너 오케스트레이션 클라우드 인프라 Golang CI/CD 파이프라인    자기소개 (미정\u0026hellip;) 차츰 업데이트 될 예정\n Golang, Cloud services, MSA, 컨테이너와 자동화, 깔끔한 아키텍쳐에 관심이 많습니다.\n어떤 기술을 배우거나 적용시킬 때 \u0026ldquo;언제\u0026rdquo;, \u0026ldquo;왜\u0026rdquo; 그 기술이 필요한지에 대해 생각해보는 것을 좋아합니다.\n특히나 요즘은 Golang을 이용한 Go스러운 개발과 Message Service에 관심이 많습니다.\n진행 중인 프로젝트에서는 MSA와 K8s, ArgoCD, Github Action, Golang, Python등을 적용하며 공부 중입니다.\n  🥅 활동 내용  [2020.10~xxxx.xx] 경희대학교 커뮤니티 앱 쿠뮤 개발 중 [2020.04~2020.08] Megazone Cloud에서 DevOps 인턴 [2020.08~2020.09] 오픈스택 컨트리뷰톤: 오픈스택 팀 멘티 [2020.01~2020.03] 위시오피스 AWS 서버 관리자 [2019.06~] 한국 AWS Communitiy KRUG의 대학생 그룹 AUSG에서 3기로 활동 중 [2019.11] 경희대학교 소프트웨어 페스티발 웹/앱 최우수상 수상  "},{"uri":"https://umi0410.github.io/portfolio/","title":"About Jinsu Park (umi0410)","tags":[],"description":"","content":"😄 소개   이름: 박진수\n  학력: 경희대학교 컴퓨터공학과 16학번 재학\n  관심사\n 깔끔한 백엔드 구조 및 설계 컨테이너 오케스트레이션 클라우드 인프라 Golang CI/CD 파이프라인    자기소개 (미정\u0026hellip;) 차츰 업데이트 될 예정\n Golang, Cloud services, MSA, 컨테이너와 자동화, 깔끔한 아키텍쳐에 관심이 많습니다.\n어떤 기술을 배우거나 적용시킬 때 \u0026ldquo;언제\u0026rdquo;, \u0026ldquo;왜\u0026rdquo; 그 기술이 필요한지에 대해 생각해보는 것을 좋아합니다.\n특히나 요즘은 Golang을 이용한 Go스러운 개발과 Message Service에 관심이 많습니다.\n진행 중인 프로젝트에서는 MSA와 K8s, ArgoCD, Github Action, Golang, Python등을 적용하며 공부 중입니다.\n  🥅 활동 내용  [2020.10~xxxx.xx] 경희대학교 커뮤니티 앱 쿠뮤 개발 중 [2020.04~2020.08] Megazone Cloud에서 DevOps 인턴 [2020.08~2020.09] 오픈스택 컨트리뷰톤: 오픈스택 팀 멘티 [2020.01~2020.03] 위시오피스 AWS 서버 관리자 [2019.06~] 한국 AWS Communitiy KRUG의 대학생 그룹 AUSG에서 3기로 활동 중 [2019.11] 경희대학교 소프트웨어 페스티발 웹/앱 최우수상 수상  "},{"uri":"https://umi0410.github.io/blog/aws/aws_eks_elb/","title":"EKS K8s에서 ELB(ALB, NLB) 제대로 설정하며 사용하기","tags":[],"description":"","content":"🐶 시작하며  본 게시글은 AWS 대학생 유저그룹인 AUSG의 활동 중 하나로서 본인(박진수)이 작성한 게시물을 포워딩한 것입니다.\n 데브옵스 인턴으로 근무한 지가 벌써 두 달이 되어갑니다. 이것 저것 배운 것이 많았던 시간이었는데, 그 중 꽤나 삽질을 했던 Kubernetes 와 ELB를 이용하는 부분에 대해 정리를 해볼까합니다. jenkins, spinnaker, argo, terraform, ansible, github action, \u0026hellip; 등등 다양한 내용을 경험할 수 있던 시간이었지만, 그 중 kubernetes에서 무슨 작업을 하던 빼놓을 수 없으면서 어딘가 깔끔히 그 흐름이 정리된 곳을 보기 힘들었던 service를 ELB에 연결하기에 대한 내용을 정리해보겠습니다.\n본 포스트는 EKS를 통해 K8s를 이용할 때를 기준으로 설명합니다.\n💁🏻‍♂️ EKS 에서 ELB를 사용해 서비스를 노출킬 때 유의사항들  🧐 : \u0026quot; ELB, NLB, ALB 대체 뭐가 다른 거야..?ㅜㅜ 쿠버네티스를 쓸 때는 어떻게 얘네를 지정하는 거지..? kubectl expose deploy {{deployment_name}} --type=LoadBalancer 하면 그냥 작동은 하던데\u0026hellip;\u0026quot;\n EKS에서 주로 사용하는 ELB는 L4의 NLB와 L7의 ALB 입니다. ALB가 L7에 대한 좀 더 다양한 설정이 가능하기 때문에 조건이 많기도 하고, AWS의 ALB만을 위한 alb-ingress-controller라는 녀석이 직접 Ingress의 설정들을 관리해주기 때문에 설정할 수 있는 옵션도 많습니다. 좋게 보면 많은 설정을 할 수 있고, 나쁘게 보면 초보자에겐 귀찮을 수 있습니다. NLB는 비교적 설정이 적고 따라서 설정해줄 수 있는 항목도 적습니다.\n쿠버네티스에서 다양한 작업을 하면서 다양한 controller을 접하게 되고, 그렇게 될 수록 annotation으로 많은 설정을 하게 됩니다. k8s를 처음 접할 때에는 annotation에 대한 정의로서 아래와 같은 문장을 접할 수 있고, 마치 기능과 크게 상관이 없을 것처럼 느껴지기도 하지만 사실 EKS를 비롯한 여러 서비스에서는 annotation을 이용해 중요한 설정 등을 기입할 수 있기 때문에 잘 설정해주어야합니다. ELB또한 모든 설정이 annotation으로 동작한다.\n \u0026quot; Label을 사용하여 오브젝트를 선택하고, 특정 조건을 만족하는 오브젝트 컬렉션을 찾을 수 있다. 반면에, annotation은 오브젝트를 식별하고 선택하는데 사용되지 않는다. 어노테이션의 메타데이터는 작거나 크고, 구조적이거나 구조적이지 않을 수 있으며, 레이블에서 허용되지 않는 문자를 포함할 수 있다.\u0026quot;\n ⚠️ ALB를 사용할 때 유의할 점  어떤 옵션들이 있고, 기본적으로는 어떻게 설정되는 지에 대한 이해가 있어야 오류 과정을 추적하기 쉬우므로 기본적으로 ALB를 AWS Console에서 사용해본 뒤에 설정할 것을 추천합니다.\n  alb ingress controller가 생성할 ALB가 사용할 서브넷을 discover하기 위해서는 올바른 태그가 달린 subnet이 존재해야한다. node 혹은 alb ingress controller에 연결된 service account가 alb를 제어하기 위한 iam permission이 부여되어야한다. internet facing한 alb를 만들지 internal한 alb를 만들지 고민해봐야한다. alb ingress controller의 log를 통해 작업에 대한 log를 볼 수 있다.  ⚠️ NLB, CLB를 사용할 때 유의할 점 https://kubernetes.io/ko/docs/concepts/services-networking/service/#aws-nlb-support\nhttps://docs.aws.amazon.com/ko_kr/eks/latest/userguide/load-balancing.html\n NLB, CLB가 사용할 서브넷을 설정하기 위해서는 올바른 태그가 달린 subnet이 존재해야한다. 어느 부분에선가 NLB, CLB를 제어하기 위한 iam permission이 부여되어야한다. (어느 부분인지 확실히는 모르겠음. 따로 설정안해도 동작하는 것을 보아 worker node가 갖는 iam role에 permission이 붙어있을 것으로 예상됨)  🌎 ALB를 사용해 서비스를 노출시키는 방법  😊 ALB는 K8s에 친숙하지 않으신 분들께는 다소 진입장벽이 있을 수 있습니다. 그냥 서비스를 노출시킬 때는 굳이 사용할 필요 없는 Ingress 라는 오브젝트도 관리해야하고, alb-ingress-contoller라는 녀석도 배포해야하며 설정이 다양하기 때문이죠! 💦\n K8s에서 EKS를 사용해 ALB를 이용하고싶은 경우 alb-ingress-controller을 배포한 뒤, Ingress를 통해 사용할 alb에 대한 rule을 설정을 해주어야합니다.\nhttps://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/controller/setup/ 의 내용을 버릴 부분이 하나도 없습니다. 위 링크를 통해 alb-ingress-controller에 대한 개념을 잡고 배포해봅니다. alb-ingress-controller.yaml의 인자를 적절히 수정해주어야합니다.\nALB가 아닌 k8s cluster 상에서 L7 LoadBalancer를 이용하는 경우에는 nginx ingress controller등을 이용하며 nginx 에 적용할 rule을 Ingress라는 K8s Object를 통해 설정합니다. ingress controller의 설정에서 자신의 class name을 적어주고, Ingress에서는 어떤 class name의 ingress controller에서 자신(Ingress이자 Rule)을 적용하도록 할 지를 annotation을 통해 설정하거나 ingressClassName이라는 필드를 통해 설정합니다.(ingress 설정에 대한 참고 - https://kubernetes.io/ko/docs/concepts/services-networking/ingress/#인그레스-클래스)\n이와 같은 경우에는 ingress controller가 직접 ingress에 명시된 rule을 이용했지만, alb-ingress-controller의 경우는 alb-ingress-controller가 nginx-ingress-controller처럼 직접 웹서버의 역할을 하는 것이 아닌, ingress에 명시된 rule을 이용하는 ALB를 생성하고 관리하는 역할을 한다는 것입니다. 이 부분이 처음에는 다소 헷갈리게 느껴질 수 있기에 길게 서술해보았습니다.\n실제로 ingress를 생성한 뒤 앞에서 배포한 alb-ingress-controller의 log를 보면 alb를 관리하기위한 여러 작업을 수행중인 모습을 볼 수 있습니다.\n그럼 이제 실제로 alb ingress controller을 통해 alb를 이용해보겠습니다.\nALB를 원활히 제어하기 위한 permission 부여  ALB iam 정책 참고  https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/alb-ingress.html https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.2/docs/examples/iam-policy.json    ALB를 제어하기 위해서는 aws의 리소스에 대한 어떠한 permission이 필요합니다. node에 부여할 수도 있고, IAM User에 부여한 뒤 alb ingress controller의 설정에서 해당 IAM User의 Key를 부여할 수도 있고, Service account와 IAM Role을 OIDC(OpenID Connect를 이용해 Service account와 IAM Role을 연결시키는 작업)를 이용해 엮은 뒤, alb ingress controller pod에 해당 Service Account를 부여할 수도 있지만 뒤의 방법들은 좀 튜토리얼치고 투머치한 감이 있기때문에, 간단히 node에 Permission을 부여하도록하겠습니다.\nworker node들이 이용하는 IAM Role에 Policy를 추가한 모습.\nalb가 사용할 subnet에 적절한 태그 달기 https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/alb-ingress.html 에 나와있듯이 ELB가 이용하는 서브넷을 자동으로 설정되도록 하기 위해서는 사용하고자 하는 서브넷에 아래와 같은 태그들을 달아주어야한다.\nkubernetes.io/cluster/\u0026lt;cluster-name\u0026gt; = shared | owned # Required kubernetes.io/role/internal-elb = 1 | \u0026#34;\u0026#34; # Optional, for internal alb kubernetes.io/role/elb = 1 | \u0026#34;\u0026#34; # Optional, for internet-facing alb a | b와 같은 표현은 a 나 b중 한 값을 가져야한다는 의미로 표현한 것입니다.\ninternet facing ALB만 이용할 것이기 때문에 kubernetes.io/role/internal-elb tag는 생략하고 태그를 달아준 모습.\nsubnet에 ALB를 사용하기 위한 태그를 제대로 달아주지 않을 경우 alb-ingress-controller 에서 아래와 같은 로그를 보게 됩니다. ALB가 생성되지도 않습니다.\ncontroller.go:217] kubebuilder/controller \u0026#34;msg\u0026#34;=\u0026#34;Reconciler error\u0026#34; \u0026#34;error\u0026#34;=\u0026#34;failed to build LoadBalancer configuration due to failed to resolve 2 qualified subnet for ALB. Subnets must contains these tags: \u0026#39;kubernetes.io/cluster/umi-dev\u0026#39;: [\u0026#39;shared\u0026#39; or \u0026#39;owned\u0026#39;] and \u0026#39;kubernetes.io/role/internal-elb\u0026#39;: [\u0026#39;\u0026#39; or \u0026#39;1\u0026#39;] alb ingress controller 배포하기.  슬슬 읽기 귀찮아질 타이밍입니다. \u0026lsquo;요놈이 IAM policy도 만들고, 서브넷에 엄한 태그를 달더니 이제는 하,,, 뭘 또 배포하라고 하는구나 아이고 내 눈아,,,\u0026rsquo; 싶겠지만, 좀 더 힘을 내어봅시다 🍻\n https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/controller/setup/ 를 참고하여 Deployment 내의 container의 args를 자신의 상황에 맞게 수정한 뒤 배포해줍니다.\n... args: - --ingress-class=alb # ingress의 annotation에 명시할 class name - --cluster-name=umi-dev # eks cluster name - --aws-region=ap-northeast-2 - --aws-api-debug=true 저는 위와 같은 식으로 설정해주었고, 잘 배포되었는지 확인해봅니다.\n$ kubectl get po -A | grep alb kube-system alb-ingress-controller-594f84b465-q4qjb 1/1 Running 0 106m 노출시킬 서비스 배포하기 간단하게 Nginx를 배포해보록하겠습니다.\napiVersion: v1 kind: Service metadata: name: ingress-test spec: selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80 nodePort: 30010 type: NodePort --- apiVersion: apps/v1 kind: Deployment metadata: name: ingress-test labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 ALB는 기본적으로 node의 Port를 AWS 상의 target group으로서 이용하기 때문에, ingress를 통해 노출시켜줄 서비스는 적어도 NodePort 타입으로 노출되어있어야 ALB ingress controller가 해당 서비스를 노출시킬 수 있습니다.(target type을 기본값인 instance가 아니라 IP로 설정하면, Pod의 IP로 트래픽이 흘러가게 할 수는 있습니다.)\n작동방식을 설명해보자면 ingress 는 service name과 service port를 설정으로 받습니다. alb-ingress-controller는 그러면 해당 service name, service port와 연결된 NodePort를 찾아서 ALB의 target group으로 등록시킵니다.\nIngress 배포하기 apiVersion: extensions/v1beta1 kind: Ingress metadata: name: \u0026#34;ingress\u0026#34; annotations: kubernetes.io/ingress.class: alb # the value we set in alb-ingress-controller alb.ingress.kubernetes.io/scheme: internet-facing spec: rules: - http: paths: - path: /* backend: serviceName: \u0026#34;ingress-test\u0026#34; servicePort: 80 https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/ingress/annotation/ 을 참고하여 Ingress를 작성해줍니다. [kubernetes.io/ingress.class는](http://kubernetes.io/ingress.class는) alb-ingress-controller에서 설정한 ingress.class를 적어주고, alb.ingress.kubernetes.io/scheme는 용도에 따라 internet-facing 혹은 internal을 적어줍니다. ingress 를 생성하기 전에 kubectl logs -f {alb-ingress-controller pod name}을 한 창에 띄워놓으면 ALB 생성 관련 로그를 쭈루룩 볼 수 있습니다.\n잘 설정되었다면 위와 같이 ALB가 생성될 것 입니다.\n$ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE ingress * bf1e76be-default-ingress-e8c7-1351183883.ap-northeast-2.elb.amazonaws.com 80 30s 인증서 설정을 통해 HTTPS 까지?!  ❤️ AWS Certificate Manager와 Ingress에 대한 annotation을 이용해 간단하게 HTTPS를 이용할 수도 있습니다! 직접하려면 도메인 소유 인증과 인증서, 비밀키 등을 모두 관리해야했는데 말이지요! 🐥\n 이런식으로 AWS의 Certificate Manager을 통해 발급받은 인증서가 있다면 이를 alb 에서 ingress에 annotation을 설정함으로써 사용할 수 있습니다.\nIngress의 annotation에 HTTPS및 인증서 관련 설정 추가해주기\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: \u0026#34;ingress\u0026#34; annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTP\u0026#34;: 80}, {\u0026#34;HTTPS\u0026#34;: 443}]\u0026#39; alb.ingress.kubernetes.io/actions.redirect-to-https: \u0026gt; {\u0026#34;Type\u0026#34;:\u0026#34;redirect\u0026#34;,\u0026#34;RedirectConfig\u0026#34;:{\u0026#34;Port\u0026#34;:\u0026#34;443\u0026#34;,\u0026#34;Protocol\u0026#34;:\u0026#34;HTTPS\u0026#34;,\u0026#34;StatusCode\u0026#34;:\u0026#34;HTTP_302\u0026#34;}} alb.ingress.kubernetes.io/certificate-arn: {Certificate Manger의 인증서 arn} ... http redirect 및 actions에 대한 내용은 저의 애교입니다. 궁금하신 분들은 한 번 적용해보시거나 알아보시면 어렵지 않게 알아내실 수 있을 겁니다! 😆\nRoute53에 ALB 추가해주기\nEKS에서 ALB를 사용하는 등의 작업을 하시는 분은 어느 정도 aws에 대한 이해가 있으리라 생각하고, ALB를 Route53을 통해 레코드로 추가하는 작업에 대한 설명은 생략하겠습니다.\nalb-ingress-controller로 AWS Certificate Manager의 인증서까지 사용한 모습.\n🌎 NLB를 사용해 서비스를 노출시키는 방법 CLB는 Deprecate 대상이라고 들었기도 하고, 굳이 써본 적이 없어 NLB로만 설명합니다. NLB는 ALB에 비해 사용이 간단합니다.\nIngress와 alb-ingress-controller를 사용했던 ALB와 달리 NLB는 서비스를 직접 노출시킵니다. 주로 Nginx Ingress Controller을 NLB에 연결해서 사용했던 기억이 납니다. NLB는 L4 LB로, Nginx를 주로 L7 LB로 사용하는 경우 이렇게 NLB를 사용합니다. ALB와 Nginx 모두 L7 LB로서 역할을 하기때문에 굳이 ALB를 사용할 필요가 없는 경우가 많았습니다.\nNLB를 통해 서비스를 노출시키기 위해선 annotaion중에서도 [service.beta.kubernetes.io](http://service.beta.kubernetes.io/)...형태의 annotation을 이용합니다. 사실 실제로 실무해서 사용해본 annotation은 거의 [service.beta.kubernetes.io/aws-load-balancer-type:](http://service.beta.kubernetes.io/aws-load-balancer-type:) \u0026quot;nlb\u0026quot; 뿐입니다. (이를 사용하지 않을 경우 디폴트가 CLB이기 때문에\u0026hellip;)\nNLB가 사용할 subnet에 적절한 태그 달기 ALB를 사용했을 때와 마찬가지로 NLB가 사용할 서브넷에 필수 태그를 달아줍니다. 기억이 안난 다면 글의 상단 ALB 파트를 참고!\nNLB로 노출할 서비스 생성하기 apiVersion: v1 kind: Service metadata: name: nginx-nlb annotations: service.beta.kubernetes.io/aws-load-balancer-type: \u0026#34;nlb\u0026#34; spec: selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80 nodePort: 30011 type: LoadBalancer 간단히 type을 LoadBalancer로 바꾸어주고, annotation에 어떤 ELB를 사용할지( NLB/CLB )만 적어주면 아래 그림처럼 손쉽게 NLB로 서비스를 노출 시킬 수 있습니다.\n인증서 설정을 통해 HTTPS까지?!  🌋: \u0026ldquo;그만\u0026hellip;. 아무도 안 궁금해\u0026hellip;\u0026rdquo; - 하지만 마지막까지 힘을 내서 EKS에서의 ELB를 정복해봅시다!\n apiVersion: v1 kind: Service metadata: name: nginx-nlb annotations: service.beta.kubernetes.io/aws-load-balancer-type: \u0026#34;nlb\u0026#34; service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp service.beta.kubernetes.io/aws-load-balancer-ssl-ports: \u0026#34;443\u0026#34; service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:ap-northeast-2:{{root}}:certificate/{{arn}} spec: selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80 nodePort: 30011 - protocol: TCP port: 443 targetPort: 80 nodePort: 30012 type: LoadBalancer backend-protocol은 tcp|tls 혹은 https|http로 설정이 돠는 듯합니다. 예를 들어 backend-protocol 로 tcp를 설정한 뒤 ssl-port로 443을 설정, 서비스의 spec에서의 포트로는 80과 443을 설정하면, 자동적으로 80은 tcp, 443은 tls를 이용하는 NLB listener로 설정되게 되는데 http,https도 마찬가지로 tcp,tls로 적절히 설정이 됩니다. 다만 http,https를 설정할 경우 X-Forwarded-For 헤더가 삽입된다고 합니다. (정확하지는 않아요\u0026hellip; 딱히 NLB의 backend protocol을 L7으로 설정하는 것이 NLB의 원래 스펙이 아니었던 점도 있고, L7을 이용하고 싶으면, ALB를 이용하는 것이 더 편하다고 생각이 들어서 따로 검증해본 적이 없기 때문에\u0026hellip; )\n⚠️단점이 하나 있다면 아직 http⇒https redirect가 불가능하다는 것인데, 이는 애초에 L4 LB를 이용하는 것과 L7 LB를 이용하는 쓰임에 대한 차이라고 생각을 하기 때문에 감수를 해야할 것 같습니다. 예를 들어 L4 NLB에 L7 nginx-ingress-controller을 연결하여 redirect는 nginx가 담당하도록하는 방식을 많이 이용하는 것 같습니다. NLB에서는 L4 의 뭔가 SSL/TLS한 작업을 하기 위함이고, L7의 https 작업이 주가 되는 것은 아니므로\u0026hellip;? 사실 이 부분은 잘 모르겠습니다\u0026hellip;💦💦 잘 아시는 분이 계시다면 알려주시면 감사하겠습니다. ㅜㅜㅜ( NLB에서 HTTP, HTTPS redirect가 안되는 이유 참고 - https://aws.amazon.com/premiumsupport/knowledge-center/redirect-http-https-elb/ )\n⭕️ NLB를 통한 HTTPS 서버 구축 결과 어쨌든 위의 annotation을 통해 올바르게 svc를 설정한다면\n$ kubectl get svc nginx-nlb LoadBalancer 10.100.180.174 ae27784521c4f4bcd96b22f2cca2358b-4bffb166fafa47f2.elb.ap-northeast-2.amazonaws.com 443:30014/TCP 37m  Route53 설정은 추가로 해주어야함. - 예시 nlb.umidev.net - ALIAS ae2778452xxxxxxxxxxxxxx.elb.ap-northeast-2.amazonaws.com  이렇게 service가 생성될 것이고, (service의 port로서 사용할 80,443 등은 service.spec에서 명시) 생성 후 A Record Alias로서 NLB의 DNS name을 넣어주면 사진과 같이 HTTPS 접속이 가능합니다!\n🐳 마치며 어차피 한 번의 검색이면 정보를 얻을 수 있는 모든 annotation이나 기타 설정에 대한 내용을 다루기 보단 나름 제가 실제로 쿠버네티스를 관리하는 데브옵스 인턴로서 일을 하면서 헷갈렸던 내용과 EKS에서의 ELB 관리에 대한 흐름을 위주로 설명하려 노력했고, 저의 삽질이 깃든 내용들입니다 ㅎㅎㅎ\n아는 범위 + 좀 더 조사하여 열심히 정리해보았지만, 부족한 부분이 있을 수도 있고 틀린 부분이 있을 수도 있을텐데, 보완해주실 내용이 있다면 말씀해주시면 열심히 검토해보겠습니다~! 감사합니다.\n🧙‍♂️글쓴이  박진수 - 👨‍👩‍👧‍👧AUSG (AWS University Student Group) 3기로 활동 중 관심사  Docker, Kubernetes 등의 컨테이너 기술 Argo, Spinnaker, Github action 등의 CI/CD 툴 Terraform, AWS를 통한 클라우드 인프라 구축   Blog - https://senticoding.tistory.com Email - bo314@naver.com  📚 참고 (References)  EKS의 Required subnet tags  https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/load-balancing.html https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/alb-ingress.html   Kubernetes Cloud provider aws - https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#aws ALB ingress controller install - https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/controller/setup/ ALB ingress Annotation https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/ingress/annotation/ NLB의 HTTPS redirect가 불가능한 이유 -https://aws.amazon.com/premiumsupport/knowledge-center/redirect-http-https-elb/ NLB service Annotations - https://kubernetes.io/ko/docs/concepts/services-networking/service/#aws-nlb-support Ingress의 class에 대해 - https://kubernetes.io/ko/docs/concepts/services-networking/ingress/#인그레스-클래스  "},{"uri":"https://umi0410.github.io/blog/golang/go-mutex-semaphore/","title":"Go 언어로 적용해보는 Computer Science - Mutex와 Semaphore","tags":[],"description":"Computer Science의 주요 파트 중 하나인 운영체제에서 이론적으로는 흔히 접할 수 있는 Mutex와 Semaphore가
실제로는 어떤 식으로 사용될 수 있을 지 Go 언어를 통해 직접 적용시키며 알아보았습니다.
","content":" Table of Contents  Mutex  Mutex를 이용한 Go program - Counter 💡 Mutex.. 그래서 언제 써요?   Semaphore  Mutex와 Binary semaphore의 유사한 점 Semaphore를 이용한 Go Program 1 - Counter Buffered channel을 이용한 Semaphore in Go 💡 Semaphore.. 그래서 언제 쓰나요? Semaphore을 이용한 Go Program 2 - 이미지 크기 변환기   마치며 참고     Go 언어로 적용해보는 Computer Science의 첫 번째 내용으로 OS 관련 내용 중 이론적으로는 흔하게 접할 수 있지만 실제 적용에 대한 내용은 찾아보기 힘들었던 Mutex, Semaphore에 대해 알아보려한다.\nMutex와 Semaphore은 각각의 추상적인 개념을 바탕으로 OS나 Go 등에서 사용될 수 있기에 세부적인 내용은 문맥에 따라 달라질 수 있다고 생각한다. 예를 들어 Go에서의 Mutex는 주로 sync.Mutex를 이용한 서로 다른 Goroutine의 동시 접근에 대한 제어를 의미하는 반면, 다른 프로그래밍 언어나 OS에서는 주로 서로 다른 Kernel thread나 Process에 대한 동시 접근 제어를 의미할 수 있다.\nMutex Mutex는 Mutual Exclusion의 줄임말로 상호 배제를 의미한다. 즉 서로 다른 워커가 공유 자원에 접근하는 것을 제한한다는 말이다. Go에서는 이 \u0026ldquo;워커\u0026ldquo;가 Goroutine이 되고, 컨텍스트에 따라 프로세스가 될 수도, 스레드가 될 수도 그 외의 다른 존재가 될 수도 있다. 공유 자원이란 여러 워커가 동시에 접근하는 자원을 말하고 이 공유 자원에 대한 동시 접근을 제한해 thread-safe하게 작업하고자 하는 영역을 Critical section이라고 한다. Cricical section은 Mutex가 Lock을 수행한 뒤 Unlock되기 전까지의 영역이며, Lock과 Unlock은 Atomic한 작업이기때문에 어떠한 경우에도 동시적으로 수행될 수 없다.\n이해하기 쉽게 예시를 통해 접근해보겠다. 원래 화장실 예제를 보고 굉장히 와닿았는데, 화장실보다는 피팅룸이 좀 더 청결한 것 같아서 피팅룸으로 예를 들어보겠다. 우리 귀여운 고퍼가 피팅룸을 한 개만 운영하는 옷가게에 방문했다고 가정해보자.\n 피팅룸 - Critical section 고퍼가 피팅룸에 들어가서 옷을 입는 작업은 하나의 Goroutine  동시에 여러 고퍼가 하나의 피팅룸에서 옷을 갈아입으려한다면 😟 난처한 상황이 발생할 것이다. 그렇기때문에 피팅룸에는 Lock, Unlock 기능이 존재해야하는데, 이 기능을 구현한다해도 Atomic하게 동작할 수 있도록 제대로 구현하지 않는다면 한 워커가 Lock을 하는 사이에 다른 워커가 동시에 Lock을 걸려할 수 있고, 그 경우 두 워커가 동시에 한 피팅룸(Critical Section)에서 작업을 하려할 것이다. 그렇기때문에 Lock, Unlock은 Atomic해야하며 이렇게 각 워커를 상호 배제시켜 동시에 작업할 수 없도록 하는 것이 바로 Mutex이다.\nMutex를 이용한 Go program - Counter Count = 0에서 시작해 동시적으로 +1을 10만 번, -1을 10만 번 수행하는 카운터 프로그램\n그럼 실생활에 비유한 Mutex는 이 정도로 마치고, Mutex가 어떻게 사용되는지 Go로 짠 간단한 Counter 프로그램을 통해 알아보자.\n Mutex를 잘 적용한 경우 - 몇 번을 수행하든 결과는 0. Mutex가 적용되지 않은 safe하지 않은 경우 - 결과가 0이 나오지 않을 수 있다.  특히 Go에서는 sync 패키지의 Mutex라는 type을 통해 간단하게 Mutex 기능을 이용할 수 있다. 동일한 Mutex struct를 참조한다면 같은 Key를 이용한다는 개념이고, mutex.Lock()을 호출한 뒤 mutex.Unlock()이 호출되기 이전까지가 Critical section이 되며 다른 goroutine들은 같은 Mutex(즉 키)에 의한 Critical section에 진입할 수 없다.\n내가 처음 Mutex를 처음 접했을 때 한 가지 헷갈렸던 것은 당시 Critical section이라는 개념이 없었기 때문에 Lock을 호출하기만 하면 런타임에 알아서 스마트하게 Lock과 Unlock사이의 변수들에 대한 접근 중 동시적인 접근만을 잠시 블락해주는 줄 알았는데, 사실 그 사이 변수들중 동시적으로 접근하려는 변수에 대해서만 잠시 블락해주는 게 아니라 Lock과 Unlock 사이를 Critical section으로 만드는 것이었다.\n자 그럼 Count라는 하나의 int형 변수에 동시적으로 접근을 하는 상황을 극대화하기 위해 여러 고루틴으로 작업해보겠다. +1을 1만 번하는 고루틴을 10개, -1을 1만 번하는 고루틴을 마찬가지로 10개 이용하겠다.\n전체 코드 참고: https://play.golang.org/p/xaLE1YkAdvd\n 우리가 일반적으로 익숙한 Sync 즉 동기적인 순차적 진행 시에는 당연히 10만번 +1, 10만번 -1 후에 결과가 0이었다. 동시적 접근을 수행하자 10만번 +1, 10만번 -1을 했지만 Count += 1 을 수행하던 중 또 다른 Count += 1이 수행되는 등의 원치않던 상황이 야기될 수 있기에 결과값이 0인 경우를 찾기 힘들다. 여러 Goroutine이 concurrent하게 진행하되 Count += 1, Count -= 1 과 같은 동시적 접근이 수행되어서는 안되는 영역은 Critical Area로 설정해 상호 배제적으로 작업되게 하기 위해 Critical section의 전/후에 sync.Mutex를 이용해 Lock, Unlock 기능을 넣어주자 기대했던 대로 0의 결과를 얻을 수 있었다.  💡 Mutex.. 그래서 언제 써요? 사실 Mutex라는 개념이 그렇게 어려운 것도 아니고, 사용법 자체가 어려운 것도 아니다. 나는 하지만 항상 \u0026ldquo;언제\u0026rdquo;, \u0026ldquo;왜\u0026rdquo; 써야하는지를 궁금해하는 편이다.\n Critical section에서 많은 시간이 소요되는 경우? ⇒ ❌ 위의 counter와 같은 작업은 대부분의 작업이 critical section 속에 있다고 볼 수 있다. 이 경우 동시적인 작업과 함께 Lock, Unlock을 하며 critical section을 관리하는 것보다 애초에 작업 자체를 동기적으로 수행하는 게 나을 수 있다. 왜냐하면 동시적 작업 속에서 Mutex를 통해 동기적으로 작업할 수 있도록 하는 것에서 오는 오버헤드는 분명히 존재하고, Critical section 밖에서 효율적으로 동시적으로 작업을 진행했다하더라도 critical section에서 병목이 발생해버려 전체적인 Throughput이 안 좋아질 것이다. Critical section에서는 적은 시간이 소요되고, 대부분은 동시적으로 작업이 가능한 경우? ⇒ ⭕ 예를 들어 어떤 API를 여러 번 호출한 뒤 그 응답 중 일부를 계속해서 더해 결과를 내는 Reduce 작업을 수행한다고 치자. 순차적으로 수행할 시 오랜 시간이 소요될 API 호출은 동시적으로 진행하고, 결과에 대한 Reduce 작업만 잠시 Critical section내에서 작업한다면 아주 좋은 성능과 함께 안전하게 작업할 수 있을 것이다!  Semaphore Mutex에서 대부분의 Lock이나 Critical section에 대한 내용을 설명했기 때문에 Semaphore에서 더 설명할 내용이 많지는 않다. Semaphore의 동작에 대한 간단한 예시는 위의 그림과 같은데, 5마리의 고퍼가 존재한다해도 동시에 접근할 수 있게하는 고퍼를 3개로 제한한다면 2마리의 고퍼는 피팅룸에 들어가지 못하고 블락된다. Locked와 Unlocked 상태 뿐인 Mutex와 달리 Semaphore는 임의의 개수를 세는 Counter처럼 동작해 임의의 개수의 워커만이 Critical section에 동시적으로 접근할 수 있도록 한다.\nCounter를 예로 들자면 10개의 워커가 존재한다해도 \u0026ldquo;5\u0026quot;를 세는 Semaphore는 한 워커가 Critical section에 진입할 때마다 Counter 값을 1씩 낮추고, Critical section을 탈출할 때마다 Counter 값을 다시 1씩 높인다. 만약 어떤 워커가 Critical section에 진입하려는데 Count 값이 0이라면 초기에 계획했던 대로 5개의 워커가 이미 동시적으로 작업중이라는 의미이므로 이 워커는 한 워커가 Critical section을 나오면서 Counter 값을 다시 1 증가시킬 때까지 Block된다.\nMutex의 Lock과 Unlock이 Atomic하기에 어떠한 경우에도 동시적으로 수행될 수 없었듯이 Semaphore의 Counting 작업 또한 Atomic해야하고 그래야만 Thread-safe한 counter로 동작할 수 있다. Semaphore가 수행하는 Counting 작업은 주로 try를 뜻하는 네덜란드어 Proberen의 앞 글자를 딴 P와 increment를 뜻하는 Verhogen의 앞 글자를 딴 V로 두 가지가 표현하는 듯하다. P는 Count 값이 0이 아니면 작업을 수행하겠다는 의미하며 만약 Count 값이 0이라면 0이 아닌 값이 될 때까지 wait했다가 0 아닌 값이 되면 count 값을 1 감소시키며 작업을 시작한다. V는 작업을 마치며 count 값을 다시 1 증가시키겠다는 의미이다.\nMutex와 Binary semaphore의 유사한 점 동시 접근 워커를 1개로 제한하는 Semaphore의 경우 Count 값이 0과 1 두 개로만 존재할 수 있는데 이를 Binary semaphore라고한다. 특히 이는 Locked와 Unlocked라는 두 가지의 상태만을 갖는 Mutex와 유사하다. 하지만 Binary semaphore는 mutex가 유사한 기능을 할 뿐 동일하지는 않다는 의견이 많다. 그 이유는 Mutex는 Lock 방식을 이용하고 Semaphore는 Signal(신호) 방식을 이용하기 때문이다. Lock 방식의 경우 Lock을 수행한 워커만이 Unlock을 할 수 있는 반면 Signal 방식은 try(작업 시도)를 수행한 워커가 아니더라도 increment(작업 완료) 신호를 보낼 수 있기에 서로 명백히 동작 방식이 다르다는 것이다. (하지만 이 부분에 대해서는 직접 실습해보지는 못했다.)\n*Semaphore를 이해하는 데에 있어 내가 착각해서 헤맸던 부분은 바로 Semaphore의 요점이 thread-safe한 count라고 착각*했던 것이다. 하지만 semaphore의 요점은 여러 워커에 대해서 thread safe한 count 기능을 제공하는 것이 아니라 임의의 숫자만큼의 동시적 접근을 허용하고, 그 이상은 Block 상태로 대기시킨다는 것이다.\nSemaphore를 이용한 Go Program 1 - Counter go에서 Semaphore를 이용하는 방법은 크게 2가지가 있는 것 같다. 아래 두 가지 방법 중 좀 더 Go스러운 channel을 이용해보겠다.\n Go 특유의 자료형인 channel을 이용하기 - channel은 여러 goroutine의 concurrent한 작업간 데이터 전송은 물론이고 동시적인 작업 중 데이터를 편리하게 동기화해주는 녀석이다. 따라서 동기적인 count와 유사한 기능을 내재하고있다. golang.org/x/sync/semaphore의 Weighted를 이용하기 - Semaphore의 P를 Acquire, V를 Release로 구현해 이용할 수 있게 했다.  전체 코드는 Mutex와 동일하며 마찬가지로 다음 링크로 참고해볼 수 있다: https://play.golang.org/p/xaLE1YkAdvd\nBuffered channel을 이용한 Semaphore in Go func DoSemaphore(maxConcurrent int64){ for i := 0; i \u0026lt; 5; i++{ Count = 0 wg := \u0026amp;sync.WaitGroup{} sem := make(chan struct{}, maxConcurrent) wg.Add(20) for i := 0; i \u0026lt; 10; i++{ go Add1Sem(wg, sem) go Sub1Sem(wg, sem) } wg.Wait() fmt.Println(\u0026#34;Concurrent goroutines + Semaphore\u0026#34;, maxConcurrent, \u0026#34;Result(Desired 0):\u0026#34;, Count, \u0026#34;\u0026#34;) } fmt.Println(\u0026#34;=====================================\u0026#34;) } func Sub1Sem(wg *sync.WaitGroup, sem chan struct{}){ defer wg.Done() for i := 0; i \u0026lt; 10000; i++{ sem \u0026lt;- struct{}{} Count -= 1 \u0026lt;-sem } } channel을 이용한 Semaphore 코드를 설명하기 위해 전체 코드 중 일부를 가져와보았다. Semaphore를 이용하기 위해 Unbuffered channel이 아닌 Buffered channel을 이용하는 이유는 다음과 같은 Unbuffered channel의 동작 방식이 Semaphore의 동작 방식과 동일하기 때문이다.\n Buffer의 크기만큼은 동시적으로 channel에 값을 넣으려는 시도가 허용됨. buffered channel이 꽉 찬 경우 채널에서 값을 꺼내지 않는 이상은 추가적으로 Channel에 값을 넣으려시도하는 goroutine은 Block됨.  (앞서 코드를 링크한 go playground에서 코드를 바로 실행해볼 수 있다.)\n동시 접근 워커를 1개로 제한하는 Semaphore는 Binary Semaphore로서 Mutex를 이용했을 때와 유사하다고 말했듯이 thread-safe하게 count 작업이 이루어져 예상되는 값이었던 0이 출력됨을 확인할 수 있다. 하지만 동시접근이 가능한 워커가 1개 이상이 되면 thread-safe하지 않게 되고, 동시접근 워커 개수가 많아질수록 결과가 더 부정확한 경향이 있는 것으로 나타났다.\n💡 Semaphore.. 그래서 언제 쓰나요?  Mutex는 thread-safe한 작업을 할 때 사용하면 되는 것 같은데 Semaphore는 개념은 알겠는데 언제 써야할 지를 잘 모르겠네요. 일정 개수만큼만 동시 접근을 허용하려는 경우가 있을까요?\n Semaphore을 사용하기 좋은 케이스는 동시에 접근할 수 있는 워커 수를 제한하는 경우이고, 이는 주로 전체 작업이 늘어지는 것을 방지하고자 하는 경우에 이용된다. 많은 작업을 동시에 수행하려하면 먼저 수행될 수 있는 작업은 먼저 수행되도록 하기보다는 전체적으로 모든 작업이 늘어지게 되고 CPU나 Memory 리소스를 많이 소모하게 되고 이는 서버의 안정성에도 좋지 않다. Semaphore를 이용해 동시 접근 워커 수를 제한하고자하는 케이스는 Go의 Worker pool 패턴을 이용하는 경우나 Pipeline pattern을 이용하는 경우와 유사하다.(처음엔 Go의 모든 Concurrent pattern들을 개별적으로 구별지으려했었는데, 공부하다보니 일정한 개수의 Worker를 이용하는 Worker Pool pattern, Channel을 기반으로 작업 내역을 쪼개어 실시간 처리하는 Pipeline, 한 채널, 여러 Goroutine을 이용하는 Fan-in Fan-out pattern 등등 다들 유사하고 연관이 되어있더라.)\nSemaphore로 동시 접근 Worker 수를 제한하지 않고 모든 Goroutine을 동시적으로 수행하는 경우엔 어떻게 될까?\nLogical Processor 개수를 훨씬 넘는 모든 Goroutine 동시적으로 작업을 진행할 경우 아무리 Goroutine이 concurrent한 작업 수행에 뛰어난 성능을 보인다할지라도 과하게 많은 수의 Goroutine은 성능 저하를 야기하지 않을까 예상했다. 하지만 user-level thread 혹은 green level thread의 일종인 Goroutine은 OS(혹은 Kernel) thread와 달리 Context switch로 인한 penalty가 거의 없어서인지 거의 Throughput 면에서의 성능 차이가 없었다.\n Kernel level thread는 OS가 스케쥴링을 담당하기 때문에 Go 프로그램이 뭐 어떻게 할 수 있는 게 아니지만 Goroutine은 프로그램이 실행되는 동안 Go 런타임이 스케쥴링을 담당한다. 같은 Kernel level thread에 속한 User level thread인 goroutine간의 switch는 cost가 거의 없다. 즉 goroutine이 많든 적든 Kernel level thread간의 context switch cost는 동일하다고 볼 수 있고, 해당 Kernel level thread에 속한 goroutine간의 context switch cost는 거의 없다.\n  참고: Kernel level thread에 대한 스케쥴링은 주로 Preemptive 방식을, User level thread에 대한 스케쥴링은 주로 Cooperative한 방식을 이용하지만 Goroutine은 User level thread임에도 Go 1.14 버전부터는 10ns를 기준으로 goroutine을 switch 할 수 있는 asynchronously preemptive한 스케쥴 방식을 지원한다고 한다. 하지만 Go 1.14가 릴리즈된 지 얼마되지 않아서인지, asynchorously preemptive scheduling에 대해서는 명확히 설명된 문서를 찾기 힘들었다. User level thread의 예로는 RxJava in Java, Coroutine in Kotlin, Goroutine in Golang이 있다.\n 하지만 Throughput 적인 측면보다는 Machine의 Resource 소모 측면에서는 모든 Goroutine이 동시적으로 수행되는 구조보다는 Semaphore을 바탕으로한 동시에 일정 개수의 워커만이 작업하는 Worker Pool 구조가 훨씬 Memory나 CPU 리소스를 적게 소모하는 듯 했다. 또한 100개의 동시 요청을 수행하는데 동일한 Throughput으로 약 10초가 걸린다고 치면, 요청당 goroutine을 생성하는 경우는 첫 번째 요청도 거의 10초가 걸린 반면 Semaphore을 이용한 경우는 먼저 온 요청은 대체로 빠르게 먼저 처리되는 경향을 보였다. 이 차이는 처음 요청을 보낸 사용자 마저 10초를 기다리게 할 것이냐, 0.1초만에 응답을 받도록 할 것이냐의 차이이다. 또한 전체 작업이 늘어지면 그 작업에 대한 메모리 점유가 지속되기에 메모리 측면에서도 비효율적이다.\nSemaphore을 이용한 Go Program 2 - 이미지 크기 변환기 마침 이번에 프로젝트에서 thumbnail 생성, image 크기 변환, hashed uri 생성 작업 등을 담당하는 image processing 서버를 개발하려했는데, image를 불러오는 I/O 작업 이후의 image processing은 CPU Bound 한 작업이기 때문에 동시적으로 동작하는 Goroutine이 일정 숫자(대게 Logical Processor 개수) 이상으로는 많아져봤자 크게 효율이 없을 것이라 예상했고, 그와 관련해 간단하게 이미지 크기 변환 프로그램을 하나 만들어 테스트 해보았다.\n약 2MB의 이미지에 대한 크기 변환 작업 요청이 동시에 30개 들어왔다는 가정을 했고, semaphore을 이용한 경우 동시적으로 최대 4개의 worker(goroutine)이 작업을 수행할 수 있게, concurrent를 이용한 경우는 30개의 요청 모두 동시적으로 작업을 하는 경우이다.\n(코드 참고(인터넷 액세스를 하는 경우 Playground에서 동작하지는 않는듯하다): https://play.golang.org/p/Vfyw6uCOIuL)\n 사실 Logical Processor 8개, RAM 16GB의 개인 노트북으로는 그 차이가 많이 나지는 않아서 AWS EC2 t2.micro instance에서 성능을 테스트해봤다.\n ubuntu@ip-172-31-12-2:~$ ./sem semaphore 변환하고자하는 Image들을 메모리에 Load했습니다. 2021/01/18 20:28:13 Elapsed: 1.974380959s ... 생략 2021/01/18 20:28:26 Elapsed: 1.159552779s 2021/01/18 20:28:26 Elapsed: 1.016627316s 2021/01/18 20:28:26 Total elapsed: 14.959516003s ubuntu@ip-172-31-12-2:~$ ./sem concurrent 변환하고자하는 Image들을 메모리에 Load했습니다. Killed 놀랍게도 동시적으로 30개의 요청을 보내는 경우에 30개의 모든 goroutine이 동시에 작업을 시도할 때에는 과도한 리소스 사용으로 인해 아예 OS가 프로세스를 Kill해버렸다. 즉 Semaphore로 동시 접근 Worker 수를 제한하지 않는 경우 t2.micro 인스턴스로 돌리는 image 서버에 동시에 30개의 image resizing 요청이 들어오면 process가 죽어버린다는 말이다\u0026hellip;!\n또한 앞서 말했듯이 동시에 요청하는 사용자가 많아지더라도 Semaphore을 이용한 경우는 나중에 들어온 요청일 수록 처리가 늘어지지만, Semaphore를 통해 동시 작업을 제한하지 않는 경우는 전체 작업이 늘어진다.\n이렇게 Semaphore는 동시 접근 워커 수를 제한하여 전체적인 Throughput 측면보다는 리소스 소모적인 측면과 먼저 처리될 수 있는 작업은 먼저 처리되도록 할 수 있다는 면에서 장점이 있음을 확인할 수 있었다.\n마치며 Mutex나 Semaphore의 개념이나 설명과 같은 이론적인 내용은 구글링을 통해 어렵지 않게 얻을 수 있는 흔한 지식인 반면, 정확히 언제 쓰면 좋을지, 언제 쓰일 수 있을지와 같은 실용적인 내용은 찾아보기 어려웠기때문에 Go를 통해 Mutex와 Semaphore을 이용해보는 간단한 프로그램을 만들어 실습해보았다.\n예제 프로그램으로서 코드를 간결하고 읽기 쉽게 깔끔하게 제공해보고자했는데, 그러기 쉽지 않았던 것 같아 아쉽다. 다음엔 기회가 되면 go의 benchmark test를 이용해보면 어떨까싶다.\n참고 Goroutine과 Goroutine scheduling에 대해 - https://thegopher.tistory.com/3\n화장실에 비유한 뮤텍스와 세마포어 - https://worthpreading.tistory.com/90\nsemaphore in Go https://medium.com/@deckarep/gos-extended-concurrency-semaphores-part-1-5eeabfa351ce\ncooperative vs preemptive - https://medium.com/traveloka-engineering/cooperative-vs-preemptive-a-quest-to-maximize-concurrency-power-3b10c5a920fe\npreemptive scheduling in go - https://blog.puppyloper.com/menus/Golang/articles/Goroutine과 Go scheduler\n"},{"uri":"https://umi0410.github.io/blog/golang/","title":"Go","tags":[],"description":"다른 언어에선 겪어보지 못했던 특이한 경우나 다른 언어와 Go의 비교 등 주로 Go를 공부하고 적용하면서 겪게된 다양한 경험에 대해 적어보려합니다.","content":"다른 언어에선 겪어보지 못했던 특이한 경우나 다른 언어와 Go의 비교 등 주로 Go를 공부하고 적용하면서 겪게된 다양한 경험에 대해 적어보려합니다.\nGolang의 기본 개념 설명은 이미 많은 자료를 찾아볼 수 있기에 이에 대한 자세한 설명은 대체로 생략할 예정입니다. Golang의 기초 관련 내용을 배워보실 분들께는 제가 번역에 기여한 A Tour of Go이나 Mingrammer님께서 번역해주신 Go by Example을 를 읽어보시면 많은 도움이 되리라 생각합니다!\nList of Contents  Go 언어로 적용해보는 Computer Science - Mutex와 Semaphore  Computer Science의 주요 파트 중 하나인 운영체제에서 이론적으로는 흔히 접할 수 있는 Mutex와 Semaphore가 실제로는 어떤 식으로 사용될 수 있을 지 Go 언어를 통해 직접 적용시키며 알아보았습니다.  Go 언어로 적용해보는 Computer Science - Cache  컴퓨터 구조를 수강하며 이론적으로 배웠던 CPU와 메모리 사이의 캐시가 어떻게 동작하는지 간단히 알아보고 그 효과를 간단한 프로그램을 통한 벤치마킹으로 알아봤습니다.  개발 썰 - Go Memory Leak(메모리 누수) 관련 이슈  Go로 이미지 프로세싱 서버를 만들면서 Memory Leak 이슈를 맞이했고 그 이슈를 해결하기 위해 했던 삽질과 그 결과에 대한 썰을 적어보았습니다.  Go vs Java - Go에서의 객체 지향  주로 Java로 설명되는 객체지향적인 특징들을 Go에서는 어떻게 적용할 수 있는지 알아봤습니다.  Go의 Pipeline pattern. 언제 사용해야할까? - Golang concurrent patterns  Golang의 Concurrent pattern 중 하나로서 Channel을 바탕으로 커뮤니케이션하며 작업을 진행하는 패턴인 Pipeline pattern에 대해 간략해 소개해봤고, 언제 사용해야할지 그 use case에 대해 알아보았습니다.  "},{"uri":"https://umi0410.github.io/experiences/megazone-cloud/stargate-infra/","title":"Stargate라는 인프라 구축기","tags":[],"description":"Terraform을 이용해 구축한 인프라. 그 위에 Jenkins, Gitlab, Spinnaker 그 외 자체 마이크로 서비스
등등을 배포한 이야기
","content":" Table of Contents  Stargate라는 개발 인프라 구축기  Terraform으로 처음 접해 본 IaC Terraform 단점 Terraform 장점 개발 관련 다양한 서비스 배포   인프라 구축에 대한 정리     Stargate라는 개발 인프라 구축기 chart by Jinsu Park\n저희 팀의 개발 인프라는 위와 같습니다. 마침 제가 입사할 쯤이 기존에 존재하던 인프라를 새로운 환경으로 이전해야할 시점이었습니다. 덕분에 저는 저희 개발 환경을 처음부터 구축하고, 우리의 서비스를 배포해보고, 그 후 운영하면서 여러 경험들을 할 수 있었습니다.\nTerraform으로 처음 접해 본 IaC  친구: IaC가 뭔 지 알아? 써봤어?\n본인: 뭐 인프라를 코드로 관리한다는 건데, 나도 몰라 ㅋㅋ\n본인: 아마 너무 고급 기술이라 대학 졸업할 때 까진 못 써볼 듯?\n 위의 대화는 제가 입사하기 약 일주일 전에 나눴던 대화인데, 입사 후에는 어느 덧 IaC와 꽤나 친근해진 것 같아 유머삼아 유머로 가져와봤습니다. 메가존 클라우드의 클라우드 원팀에서 일하기 전까지는 IaC는 저와는 거리가 먼 토픽이었고, Kubernetes 또한 minikube로 몇 가지 Object들을 배포해본 것이 다였습니다. 하지만, 저는 저희 팀에서 근무하게 되면서 terraform을 통해 위의 차트에 그려진 모든 인프라를 구축하고 파이프라인을 구축하게 됩니다.\nterraform을 사용하며 느꼈던 점은 \u0026lsquo;장점만 존재하는 기술은 드물 것이다\u0026rsquo;라는 점입니다. 분명 대부분은 장점이 있으면 그에 따른 단점이 존재할 것이고, 개인적으로는 테라폼도 장단점이 공존하고있다는 느낌을 받았습니다. 저의 주관적인 느낌에 따른 가장 큰 장점과 단점을 몇 개만 설명해보겠습니다.\nTerraform 단점 우선 단점부터. \u0026ldquo;사소한 인프라 변경도 코드에 반영하려면 문서를 찾아봐야하고, plan 내용을 검토해야한다\u0026rdquo;.\n저희는 평소에는 EKS의 로그를 켜놓지 않았습니다. 그런데 언젠가 EKS 로그를 CloudWatch를 이용해 분석해야한 적이 있는데, AWS 콘솔에서 바로 눈 앞에 로그 설정 칸이 있었음에도 EKS 로그를 설정하는 terraform Docs를 찾아본 뒤 plan을 분석한 뒤 apply 했어야합니다. 물론 아마도 로그 설정 쯤이야 잠깐 콘솔로 설정했다가 콘솔로 해제하면 그 한 번쯤은 문제가 없었겠지만, 그런 식으로 이번 한 번만, 이것쯤이야 하면서 코드가 아닌 매뉴얼로 직접 인프라 형상을 제어하는 경험이 쌓이게되면 형상이 깨져서 다시 형상을 맞추기 힘들수도 있고, 애초에 그런 수작업이 많이 들어가야하는 경우가 있다면 오히려 IaC를 이용할 필요가 없다고 생각했기 때문에 최대한 인프라는 terraform code로만 작업한다는 저의 원칙을 지키기 위함이었습니다.\n즉 자동화보다는 수작업이 편한 경우는 굳이 IaC라는 컨셉을 이용하는 것이 더 불편한 경우도 존재할 수 있을 것 같다는 생각이 들었습니다.\nTerraform 장점 \u0026ldquo;내가 사용하고 있는 인프라 전체를 한 눈에 보기 쉽다\u0026rdquo;\n물론 클라우드 서비스 하나에 대한 정보를 보고싶으면, 웹 콘솔에 들어가서 확인하는 것이 편하겠지만, 내가 이용하고 있는 AWS내의 모든 클라우드 서비스에 대한 정보나 설정을 보기에는 terraform 코드나 output, state 등이 더 알아보기 편할 수 있습니다.\n\u0026ldquo;혹시 장애가 생긴 경우 그 원인을 추적하기 쉽다.\u0026quot;\n수작업으로 클라우드 인프라를 관리하는 경우에 자신이 모르는 어떤 변동사항이 있고, 그 변동사항이 어떤 버그를 야기하고 있다면, 수작업으로 작업을 진행하는 경우에는 그 변동사항을 알아채고 트러블슈팅하기 쉽지 않을 것입니다.\n하지만 테라폼 코드를 통해 인프라를 관리하면 변동사항을 테라폼이 알아서 잡아주고, 혹은 코드에 대한 커밋 내역 등을 통해 변동 사항을 체크해 볼 수도 있을 것입니다. 따라서 그 변동사항에 맞는 트러블 슈팅을 하기 쉬울 것 입니다.\n개발 관련 다양한 서비스 배포 테라폼으로 인프라를 구축한 뒤에는 위의 차트에서 오른쪽에 작게 Stargate 라는 곳에 적힌 서비스들을 배포했습니다. 이 부분에 대해서는 너무나도 하고싶은 얘기가 많지만, 지루해질 수 있으니 그 중 기억에 많이 남거나 애착이 가는 서비스들에 대한 리뷰를 간단히 적어보겠습니다.\n Nginx Ingress Controller  로컬에서 minikube로만 개발하다가 처음으로 Ingress를 사용하게 되었습니다. helm을 이용한 게 아니라, helm으로 만들어진 manifest를 수동으로 하나하나 설정해서 설치하고 관리했습니다. 설정이 꽤나 복잡했었기에 설정을 많이 변경하는 경우 helm으로 설치하는 건 어떨까싶습니다\u0026hellip; 일하면서 Nginx로 L7 로드밸런싱 뿐만 아니라 L4 로드밸런싱도 몇 번 다룰 일이 있었고, 여러 Nginx Ingress Controller을 배포할 일도 있었고, gRPC 통신을 위한 설정을 해야할 일도 있었는데, 점점 수작업으로 하다보니 설정이 너무 복잡해져서 헷갈렸던 적이 있습니다. 뭔가 단점만 적은 것 같은데, K8s의 Ingress Controller로서 성능적인 부분은 제가 잘 모르겠지만, 크게 불편함 없이 잘 사용했습니다.   ALB Ingress Controller  서비스에 대해 직접 L7 로드밸런싱을 수행할 경우 이용합니다. 헷갈렸던 점은 보통은 Nginx Ingress Controller는 사실 Ingress는 설정이고, 컨트롤러가 실제로 로드밸런싱을 수행하는데, ALB는 ALB Ingress Controller가 Ingress를 통해 ALB를 만들고 실제 로드밸런싱은 ALB Ingress Controller가 아니라 ALB가 한다는 점이었습니다. 설정에 따라 다르지만 기본적으로는 노출시키고자 하는 서비스는 NodePort급 이상으로 서비스가 열려있어야하는데, 실수로 ClusterIP로 노출시켜 ALB가 서비스를 제대로 찾지 못한 적이 종종 있었습니다. 주의..!   Cert Manager  자동으로 cert를 발급해주고 갱신해주는 서비스입니다. Let\u0026rsquo;s Encrypt를 이용했고 무료입니다. 정말 정말 편리했습니다! DNS, HTTP Challenge, TLS에 대해 많이 배울 수 있었습니다. TLS 통신 과정은 공부해도해도 정확한 순서는 까먹게 돼서\u0026hellip; 다시 공부해봐야겠습니다.   Jenkins  편한데, 관리하기 귀찮을 수 있을 것 같습니다. 결정적으로 코드로 관리할 수도 없는데, UI도 그렇게 직관적인지는 잘 모르겠습니다.   Spinnaker  정말 편리합니다. Jenkins test 결과를 CD에 이용할 수도 있고, 편리하고 다양한 문법, 파이프라인 종류, 직관적인 UI/UX. 사실 Argo를 도입하고 싶었지만, Argo는 약간 가벼운 쿠버 환경에 대한 배포용 같은 느낌, IaC는 적극 도입되었으나 현실에 도입은 쉽지 않은 기술적 이상향 같은 느낌이 컸고, 현실적으로는 좀 더 안정적인 Spinnaker를 유지하게 됐습니다. 좀 느리다고 생각했는데, 이 글을 쓰면서 생각해보니 개발에서는 K8s Deployment의 Grace Period 때문인 것 같아 그걸 좀 줄여볼 껄 싶습니다. 즉 다시 생각해보니 별로 느리지 않은 듯합니다. 배포 자체는 안정적이고 좋은데, 설정할 때 버그가 종종 있습니다. 버그인지 제 실수인지는 모르겠지만 Pipeline stage 에서 Bake라는 모드가 \u0026ldquo;우와 신박하다!\u0026rdquo; 라고 생각했지만, 설정이 제대로 되지 않거나, OAuth login 실패에 대한 처리, Artifact 경로 설정이 너무 번거로운 점 등의 단점이 있었습니다.   Keycloak  우리의 개발툴들에 대한 인증을 사내 계정으로 연동시켜주는 Single Sign On 기능을 지원했습니다. 새로운 팀원은 각 서비스에 별도의 가입 과정 없이, 앱 레벨에서 따로 권한 부여가 필요한 것이 아니라면 사내 계정으로 바로 이용이 가능했습니다. 문서가 제대로 정리된 게 다소 부족한 느낌이었어서 세밀한 설정이나 정확한 작동원리를 파악하기는 쉽지 않았던 점이 조금 아쉽습니다.    인프라 구축에 대한 정리 내용이 너무 길어지면 읽기 힘들 것 같아 최대한 느낀 점 위주로 간단히 정리해보려고 노력해보았습니다. 인턴으로 일을 하기 전에 가벼운 마음 속 목표가 하나 있었습니다.\n \u0026lsquo;어떤 걸 어느정도 사용해보면 그것에 대한 주관적인 평가를 내릴 수 있는 사람이 되고싶다.'\n 누군가 \u0026ldquo;도커가 설치하기 정말 쉽더라구요.\u0026rdquo;, \u0026ldquo;minikube 써보니 정말 편리하더라구요.\u0026rdquo;, \u0026ldquo;Jenkins 보다는 travis가 편리더라구요.(혹은 그 반대)\u0026rdquo; 이런 얘기가 나와도 과거에는 공감할 수도, 제 주관적인 평가를 내릴 수도 없었습니다. 저의 경험으로 구성된 모집단이 없었기 때문입니다. 하지만 저희 클라우드 원 팀에서 근무하면서 어떻게 보면 깊이는 다소 얕았을 지 몰라도 정말 다양한 서비스를 접하고 다채로운 경험을 할 수 있었던 것 같습니다! 덕분에 이제는 어떤 서비스를 접하든, 새로운 언어를 접하는 저만의 느낌을 가질 수 있고, 의견을 말할 수 있을 것 같습니다!\n"},{"uri":"https://umi0410.github.io/experiences/megazone-cloud/","title":"메가존 클라우드 데브옵스 인턴 후기","tags":[],"description":"안녕하세요. 이번에 메가존 클라우드에서 데브옵스 인턴으로 근무를 하게되었던 박진수입니다.
너무 좋은 팀원들과 많은 경험을 하며 단시간에 성장할 수 있었고, 일하는 동안 매 순간 순간이 너무
행복했었기에 이렇게 인턴 후기를 작성해봅니다.
","content":" Table of Contents  지원동기 Megazone Cloud: CloudOne Team? 진행했던 업무들 느낀 점     안녕하세요. 이번에 메가존 클라우드의 클라우드 원 팀에서 데브옵스 인턴으로 근무를 하게되었던 박진수입니다. 너무 좋은 팀원들과 많은 경험을 하며 단시간에 성장할 수 있었고, 일하는 동안 매 순간 순간이 너무 행복했었기에 이렇게 인턴 후기를 작성해봅니다. 한 학기를 쉬고 2020.04.13~2020.08.31 까지 인턴으로 근무 했고, 다시 2020-2학기부터는 학교 복학을 하게되었습니다. 기술적인 얘기는 이곳 저곳에 많으니 항상 제가 가장 중요시하는 \u0026ldquo;느낀 점\u0026rdquo; 과 \u0026ldquo;배운 점\u0026rdquo; 을 위주로 적어보겠습니다!! 글은 위의 목록의 순서로 진행해보겠습니다.\n지원동기 저는 AWS와 배포에 관해 흥미가 있어 종종 AWSKRUG 라는 그룹의 소모임에 가서 핸즈온이나 세미나에 참여하곤 했는데요. 그곳에서 저희 팀원들을 만나게 되었고, 그것이 인연이 되어 채용으로까지 이어질 수 있었습니다.\n평소 AUSG 라는 대학생 AWS 사용자 모임의 일원으로서 참여하며 AWS 를 이용한 클라우드 인프라, CI/CD 파이프라인 구축을 통한 자동화, 컨테이너 등에 관심이 많았는데, 마침 메가존 클라우드의 저희 CloudOne 팀에서는 팀의 모든 마이크로서비스 및 기타 서비스들을 EKS라는 AWS의 Managed Kubernetes Cluster Service 위에 자동화시켜 배포를 하고있었고, 개발 인프라를 이전하려던 참이었기에 저의 관심사와 향후 목적에 잘 부합했습니다.\nMegazone Cloud: CloudOne Team?  Megazone Cloud: 국내 최초 \u0026amp; 최대 AWS 프리미어 컨설팅 파트너, AWS 컨설팅, 구축, 운영 및 빌링 서비스 제공. 메가존클라우드는 2009년부터 클라우드를 차세대 핵심 사업으로 성장시키며 ‘클라우드 이노베이터(Cloud Innovator)’로서 고객님들의 클라우드 전환의 과정마다 최선의 선택을 하실 수 있도록 다양한 서비스를 제공하고 있습니다. - 출처(https://www.megazone.com/)\n SpaceONE Preview\n메가존 클라우드에 대한 설명은 위와 같고, 제가 근무했던 CloudOne Team에 대해 간단히 소개해보겠습니다. CloudOne 팀은 SpaceONE 이라는 Multi Cloud 에 대한 오픈소스 CMP(Cloud Management Platform)를 개발하는 팀입니다. AWS, Azure, GCP, IDC에 대한 다양한 관리, 모니터링을 지원하는 것이 목표이고, 빠르게 발전되어나가고있습니다! 주요 내용들은 아래와 같은 키워드를 갖고 있습니다.  AWS EKS를 이용한 Kubernetes 환경, 마이크로서비스 아키텍쳐 Composition API를 이용하는 최신 Vue 기술 Terraform을 통한 IaC 자동화된 CI/CD, 다양한 배포 전략, 인프라 모니터링 HTTP2를 이용한 gRPC API  진행했던 업무들  링크 형식이므로 링크를 눌러 읽어주시면 됩니다.\n  2020.04: Stargate 라는 개발 인프라를 구축  terraform 을 통한 개발 인프라 구축 EKS 클러스터에 jenkins, spinnaker, grafana 등등의 개발 도구들을 배포함 종종 발생하는 장애 상황을 멘토님과 트러블슈팅함   2020.05: 개발 CI/CD 파이프라인 구축  Experiment 환경에 배포 =\u0026gt; Test, CI =\u0026gt; Development 환경에 배포를 자동화 Spinnaker와 Jenkins, Github Action을 이용한 자동화 파이프라인 구축   2020.06: Argo Project들을 PoC  argo cd, argo(workflow), argo-event 등의 다양한 프로젝트에 대한 PoC를 진행   2020.07: spaceone-helm Helm3 Chart 개발  개별로 배포되고 운영되던 우리 팀의 서비스인 SpaceONE을 패키지로 배포할 수 있게해주는 Helm Chart를 개발   2020.08: spacectl 설계, 개발 참여  우리 팀의 서비스인 SpaceONE에 대한 API 작업을 수행하는 CLI 도구에 대한 설계와 개발에 참여함.    느낀 점 그 동안 혼자 개발 및 평소 관심분야였던 클라우드, 컨테이너 등등의 주제로 공부해왔었는데, 과연 이 내용들이 정말 실무에 도움이 될 지, 제가 잘 나아가고 있는 건지 확신이 들지 않았습니다. 하지만 CloudOne팀에서 다양한 경험을 하면서 \u0026lsquo;제가 공부해온 길이 틀리지만은 않았구나\u0026rsquo;라는 느낌을 받을 수 있었고, 보완해야할 부분들은 보완하면서 불확실한 자세가 아닌 확신과 열정을 가진 자세로 좀 더 몰두할 수 있을 것 같습니다!\n배우고 느낀 내용이 너무 많아 글이 다소 길어졌습니다. 기술을 거부감 없이 접하되 기술이 다가 아닌, 팀원들을 위해 솔선수범하는 개발자, 팀원들의 생각을 읽어줄 수 있고, 잘 이해해줄 수 있는 개발자가 되기 위해 노력해야겠단 생각이 듭니다. 쉽진 않겠지만, 나아가고 싶은 방향은 정해진 것 같아 다행입니다!\n"},{"uri":"https://umi0410.github.io/experiences/megazone-cloud/ci-cd-pipeline/","title":"Github Action, Spinnaker을 이용한 CI/CD 파이프라인 구축기","tags":[],"description":"Github Action과 Spinnaker을 이용한 CI/CD 파이프라인 구축기
","content":" Table of Contents  Github Action을 사용하게 된 배경 Github Action vs Jenkins 그래서 어떤 CI/CD를 자동화하였나요? Github Action을 다뤄보면서 느낀 점     Github Action을 사용하게 된 배경 저희 팀은 원래 CI용으로 Jenkins를 사용했습니다만 팀이 개발 중이던 서비스가 오픈소스가 목표인 프로젝트였고, Github Action이 빠르게 발전해나가면서 비용도 무료가 되었고, 좋은 Action들이 많이 생겨나고 있었기에 어느 정도 프로젝트 구조가 잡힌 뒤에는 Github의 Public Repository로 프로젝트를 관리하고 Github Action을 CI 도구로 채택하게되었습니다. Integration Test를 제외한 모든 빌드 및 일부 배포를 Github Action을 이용하게되었고, 대부분의 배포에는 사용하던대로 Spinnaker을 이용했습니다.\nGithub Action vs Jenkins 이 부분 역시 느낀 점 위주로 요약해보겠습니다.\n   Github Action Jenkins     내가 서버를 관리할 필요가 없다. 내가 직접 master을 띄우고, slave를 띄워우고, 관리해줘야한다.   VM이 배치되어 제공되는 데에 좀 시간이 든다. 내 커스텀 이미지를 사용할 수가 없다보니 반복되는 패키지 설치나 환경 설정을 매번 해야해서 좀 느리다. 내가 필요한 Plugin을 설치해놓거나 설정을 입력해 놓으면 매번 빌드할 때 따로 제공할 필요 없다.   Code로 관리가 가능하다! 처음엔 조금 어려울 수 있지만, 알고 나면 쓰기 너무 쉽다. 처음 접한 사람이 사용하기에는 Github Action보다 편리할 수 있지만, 그렇다고 훌륭한 UI/UX는 아닌 듯 하다.   요즘 핫하고, 빠르게 발전 중이다. 구식이다.    비용이 저렴하게 풀리고 있고(퍼블릭의 경우 아마 무조건 무제한 공짜), 원래 지원하지 않던 매뉴얼 트리거가 2020.07부터 제공되기 시작했다는 점, 누구든 오픈소스로 Github Action에서 남들이 사용할 수 있는 action 을 만들 수 있다는 점등을 보고 Github Action이 빠르게 발전 중이라는 생각이 들었습니다.\n그래서 어떤 CI/CD를 자동화하였나요? chart by Jinsu Park\n세로로 길어서 좀 보기 불편하실 수도 있는데, 위의 차트가 저희의 깃헙액션에 대한 차트입니다. 레포지토리마다 조금씩 다른 부분이 있고, 프론트엔드의 경우 파이프라인이 다양했는데, 우선 백엔드의 깃헙액션 및 CI/CD 진행 방식에 대해 요약해보겠습니다. (Chart를 작성한 지가 좀 돼서 설명과 조금 다른 부분이 있을 수도 있습니다.)\n개발 스프린트 진행 시에는 아래와 같이 진행되었습니다. (\u0026lt;상황 설명\u0026gt; =\u0026gt; \u0026lt;Github Action 수행 내용\u0026gt; 형식으로 나했습니다.)\n 개별 Fork 후 Master Branch에 Pull Request =\u0026gt; lint, basic unit test 진행. 통과된 PR만 Merge열 Master에 Merge 혹은 Commit이 Push됨 =\u0026gt; CI Action이 실행됩니다. 개발용 docker registry 에 업로드 해당 registry에 업로드 된 것을 감지하고 Spinnaker가 Experiment 환경에 배포 Experimental 환경을 이용해 Jenkins가 Integration Test를 진행 Integration Test 성공 시에 Dev 환경에 배포.  이후 스프린트 막바지 QA 기간에는 Experiment, Dev 환경이 주로 QA 환경으로 사용되었고, 파이프라인은 다음과 같았습니다.\n 검증이 어느 정도 끝난 커밋에 대해 Git Tag를 {{VERSION}}-rc{{RC_NUMBER}} 형태로 달아 푸시 - Github Release가 생김 =\u0026gt; 도커 이미지 빌드 후 Production Docker Registry에 이미지 업로드 =\u0026gt; tag가 달린 커밋을 기점으로 자동으로 버전 명의 브랜치를 만듦. Production Docker Registry의 업로드를 감지하고 Dev에서 손수 QA 진행 문제가 있을 경우 rc number를 올려서 다시 태그를 달고 업로드 후 재차 QA 문제가 없을 경우 해당 태그를 바탕으로 rc를 지우고 실제 버전으로서 태그를 달아 푸시 =\u0026gt; 다시 업로드 Production은 매뉴얼 배포.  Github Action을 다뤄보면서 느낀 점 처음에는 Github Action에 그렇게 만족을 하지 못했습니다. 초기에는 Manual Trigger가 지원되지 않았던 데다가, 가뜩이나 Github Action을 잘 몰랐기에 한 번 Github Action을 수정하여 테스트 하고싶을 때 마다 커밋을 하나씩 날려야했던 게 불편했고, 브랜치를 따로 만들거나 fork를 떠서 Github Action 테스트 후 해당 workflow만 마스터에 머지하는 방식 등등 다양한 방식을 사용했었는데, 어느 정도 익숙해지고 Github Action에 Manual Trigger도 등장하게 되면서 꽤나 만족도가 높아졌습니다.\nJenkins와 달리 제가 서버를 이용하지 않아도 된다는 점도 맘에 들긴했는데, 종종 Github 서버가 죽는 일이 발생해서 난감했던 적이 있긴합니다.\n하나 재미있었던 점은 Github Action을 이용하면서 저희의 CI/CD 전략이 꽤나 고도화되었는데, 그 과정에서 팀원들과 자유롭게 의사소통하는 과정이 재미있었고, 저 또한 자유롭게 의견을 나눌 수 있었던 경험을 할 수 있었다는 것입니다.\n후에 저희 SpaceONE의 CLI API 클라이언트인 spacectl의 설계에도 Github Action의 구조를 모티브삼았는데 이때에도 Github Action에 대한 지식이 많은 도움이 되었고, 퇴사 후에도 개인적인 Github 활동을 하면서 자유롭게 Github Action을 사용할 수 있었기에 든든한 개발 도구를 얻은 느낌입니다. 코드로 제가 하고싶은 것을 뭐든 정의할 수 있고, 만들어져 있는 작업은 편하게 가져다 쓰면 되기 때문에 빌드나 배포에 관해 재미있는 번뜩이는 아이디어가 있을 때 바로 바로 적용할 수 있고, 실제로 현재의 Github Page도 Github Action을 통해 다양한 트릭을 이용할 수도 있었고, 빌드 후 배포 또한 자동화 되어있습니다!\n"},{"uri":"https://umi0410.github.io/experiences/","title":"Experiences","tags":[],"description":"","content":"주요 활동 내역입니다.  메가존 클라우드 데브옵스 인턴 후기  안녕하세요. 이번에 메가존 클라우드에서 데브옵스 인턴으로 근무를 하게되었던 박진수입니다. 너무 좋은 팀원들과 많은 경험을 하며 단시간에 성장할 수 있었고, 일하는 동안 매 순간 순간이 너무 행복했었기에 이렇게 인턴 후기를 작성해봅니다.  "},{"uri":"https://umi0410.github.io/blog/aws/","title":"AWS","tags":[],"description":"AWS와 관련한 내용을 정리","content":"  EKS K8s에서 ELB(ALB, NLB) 제대로 설정하며 사용하기  🐶 시작하며 본 게시글은 AWS 대학생 유저그룹인 AUSG의 활동 중 하나로서 본인(박진수)이 작성한 게시물을 포워딩한 것입니다. 데브옵스 인턴으로 근무한 지가 벌써 두 달이 되어갑니다. 이것 저것 배운 것이 많았던 시간이었는데, 그 중 꽤나 삽질을 했던 Kubernetes 와 ELB를 이용하는 부분에 대해 정리를 해볼까합니다. jenkins, spinnaker, argo, terraform, ansible, github action, \u0026hellip; 등등 다양한 내용을 경험할 수 있던 시간이었지만, 그 중 kubernetes에서 무슨 작업을 하던 빼놓을 수 없으면서 어딘가 깔끔히 그 흐름이 정리된 곳을 보기 힘들었던 service를 ELB에 연결하기에 대한 내용을 정리해보겠습니다.\n "},{"uri":"https://umi0410.github.io/experiences/megazone-cloud/argo-poc/","title":"Argo Project들에 대한 PoC(개념 증명) 진행","tags":[],"description":"메가존 클라우드의 클라우드 원 팀에서 일하면서 Argo의 다양한 서비스들에 대한 Poc(개념 증명)을
진행했던 이야기를 적어보았습니다.
","content":" Table of Contents  Argo PoC를 진행하게 된 배경 Argo Project 구조  Argo Event (Official site) Argo, Argo Workflow (Official site) ArgoCD (Official site)   Argo Project를 조합한 CI/CD 파이프라인 구축기  파이프라인 수행 과정   Argo Project들에 대한 PoC를 진행하며 느낀 점     Argo PoC를 진행하게 된 배경 입사 초기부터 Argo에 대해 간간히 이야기를 들어왔습니다. 저희는 원래 Spinnaker을 사용했는데, 쿠버네티스 환경에 좀 더 친화적이라는 Argo를 도입해보는 것은 어떨까에 대한 얘기였는데요. 어느정도 개발 인프라 구축이 완료되고 한가해지자 잠깐이나마 Argo를 사용해볼 수 있었습니다. 결과적으로 Argo 도입은 적합하지 않다고 판단이 되었고 따라서 충분히 써볼 수는 없었기에 기록용으로만 간단히 적어보겠습니다.\nArgo Project 구조   눌러서 참고용 Argo Draw.io Chart들 보기    Argo가 워낙 빠르게 변화하는 서비스다보니 지금은 구조가 많이 변경되었을 수도 있습니다!\n chart by Jinsu Park\n  간편하게 사용할 수 있는 순서는 argo, argo(workflow), argo event 순이라고 생각되는데, 실질적인 파이프라인 구축은 argo event, argo (workflow), argo 순으로 이뤄지는 셈이라 후자의 순서에 맞춰 설명해보겠습니다.\nArgo Event (Official site) image by argo-events\n셋 중 가장 베이비 프로젝트입니다. 빠르게 개발되고 있고, 변화하고있기에 제가 argo project들을 만났던 시절과 많이 구조가 달라져있습니다. 간단하게 말하자면 이벤트를 감지하여 어떤 작업을 수행할 수 있습니다. argo-events의 공식 홈페이지에서 제공되는 위의 이미지와 같이 다양한 이벤트 소스를 이용해 다양한 이벤트를 트리거할 수 있습니다. 예를 들어 AWS SQS에 메시지가 생기면, 그 메시지를 가져와서 어떤 작업을 수행할 수 있습니다. 혹은 웹훅 서버를 돌려서 웹훅 요청이 오면, 어떤 작업을 트리거할 수 있습니다.\n제가 구상한 파이프라인에서는 이벤트를 감지하는 역할로서 Argo Events를 앞에 두고, Argo Events가 Workflow를 생성는 작업을 트리거하는 방식으로 CI/CD에 이용할 수 있습니다.\nArgo, Argo Workflow (Official site) 셋 중 가장 오래된 프로젝트이고, 별도 많습니다. Argo 라는 이름을 가졌고 실질적으로는 Workflow 관련 프로젝트입니다. Workflow란 컨테이너를 이용해 진행되는 일련의 step들을 정의하는 CRD(K8s Custom Resource Definition 입니다.)\n처음엔 \u0026lsquo;굳이 Workflow가 필요할까\u0026rsquo; 싶었지만, 쿠버네티스 상에서 일련의 Job을 연속적으로 수행할 수 있는 방법이 현재까지는 없는 것으로 알고 있습니다. K8s Job의 Container에 대한 InitContainer을 지정함으로써 한 Job에 대한 두 Container의 순서를 명시할 수는 있지만, Workflow처럼 다양하게 일련의 Job을 연속적으로 수행하기는 힘든 것으로 알고 있습니다.\nArgoCD (Official site) ArgoCD만 보면 가장 간단하게 실제 CD에 적용할 수 있는 프로젝트 중 하나가 아닐까싶습니다. ArgoCD 자체에 대한 설정, 소스가 될 Repository에 대한 설정, Project, Application에 대한 설정 등등 모든 것이 Code IaC에 특화된 독특한 프로젝트입니다.\nUI가 직관적이고 알아보기 쉽지만, 반대로 대규모 애플리케이션이 될 경우 너무도 배포 현황을 보여주는 맵이 커지기 때문에 알아보기 쉽지 않을 수 있을 것 같습니다. GitOps(\u0026ldquo;GitOps의 핵심은 Git 저장소에 저장된 쿠버네티스 매니페스트 같은 파일을 이용하여, 배포를 선언적으로 한다는 것입니다. 출처\u0026quot;) 라는 말에 아마 빠지지 않고 등장하는 CD 도구인듯합니다. Git에 올라가는 Manifest가 그대로 K8s 클러스터에 적용됩니다. 마치 Github Page나 S3(웹호스팅 설정이 된 S3 Bucket)에 올린 파일들이 바로 하나의 웹 애플리케이션처럼 동작하는 것과 비슷한 느낌입니다. 간단하게 사용하기에는 ArgoCD가 참 좋아보였지만, 실제로 업무적으로 사용하지는 못하겠다고 판단한 이유가 몇 가지 있습니다.\n 애플리케이션 하나에 대한 배포는 쉽지만, 연속적인 배포나 다양한 배포 파이프라인을 구성하기는 어려웠다. Github Push 시에 webhook을 설정해서 ArgoCD의 배포를 트리거하도록 했는데 이 부분이 최적화가 덜 되었는지 어떤 repository든 하나의 ArgoCD 환경에서는 Git platform 당 하나의 secret만 설정이 가능했고, 한 repository가 push되면 다른 repository도 배포가 다 같이 이루어져버렸습니다. (이 부분은 패치됐을 수도 있습니다.) 소규모 애플리케이션은 배포 상황을 한 눈에 보기 쉽지만, K8s Object가 많아지면 많아질수록 점점 하나하나 눈에 보이지 않고, 잡다한 Object들(ConfigMap, Secret, \u0026hellip;)로 인해 인식하기 힘듭니다.  Argo Project를 조합한 CI/CD 파이프라인 구축기 파이프라인 수행 과정  (Argo Event) Github Push를 감지하고 Argo Workflow CRD를 생성 Argo workflow를 통해 다양한 step들을 이용하기 위해 바로 ArgoCD가 Github Push를 받지 않고, Argo Event가 받도록 했습니다. Argo Workflow의 step들 수행  Slack에 workflow 시작 알림 Experiment 환경에 배포 (Argo CD 이용) Interation Test 실행 트리거 (Jenkins 이용) Integration Test 결과가 성공이면 Dev 환경에 배포 (Argo CD 이용) 어느 step에서든 실패 시 Slack에 실패 알림    이 정도 Pipeline을 짜면 사실 사용할 정도는 될 수 있겠지만, \u0026ldquo;일\u0026quot;로서 본다면 굳이 현재 사용 중인 Spinnaker에 비해 장점이 뚜렷하게 느껴지지 않는 Argo를 위해 파이프라인을 이전하기에는 역부족이라는 판단이 들었습니다. 예를 들어 과정 목록에서는 간단히 묘사했지만, Spinnaker에서는 UI에서 알아서 처리되던 부분들을 하나 하나 webhook을 걸어주거나, 따로 bash script를 짜야하는 경우들이 많았습니다.\nArgo Project들에 대한 PoC를 진행하며 느낀 점 개인적으로는 많이 애정이 갔던 프로젝트들이고 신기했던 배포방식에 대한 소개였으며, 이 프로젝트들을 공부해보면서 다양한 아키텍쳐에 대해 경험해볼 수 있었던 것 같지만, 현실적인 벽에 부딪혀 도입을 할 수 없었던 아쉬움 컸습니다. 마치 좋아하는 분야가 있었지만, 현실의 벽에 부딪혀 꿈을 접고, 어떠한 현실적인 진로 나아가는 것과 같았달까요?\n그리고 개인적인 아쉬움은 위의 차트를 손수 그리며 PoC 문서를 작성했었지만, 결국은 PoC를 진행한 담당자인 저조차 \u0026ldquo;음\u0026hellip; 실사용은 힘들 것 같은데요\u0026quot;라는 의견을 냈던 터라 바쁜 일정 속에서 reject할 개념 증명 리뷰에 많은 시간을 할애할 수 없었기에, 팀원들과 이 내용을 공유할 수 없었던 점에 조금 아쉬움이 남습니다..! 만약 정말 Argo를 사용하는 방식이 뛰어난 방식이었다면, 저도 적극 추천하며 함께 리뷰해주길 기대했겠지만, 현실적으로 저희 팀의 배포 방식과는 맞지 않았다고 판단해서 그랬던 것이기에 괜찮습니다~!\n"},{"uri":"https://umi0410.github.io/blog/","title":"Blog","tags":[],"description":"","content":"자유로운 주제로 작성한 블로그 게시물들입니다.  Go  다른 언어에선 겪어보지 못했던 특이한 경우나 다른 언어와 Go의 비교 등 주로 Go를 공부하고 적용하면서 겪게된 다양한 경험에 대해 적어보려합니다.\n AWS  AWS와 관련한 내용을 정리\n "},{"uri":"https://umi0410.github.io/experiences/megazone-cloud/spaceone-helm/","title":"SpaceONE Helm Chart 개발","tags":[],"description":"SpaceONE Helm Chart를 개발했던 경험
","content":" Table of Contents  SpaceONE Helm Chart란? spaceone-helm 설계 spaceone-helm chart 구조 마이크로서비스들의 버전 관리를 시작 Helm Chart를 통해 패키지화하며 느낀 점     SpaceONE Helm Chart란? spaceone-helm 은 저희 CloudOne 팀이 개발하는 서비스인 SpaceONE을 helm chart를 이용해 패키지화하는 프로젝트입니다. 원래의 저희 환경은 MicroService들을 개별 배포하고있었지만 오픈소스로 개발되는 저희 서비스를 저희 팀원들 뿐만아니라 다른 개발자들이 쉽게 개발할 수 있고, SpaceONE을 모르던 사용자들도 쉽게 SpaceONE을 구축해볼 수 있도록 하기 위해 패키지화도 진행하게되었습니다.\n일반적으로 만들어진 Helm Chart를 이용해보기만했지 직접 Chart를 만드는 것은 처음 해본 일이기도 했고, Chart를 개발하면서 새로운 환경에 저희 서비스를 배포해보다보니 삽질하며 고생도 꽤 했고, 무엇보다 프로젝트의 시작부터 퇴사 전까지의 작업들을 거의 제가 도맡아한 프로젝트였기에 개인적으로 애정이 많이 갔습니다 ^_^! 그리고 저도 이제는 저희 Chart를 이용해 SpaceONE을 구축형으로 손쉽게 이용할 수 있었습니다.\nspaceone-helm 설계 일단은 심플하게 사이드카 없이 저희의 마이크로서비스들만을 배포하고, 그 외에 필요한 서비스들도 최대한 Cloud Service에 의존하지 않고 그때 그때 손쉽게 서비스를 내렸다 올렸다 할 수 있도록 K8s 클러스터 위에 함께 배포하는 형식으로 Chart를 설계했습니다.\n무엇보다 chart 개발의 목적은 아래 두 가지 사항이 컸기 때문에, minikube 혹은 EKS 띄운 마이크로서비스들과 로컬에서 통신이 가능하도록 해야했고, 사용하기 편리한 구조를 만들기 위해 고심했습니다.\n 팀원이 아닌 개발자들도 자신의 부가적인 마이크로서비스를 로컬에 띄워 개발할 수 있도록 지원.  로컬에서도 쿠버네티스 클러스터 내의 서비스에 접속이 가능해야하고, 경우에 따라 클러스터에서도 본인의 로컬 서비스로 접속을 할 수 있어야함.   오픈소스로서 임의의 사용자가 서비스를 구축해보고자 시도할 때 손 쉽게 구축할 수 있도록 지원.  마치 유저들의 사용자 경험을 중요시해서 디자인, 기획을 하듯 Chart의 사용자들이 직관적이고 편리하게 구축할 수 있도록 Configuration values(values.yaml in Helm Chart)를 설계함.    spaceone-helm chart 구조 templates/ ├── backend │ ├── config │ │ ├── config-conf.yml │ │ ├── config-deployment.yml │ │ └── config-svc.yml │ ├── identity │ │ ├── identity-conf.yml │ │ ├── identity-deployment.yml │ │ └── identity-svc.yml │ ├── inventory │ │ ├── inventory-conf.yml │ │ ├── inventory-deployment.yml │ │ └── inventory-svc.yml │ ├── inventory-scheduler │ │ ├── inventory-scheduler-conf.yml │ │ ├── inventory-scheduler-deployment.yml │ │ └── inventory-scheduler-svc.yml │ ├── inventory-worker │ │ ├── inventory-worker-conf.yml │ │ ├── inventory-worker-deployment.yml │ │ └── inventory-worker-svc.yml #... Backend 생략 ├── consul, mongo, redis # 디테일한 파일구조는 생략 ├── frontend │ ├── console # 디테일한 파일구조는 생략 │ └── console-api # 디테일한 파일구조는 생략 ├── ingress │ └── ingress.yaml ├── initializer │ ├── initialize-spaceone-conf.yml │ ├── initialize-spaceone-job.yml │ ├── spacectl-apply-conf.yml │ └── spacectl-conf.yml └── supervisor └── supervisor ├── supervisor-conf.yml ├── supervisor-deployment.yml └── supervisor-roles.yml 저희 spaceone-helm chart는 꽤나 구조가 간단한 편은 아니라고 생각됩니다. 웹페이지 환경상 너무 부수적인 부분은 tree에서 생략하기도 했습니다. 꽤나 복잡한 구조를 가졌기에 패키지화하는 부분이 쉽지 않았지만, 그런 과정 속에서 많이 트러블 슈팅을 경험하고 성장할 수 있었던 것 같습니다.\n그리고 반대로 생각하면, 서비스가 패키지화하기도 쉽지 않을만큼 복잡한 구조를 가졌다면, 당연히 손수 배포하는 것은 그것보다 몇 배는 어려울 것이므로 누군가가 저희 서비스를 구축형으로 이용해주기 위해서는 패키지화가 필수라는 생각이 들었습니다.\n마이크로서비스들의 버전 관리를 시작 Helm Chart를 개발하고, CI가 고도화되기 전까지는 단순히 팀 내에서 개발을 진행하면서 개발환경에 대한 배포는 latest tag만을 이용해 자동으로 진행하고, QA를 진행하면서 커밋을 멈추고, 그 시점에 빌드된 Docker image의 Tag를 바탕으로 상용 환경에도 배포를 하곤했습니다. 하지만, Chart에서도 tag를 latest로 유지하거나 가독성이 좋지 않은 임의의 tag를 이용해 이미지를 제공하기는 힘들었고, 맞는 방향이 아니라고 생각했습니다.\n따라서 저희는 꼭 Helm을 통한 패키지 뿐만 아니라 롤백에 대한 안정성, 버전 간의 Update 내역 관리 등을 위해 {{MAJOR}}.{{MINOR}}.{{SPRINT_NUMBER}}-{{EXTRA_TAGS}} 형태를 통해 버전을 관리하고자했습니다.\nHelm Chart를 통해 패키지화하며 느낀 점 Helm Chart를 통해 저희 서비스를 패키지화하기 전까지는 마이크로서비스 형태로 관리되는 서비스를 새로운 환경에 완전하게 구축한다는 것은 쉽지 않았습니다. Container라는 것이 ReadOnly 레이어로 이미 구성이 완료된 이미지를 바탕으로 생성되기 때문에 언제 어디서든 구동이 가능하다는 것이 장점이겠지만, 현실적으로는 환경에 따라 이것 저것 설정해줘야하는 것이 있었기때문입니다.\n이러한 난관들은 어떠한 변수처리와 자동화를 통해 해결할 수 있을텐데, 그러한 작업을 해준 녀석이 바로 Helm이었고, 그때 그때 설정을 바꿈으로써 커스터마이징할 수 있다는 점이 알고는 있었습니다만 막상 저희 서비스에 도입해보니 꽤나 만족스러웠습니다.\n일화로 사내 네트워크가 막힌 상황에서도 받아놓은 이미지들만 있으면 네트워크 접속을 할 필요 없이 minikube와 로컬 서버를 이용해 minikube의 클러스터와 통신하면서 개발을 진행할 수도 있었습니다.\n누군가 제가 만든 서비스를 사용해준다는 것은 참 뿌듯한 일이었고, 앞으로도 저희 helm chart가 잘 발전되어 더욱 더 편리하게 구축할 수 있는 형태로 제공될 수 있기를 기대해봅니다!\n"},{"uri":"https://umi0410.github.io/blog/golang/go-cpu-cache/","title":"Go 언어로 적용해보는 Computer Science - Cache","tags":[],"description":"컴퓨터 구조를 수강하며 이론적으로 배웠던 CPU와 메모리 사이의 캐시가 어떻게 동작하는지 간단히 알아보고
그 효과를 간단한 프로그램을 통한 벤치마킹으로 알아봤습니다.
","content":" Table of Contents  시작하며 캐시란 Spatial locality Temporal locality 두 지역성 비교  공간 지역성에 치우친 캐시 구조 시간 지역성에 치우친 캐시 구조   프로그램을 통한 벤치마킹 마치며 참고     시작하며 저번 학기에 컴퓨터 구조를 수강하면서 간과하고 있던 로우 레벨의 지식에도 흥미가 생겼었다. 그 중 CPU와 Memory, Disk의 역할에 대해 알아볼 수 있었고 캐시는 CPU와 Memory 사이에 위치해 메모리 대신 빠르게 CPU에게 데이터를 제공하는 녀석이라고 배웠다.\n이전에는 주로 캐시라고 하면 주로 CDN과 같은 네트워크에서 쓰이는 캐시들밖에 몰랐다. 그렇다보니 L1 캐시, L2 캐시 같은 얘기를 들으면 OSI 7계층과 연관 지어 \u0026lsquo;음..? L2 캐시는 스위치에서 쓰는 캐시인가..?\u0026rsquo; 라는 상상을 하곤했다.\n이번에는 Go를 통해 배열에 여러 차례 접근하는 프로그램을 만들어보고 벤치마킹을 통해 캐시라는 녀석이 어떤 효과를 가져다주는지 직접 확인해보려한다.\n캐시란 캐시는 아주 다양한 문맥에서 사용된다. 공통적으로 \u0026ldquo;사용자가 요청할 것 같은 데이터를 작고 빠른 저장소에 저장해놓음으로써 좀 더 빨리 해당 데이터를 제공한다\u0026quot;는 목적을 갖는다. CDN, DB, REST API, Memory, CPU 등등 다양한 곳에서 쓰일 수 있을 것 같다. 그 중 이번에는 CPU와 메모리 사이의 캐시에 대해 알아보겠다.\nCPU와 메모리 사이의 캐시는 메모리의 데이터를 얻기 위해 메모리에 직접 접근하지 않고 캐시라는 빠른 저장소를 이용해 해당 데이터를 얻게끔해준다. 예를 들어 변수 a=10 이라는 데이터가 메모리에 존재한다해도 a의 값을 얻기 위해 메모리에 직접 접근하기 보다는 가까우면서 빠르게 이용 가능한 캐시에서 데이터를 가져올 수도 있다는 것이다. 사실 캐시의 개념적인 측면에서 보면 메모리 또한 디스크 대신 빠르게 값을 전달해주기 위한 경우일 수 있으니 캐시 기능을 한다고 볼 수 있다. 그리고 CPU와 메모리 사이에 정말 캐시라는 이름을 갖는 녀석들은 프로세서 속에 있는 L1 캐시, 프로세서 옆에 있는 L2 캐시, 프로세서들이 공유하는 L3 캐시가 있긴 하지만 이는 시대가 지나면서 얼마든지 변할 수 있는 내용들이기 때문에 어떤 캐시가 어디에 있고 누구랑 누가 공유하는지와 같은 세부 내용은 크게 중요하진 않을 것 같다.\n물리적인 크기나 거리는 속도와 반비례할 수 밖에 없다. 거리가 멀면 정보가 전달되는 속도가 느려지고 크기가 크면 여러 Mux나 Gate를 이용한다는 것이기 때문에 느려진다. 그렇기때문에 캐시는 작고 가까워야한다. 데이터를 요청하는 녀석은 CPU이기 때문에 캐시는 CPU 속 혹은 그 근처에 위치한다. 또한 작아야하기때문에 모든 정보를 담을 수 없고, 사용자가 요청할법한 데이터만을 담아야한다. 이 때 어떻게 사용자가 요청할 법한 데이터를 정할까? 이는 공간 지역성과 시간 지역성이라는 중요한 두 가지 성질을 기반으로 한다.\n이외에도 태그나 충돌 같은 개념들이 있긴하지만 실제로 벤치마킹해보기도 쉽지 않고 다소 지엽적인 내용이라 간단히만 정리해보면 태그 없이 주소값을 모듈러(나머지)연산해서 cache line index를 결정하고 그것만을 이용해 데이터를 저장하면 한 line 내에 저장할 워드(Word)에 대한 충돌이 발생할 수 있다. cache line이 20개인 캐시는 0번지와 20번지가 같은 line이므로 충돌이 발생해 계속해서 같은 line에 서로의 데이터가 번갈아 저장될 수 있다는 것이다. 하지만 태그를 이용하면 cache line 수는 줄어들더라도 한 line내에 여러 태그의 정보를 저장할 수 있게되어 cache line이 10개인 cache의 한 line에 0번지와 20번지의 데이터가 다른 태그로 저장되어 불필요한 충돌을 방지할 수 있다는 장점이있다. 간단히 설명하기는 힘든 내용이라 좀 더 자세히 알고싶다면 Direct mapped cache나 Fully associative cache 등으로 검색해보기를 권장한다.\nSpatial locality Spatial locality(공간 지역성)이란 지금 요청 받은 데이터와 가까운 곳에 위치한 데이터는 높은 확률로 다시 요청 받게 된다는 성질이다. 예를 들어 100번지의 a=10과 108번지의 b=20이 존재할 때 변수 a를 요청하면 이후 a와 가까운 주소에 저장된 b 또한 높은 확률로 요청된다는 것이다.\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { var ( a int = 10 b int = 20 c int = 30 ) fmt.Printf(\u0026#34;a: %p\\nb: %p\\nc: %p\\n\u0026#34;, \u0026amp;a, \u0026amp;b, \u0026amp;c) } /* Output: a: 0xc000100010 b: 0xc000100018 c: 0xc000100020 */ a, b, c의 크기는 8바이트로 주소값 또한 8바이트가 차이난다. (16진법이기에 20과 18의 차이는 8이다.) 즉 대체로 비슷한 시기에 할당된 변수는 근접한 메모리 주소를 갖게 된다. 우리는 비슷한 시기에 할당한 변수 혹은 연속된 배열 요소에 자주 빠른 시일 내에 접근을 하지 맨 위에서 선언한 변수와 저 멀리 맨 밑에서 선언한 변수를 마구잡이로 왔다 갔다 하면서 작업을 하지 않는 편이기 때문에 공간 지역성을 근거로한 캐시가 효력을 갖게 된다. 만약 공간적으로 먼 맨 위의 변수와 맨 아래의 변수를 자주 번갈아가며 접근한다면 그것은 시간지역성을 띄는 경우이다.\nTemporal locality Temporal locality(시간 지역성)이란 최근에 요청했던 데이터는 높은 확률로 다시 요청 받게 된다는 성질이다. 예를 들어 100번지의 a=10과 9999번지의 b=20은 서로 주소적인 거리는 멀지만 둘 다 최근에 호출됐다면 캐시에 적재하겠다는 것이다. 캐시는 주로 직사각형 형태로 생겼으며 가로(행)는 연속된 주소의 데이터를 저장하는 공간 지역성, 세로(열)는 최근에 호출된 데이터를 저장하는 시간 지역성을 담당한다.\n두 지역성 비교 12칸의 캐시가 있다고 가정하자. 가로로 4칸 세로로 3칸 존재한다면 공간/시간 지역성의 균형이 잡힌 캐시라고 볼 수 있다.(경우에 따라 다르겠지만)\n시간 지역성은 최근 불린 데이터는 다시 불릴 확률이 높다는 것이고 이는 연속된 공간이 아닌 다양한 공간(주소)의 데이터를 캐시에 저장한다는 말이기도 하다. 0번지 부근, 16번지 부근, 24번지 부근의 다양한 공간의 데이터를 저장할 수 있으면서 그 녀석들간의 주변 데이터도 제공하는 공간지역성도 만족한다.\n두 가지 지역성에 의해 다양한 캐시들이 데이터를 적재하고 제공한다. 요점은 캐시는 빠르게 동작해야하고 그러기 위해선 크기가 작고 가까워야하며 크기가 작기 때문에 모든 데이터를 담을 수 없으니 알짜 데이터만을 담아야하는데 그 알짜는 지역성을 기반으로 선별된다는 것이다. 크기가 한정적이기 때문에 한 지역성을 키우면 한 지역성은 작아질 수밖에 없다.\n공간 지역성에 치우친 캐시 구조 한정된 크기의 캐시 속에서 공간지역성을 극대화시켜버리면 당연히 인접한 공간의 자료만 이용할 수 있고, 최근에 불린 데이터들은 안중에도 없고 인접한 공간의 데이터만을 저장하게 된다. 예를 들어 다음과 같은 시간 지역성이 필요한 경우에 제대로 기능을 할 수 없다.\n 0~11번지 사이의 데이터가 한 번 접근 ⇒ 캐시에 0~11번지 적재 이후 12번지의 데이터에 접근 ⇒ 캐시에 데이터가 없기때문에 0~11번지의 데이터 대신 12~23번지의 데이터를 캐시에 적재 다시 최근에 접근했던 데이터인 0번지의 데이터에 접근 시도 ⇒ 0번 데이터는 최근에 접근했던 데이터임에도 시간 지역성이 활용되지 못함 ⇒ 캐시에서 데이터를 찾을 수 없음.  시간 지역성에 치우친 캐시 구조 위의 경우 최근에 호출된 다양한 주소의 데이터들을 캐시에 저장해준다. 하지만 캐시의 크기는 한정되어있기 때문에 세로가 길어지면 가로는 짧아진다. 즉 최근 접근을 시도한 다양한 주소의 데이터를 저장할 수 있지만 그 데이터의 인근 데이터에 대한 저장은 많이 할 수 없다는 것이다.\n예를 들어 위의 그림과 같은 경우 최근 0, 3, 12, 18, 6, 20번지의 데이터에 접근했고 이후에도 해당 번지에 대한 데이터를 캐시를 통해 이용할 수 있다. 바로 내가 최근에 접근했던 데이터이기때문이다. 하지만 만약 18번지의 데이터에 접근한 경우 높은 확률로 공간지역성에 의거 19, 20, 21, \u0026hellip; 번지의 데이터에 접근하겠지만, 이 예시는 시간지역성에 치우쳐져 19번지의 데이터만을 캐시에서 제공받을 수 있다.\n프로그램을 통한 벤치마킹 저번에 Mutex, Semaphore를 직접 벤치마킹해보면서 Go의 내장 벤치마크 기능을 이용하면 좀 더 편리하게 결과를 보여줄 수 있을 것 같았기에 이번에 Go의 내장 벤치마크 기능을 이용해봤다.\n1000행 1000열의 2차원 int형 배열의 어떠한 요소에 접근해서 +1 하는 작업을 1000회 수행하는 것을 하나의 싸이클로 하는 벤치마크를 작성했다. 2차원 배열은 가로로는 연속적인 주소값을 갖기에 공간 지역성을 활용할 수 있지만 세로로는 N * (int형 자료형의 크기)씩 차이 나는 주소값을 갖기 때문에 공간 지역성을 활용하기 힘들고, 최근 접근했던 주소라면 시간 지역성은 활용할 수 있다.\n공간 지역성 (가로로 연속적인 데이터)\n 공간 지역성을 사용하는 경우 가로로 연속된 요소에 접근. 즉 연속된 주소를 갖는 1000개의 요소에 접근 공간 지역성을 사용하지 않는 경우에는 세로로 요소에 접근. 즉 1000 * int 자료형의 크기만큼 차이나는 연속되지 않은 주소의 1000개의 요소에 접근  시간 지역성 (연속적 주소와 상관없이 최근에 불린 데이터)\n 시간 지역성을 사용하는 경우 주소값이 근접하진 않지만 4개 혹은 16개의 데이터에만 계속해서 접근 시간 지역성을 사용하지 않은 경우에는 계속해서 처음 접근하는 데이터에만 접근  package main import ( \u0026#34;testing\u0026#34; ) var ( Size int = 1000 ) func generateArray() [][]int{ arr := make([][]int, Size) for i := 0; i \u0026lt; Size; i++{ arr[i] = make([]int, Size) for j := 0; j \u0026lt; Size; j++{ arr[i][j] = 0 } } return arr } func BenchmarkSpatialLocality(b *testing.B){ b.Run(\u0026#34;공간지역성 사용\u0026#34;, func(b *testing.B) { arr := generateArray() b.ResetTimer() for n := 0; n \u0026lt; b.N; n++{ for i := 0; i \u0026lt; Size; i++{ arr[0][i] += 1 } } }) b.Run(\u0026#34;공간지역성 X\u0026#34;, func(b *testing.B) { arr := generateArray() b.ResetTimer() for n := 0; n \u0026lt; b.N; n++{ for i := 0; i \u0026lt; Size; i++{ arr[i][0] += 1 } } }) } func BenchmarkTemporalLocality(b *testing.B){ b.Run(\u0026#34;시간 지역성 적극 사용. 최근 접근한 데이터 4개.\u0026#34;, func(b *testing.B) { arr := generateArray() b.ResetTimer() for n := 0; n \u0026lt; b.N; n++{ for i := 0; i \u0026lt; Size; i++{ arr[i%4][0] += 1 } } }) b.Run(\u0026#34;시간 지역성 조금 사용. 최근 접근한 데이터 16개.\u0026#34;, func(b *testing.B) { arr := generateArray() b.ResetTimer() for n := 0; n \u0026lt; b.N; n++{ for i := 0; i \u0026lt; Size; i++{ arr[i%16][0] += 1 } } }) // 사실 완벽하게 새로운 데이터는 아님. 벤치마킹하는 동안 계속해서 반복되기 때문에  b.Run(\u0026#34;시간 지역성 X. 새로운 데이터에만 접근.\u0026#34;, func(b *testing.B) { arr := generateArray() b.ResetTimer() for n := 0; n \u0026lt; b.N; n++{ for i := 0; i \u0026lt; Size; i++{ arr[i][0] += 1 } } }) } goos: linux goarch: amd64 pkg: playground/unix-socket/cache BenchmarkSpatialLocality/공간지역성_사용 50000 894 ns/op BenchmarkSpatialLocality/공간지역성_X 50000 6561 ns/op BenchmarkTemporalLocality/시간_지역성_적극_사용._최근_접근한_데이터_4개. 50000 1918 ns/op BenchmarkTemporalLocality/시간_지역성_조금_사용._최근_접근한_데이터_16개. 50000 4153 ns/op BenchmarkTemporalLocality/시간_지역성_X._새로운_데이터에만_접근. 50000 6564 ns/op 결과를 확인해보니 간단한 배열 내의 요소들에 대한 연산인데도 꽤나 차이가 컸다.\n마치며 저번 학기에 컴퓨터 구조를 수강하면서 CPU-Memory 캐시의 효과를 직접 배열에 대한 프로그램을 통해 보여주는 예시를 보고 신기했던 기억이 있어서 이렇게 벤치마킹 프로그램을 작성해봤다. 다른 CS 주요 지식들에 비해 어려울 것은 없는 편이고 우리가 쉽게 접해오던 내용이라 더 이해하기 쉽지 않았을까 싶다.\n참고  캐시란 - https://ko.wikipedia.org/wiki/캐시 캐시가 동작하는 아주 구체적인 원리 - https://parksb.github.io/article/29.html cache mapping - https://m.blog.naver.com/jkssleeky/220478400046  "},{"uri":"https://umi0410.github.io/blog/golang/go-memory-leak-issue/","title":"개발 썰 - Go Memory Leak(메모리 누수) 관련 이슈","tags":[],"description":"Go로 이미지 프로세싱 서버를 만들면서 Memory Leak 이슈를 맞이했고
그 이슈를 해결하기 위해 했던 삽질과 그 결과에 대한 썰을 적어보았습니다.
","content":" Table of Contents  시작하며 이슈의 원인을 파헤치기 위해 했던 노력들\u0026hellip; 어떤 상황에 Memory Leak이 발생한 것일까? 해결 해보기  Go runtime은 OS에게 더 이상 이 메모리가 필요하지 않다고 알려주기만 하지 그 Memory를 실제로 회수할 지 말지는 OS에게 달려있다. Go 1.16 버전부터는 메모리 계산 방식이 바뀔 것이다. (그래서 괜찮을 것이다.)   마치며     시작하며  이 글은 Go 1.15 버전을 바탕으로 개발하며 겪은 이슈에 대해 설명하고 있으며 Go 1.16에서는 해당 이슈가 해결될 것이라고 합니다.\n 진행 중인 쿠뮤라는 프로젝트에서 Go를 이용해 이미지에 대한 url 해싱, 리사이징, 섬네일, 센터 크롭 작업을 하는 이미지 프로세싱 마이크로서비스를 개발하고있었다. 1차적으로 어느 정도 개발이 끝난 뒤 벤치마크 겸 부하 테스트 겸해서 얼마나 해당 마이크로서비스가 잘 버티면서 작업을 수행하는지 확인해보려 했으나 Memory가 한 번 치솟게되면 어느 정도 이하로 떨어지지 않는 이슈가 발견되었다.\n허용할 수 있는 양보다 많은 요청을 보낼 경우 애플리케이션 레벨 이전에서 요청을 차단해주지 않으면 애플리케이션 단에서는 터지거나 문제가 생기는 것은 당연하겠지만 사진처럼 작업을 다 수행한 뒤에도 메모리가 제대로 해제되지 않는 것이 이슈였다.\n이슈의 원인을 파헤치기 위해 했던 노력들\u0026hellip; 이 이슈를 잡아보려 여러 가지 디버깅 작업을 해봤으나 계속해서 메모리 이슈가 발생했고, 각종 커뮤니티에서 도움을 구해보고자했다. Go 오픈 카톡방, Reddit, Slack 등에서 의견을 구해보았다. 재미있는 경험이었으며 덕분에 원인을 파악할 수 있던 것 같다.\nGo Slack에서 사람들에게 의견을 구해본 모습 reddit 커뮤니티에서 사람들에게 의견을 구해본 모습 어떤 상황에 Memory Leak이 발생한 것일까? 그럼 자세히 어떤 이슈가 있었고, 어떻게 그 상황을 분석할 수 있었으며 어떻게 해결할 수 있을지 알아보도록하겠다.\ntl;dr - 자세한 묘사보다는 그냥 딱 원인 결과만 궁금하신 분들을 위한 요약\n 원인 - 힙 메모리에 메모리를 할당받았는데 혹시 재사용할 수도 있어 OS에게 바로 메모리를 반환하지 않음. 결과 - OS가 메모리 부족하니 달라고 하면 그때 힙 메모리 사이즈를 실제로 줄인다. (하지만 go 1.16 버전부터는 개선될 예정이다.) 정확히 이러한 것을 메모리 누수라고 하는지는 명확하지 않다. 런타임이 놀고있는 메모리를 갖고는 있지만 언제든 OS에게 반환해줄 수는 있기 때문이다. (하지만 매끄럽게 OS에게 반환해주는 느낌은 아니긴했다.)  package main import ( \u0026#34;time\u0026#34; ) func main(){ for routine := 0; routine \u0026lt; 10; routine ++{ DoFloat() } for { // dummy waiting..  time.Sleep(3*time.Second) } } func DoFloat(){ var tmp [400000000]float64 tmp[0] = 0 // tmp에 접근하지 않으면 unused variable이 되기 때문에 dummy한 access 작업 수행 } 처음엔 이미지 리사이징 시 메모리가 제대로 해제되지않는 것을 보고 이슈를 발견했지만, 사람들에게 도움을 구하고자 할 때, 이슈에 대해 설명할 때 전체 프로그램 코드를 첨부할 수는 없었기에 문제 상황을 간단하게 표현할 수 있는 코드를 짜보고자했다. 이리 저리 프로그램을 간소화하면서 커다란 배열 생성시에도 같은 메모리 이슈가 발생한다는 것을 알게되었다.\n그래서 아주 간단한 배열 생성 예시를 통해 사람들에게 이 이슈에 대해 설명해보고자했다. 이 예시에서는 8바이트의 float64로 이루어진 400000000칸의 배열 tmp를 선언한다.\n8바이트가 400000000칸이면 400000000 * 8 / 1024(KB) / 1024(MB) / 1024(GB) = 약 2.9GB 혹은 그대로 1000단위 씩으로 나눠 3.2GB을 할당하는 것이다.\n원래는 스택 메모리에 할당된 뒤 다른 곳에서 이 녀석을 참조하는 일이 없기 때문에 바로 release되어 스택 메모리에서 점유가 해제되어야한다. 하지만 몇몇 경우에 스택이 아닌 힙에 데이터가 저장될 수 있다고 하는데, 이 경우에는 너무 큰 값을 선언하여 스택이 아닌 힙에 데이터가 저장되었고, 힙이 할당받은 메모리를 해제해주지 않아 생기는 문제였다. 사실 이런 현상도 정확히 메모리 누수 혹은 Memory Leak라고 하는지는 잘 모르겠다. 왜냐하면 메모리에서 해제할 수 없는 수준으로 그 값의 주소를 잃어버려 실제로 그 공간이 누수가 되는 것이 아니라 아직 딱히 OS가 부담을 느끼지 않기 때문에 Go Runtime의 Heap에서 해당 주소는 비워뒀지만 OS에게 반납은 안 한 상태인 것이기 때문이다.\n실행해보면 10번의 DoFloat() 후에 그냥 for loop에서 time.Sleep 중이기에 CPU를 거의 사용하지 않고 있는 반면 내 Laptop의 16GB의 Memory 중 20.5%인 3.2GB를 사용 중인 것을 확인할 수 있었다.\n해결 해보기 Go runtime은 OS에게 더 이상 이 메모리가 필요하지 않다고 알려주기만 하지 그 Memory를 실제로 회수할 지 말지는 OS에게 달려있다.  The Go runtime only advises the OS when it no longer needs memory and it is up to the OS to reclaim it - @justinisrael\n 내가 Reddit에서 사람들께 여쭤봤던 글에 담긴 한 댓글을 인용해보았다. 과연 저 말이 사실일까? 프로세스를 여러번 띄워보았다.\n프로세스를 하나 띄웠을 때에는 아까는 Dummy waiting 중이면서도 Memory를 20% 가까이 점유하고 있었는데 여러 프로세스를 띄우면서 메모리가 부족해지자 놀고있던 Heap memory를 반환하여 거의 약 0.6%의 메모리만 점유 중인 것을 볼 수 있다.\n마지막 프로세스는 아직 DoFloat() 작업을 진행 중이므로 여전히 19.8%의 메모리를 점유 중이고, 작업이 완료된 뒤에도 OS가 Reclaim(다시 메모리를 가져가는 것)하기 전까지는 약 20%대를 유지하는 것으로 보여졌다.\n하지만 좀 더 자세히 기록해보고싶었다. Go의 내장 패키지 runtime의 Memory 관련 기능을 이용하면 될 것 같았다. runtime.ReadMemStats(*runtime.MemStats)를 이용하면 런타임 도중 자신의 런타임 상황을 알아볼 수 있다. 자세한 사용법은 구글링을 통해 쉽게 얻을 수 있으니 지면 관계상 생략한다.\n즉 힙 메모리가 확보간 공간은 거대한 배열을 생성하는 함수인 DoFloat을 진행하는 동안은 실제로 Allocate 할당하여 사용하는 반면 DoFloat을 모두 마친 뒤에는 힙 메모리는 공간을 확보하고는 있지만 Idle(놀고 있는) 상태로 존재하는 것이었다.\n나는 이러한 경우를 처음 맞이했지만, 각종 가비지 컬렉터가 있는 언어에서 각각의 가비지 컬렉터나 런타임을 구현하는 방법에 따라 이런 식으로 힙 메모리를 재사용하는 경우를 대비해 한 번 할당받은 메모리를 완전히 OS에게 반환하지 않는 경우가 있다고 한다.\nGo 1.16 버전부터는 메모리 계산 방식이 바뀔 것이다. (그래서 괜찮을 것이다.)  \u0026ldquo;There was a change to how memory is calculated on 1.16\u0026rdquo; - @gopj\n 과연 사실일지 확인해보자. 간단히 Docker를 통해 나의 Local 환경에서 별 다른 세팅 없이 Go 버전을 다르게 하여 실행할 수 있었다. 실제로 이 이슈를 커뮤니티에 제기하기 전에도 Docker를 이용해 1.2, 1.3, 1.4 등의 버전에서도 실행해보았다. 나의 랩탑은 1.5 버전을 사용 중이었다. 1.16 버전은 아직 rc는 Release Candidate의 줄임말로 보통 배포 후보 버전을 의미한다.\n# DockerfileFROMgolang:1.16-rcWORKDIR/appCOPY main.go main.goENTRYPOINT [\u0026#34;go\u0026#34;]CMD [\u0026#34;run\u0026#34;, \u0026#34;./main.go\u0026#34;]# 이미지 빌드 후 컨테이너 실행 $ docker build . -t tmp \u0026amp;\u0026amp; docker run --rm --it tmp 1.16-rc 버전을 사용하자 결과적으로 다른 메모리 부하가 심한 프로세스를 실행시키지 않았지만 수십 초 이내에 사용하지 않는 힙 메모리가 OS에게 반환되었다!!!\n 하지만 여전히 runtime.MemStats에서는 HeapMemory에 Idle한 메모리 크기가 크게 잡혀있었는데, 이 부분은 rc 버전이기 때문에 runtime까지 완전히 기능이 개발되지 않아서인지 이전에도 메모리는 반환했지만 메모리를 표시하기 위한 계산의 문제만 개선이 된 것인지는 확실하진 않지만 여튼 이슈에 대해서는 파악할 수 있었다!\n 마치며 개발이나 운영을 하면서 이런 저런 이슈들이 있었지만 이번 경우처럼 로우 레벨스럽게 들어가서 언어의 Runtime까지 까보게되었던 적은 드물었던 것 같다. 도저히 원인을 모르겠어서 \u0026lsquo;하 결국 포기하고 이 마이크로서비스는 Lambda로 돌려야하나\u0026hellip;\u0026rsquo; 싶었다. 하지만 몇 시간을 고생하고 직접 의견을 구하러 다니면서 새로 배우게 된 내용도 많았고, 외국에 계신 몇몇 개발자 분들과도 이렇게 소통할 수 있다는 것이 신기했다. (그리고 참 세세한 지식까지 겸비한 분들이 많다는 것이 놀라웠다\u0026hellip;)\n결과적으로 해당 이슈는 아마 Go의 1.16 버전이 패치되면 완전히 해결 가능할 것 같고 그 이전에도 사실 Host에서 메모리 부담을 느끼면 Go runtime이 안 쓰고 있는 힙 메모리를 반환해준다고 하니 큰 문제는 없을 것 같다. 하지만 실제로 Host가 부담을 느끼는 선 이전에 메모리를 반환받고 싶다면 Pod level에서 메모리 리소스를 제한해볼 수는 있을 것 같다.\n혹시 다음에도 이런 흔치 않은 고된 이슈를 맞이한다면 그 원인과 해법을 이렇게 또 기록할 수 있기를 바래본다.\n"},{"uri":"https://umi0410.github.io/experiences/megazone-cloud/spacectl/","title":"SpaceONE CLI Client인 spacectl 설계 및 개발","tags":[],"description":"spacectl이라는 CLI 프로그램 개발기
","content":" Table of Contents  spacectl이란?  소개 사용 예시   설계를 하며 느낀 점 개발하면서 느낀 점     spacectl이란? 소개 spacectl 은 저희팀이 개발하는 서비스인 SpaceONE의 gRPC API request를 CLI로 손쉽게 수행할 수 있도록 해주는 도구입니다. 파이썬을 통해 개발했고 Click 이라는 모듈로 CLI 환경을 손쉽게 사용할 수 있었고, Jinja2를 통해 상세한 Manifest 들에서 변수 치환, 분기 등을 수행할 수 있었습니다.\n사용 예시 A simple example 간단하게 spacectl이 어떤 식으로 이용되는 도구인지 예시를 보여드리겠습니다. 아래의 커맨드를 이용해 손쉽게 SpaceONE의 다양한 마이크로서비스들의 API를 이용할 수 있습니다.\n$ spacectl list domain domain_id | name | state | plugin_id ... domain-abc123abc | umi0410| ENABLED | ... $ spacectl list server -p domain_id=domain-abc123abc server_id | name | provider ... server-abc123abc | foo | aws ... apply command spacectl apply command는 kubectl의 apply와 유사하게 없으면 만들고, 있으면 업데이트하고 혹은 단순히 어떤 API를 Execute하는 Task들의 플로우를 관리해주는 커맨드 입니다.\n 아쉽게도 퇴사 전에 마무리를 짓지는 못했습니다. ㅜ.ㅜ 퇴사 전까지 진행한 작업은 일부 리소스에 대한 CRU(create, read, update), 대부분의 리소스에 대한 Excute(execute할 API를 설정)까지입니다.\n # main.yaml import: - mongo.yaml # 개별 yaml file에서는 terraform/ansible과 같이 수행할 Task들을 정의 - root_domain.yaml - repository.yaml var: domain_name: root domain_owner: admin admin_username: admin admin_password: admin $ spacectl apply main.yaml 설계를 하며 느낀 점 이 프로젝트에 대한 실제 개발 업무 이전에는 꽤나 설계 업무가 많았습니다. 저는 그 동안은 혼자 주로 개발을 해왔고, 실행력 좋게 시작은 하지만 설계는 충분하지 않은 채 성급하게 실행에 옮겼던 경험이 많습니다. 또한 학생이었고, 개발 경력이 길지 않았기에 사실상 \u0026ldquo;개발 = 그때 그때 새로운 내용 공부\u0026quot;와 같은 느낌이었기에 애초에 설계를 하려해도 \u0026lsquo;뭐가 필요하고 뭐가 가능할 것이고 뭐가 힘들 것인가\u0026rsquo; 를 판단하기 어려웠습니다.\n하지만 팀원들과 함께 개발하면서 밥 먹을 때, 회의 할 때 틈틈히 설계 방식과 요령에 대해 상의했고, 처음으로 설계를 어느 정도 굳힌 뒤 개발에 들어들어갔던 경험이었습니다.\n인턴 기간 막바지에 이 설계에 참여하게 된 것은 정말 값진 경험이었다고 생각했습니다. 사실 단순히 데브옵스로서 일할 때는 남들과 의사소통할 일이 그리 많진 않았는데, 이 설계를 맡게 되면서 많은 회의와 대화를 하게되었습니다. 선배 개발자분들과 설계에 대해 잦은 회의를 하면서 제가 어떤 개발자가 되고싶은지 직접 느낄 수 있었던 것 같습니다. 그 배경에는 두 가지 충격이 있었습니다.\n \u0026lsquo;내가 남의 생각을 잘 읽는 편은 아니었나보군\u0026hellip;?' \u0026lsquo;선배 개발자분은 내가 개판으로 설명해도 어떻게 귀신같이 나보다 내 생각을 잘 읽으시지?' =\u0026gt; 마치 축구할 때 노련한 축구선수와 함께 뛰면서 저 행동을 귀신같이 예측하고서는 너무나도 잘 밀어주는 느낌을 받았습니다\u0026hellip;  제가 평소에 말을 잘하는 편이라고 생각했는데, 남의 생각을 이해하고 읽어내는 능력은 그리 뛰어나지많은 않구나라는 생각을 하게됐습니다. 설계 내용이 꽤나 추상적으로 구두로 진행되었기에 그랬을 수도 있겠지만, 큰 충격은 선배 개발자분의 노련함이었습니다.\n후에 누군가 어떤 개발자가 되고싶냐, 협업할 때 어떤 노하우가 있느냐 이런 내용을 물어보면 자신있게 남의 생각을 잘 이해하고, 알아주는 사람과 관련해 대답할 수 있는 사람이 될 수 있었으면 좋겠습니다.\n또한 설계 깊게 진행하기 전에 침착하게 자료 조사를 잘 해야한다는 것을 느꼈습니다.\n하나 일화로 원래는 설정 파일에서 ${{ tasks.umi0410.output }} 이런 식의 변수를 이용한 설정을 치환한 뒤 API를 수행해야할 때, 아마 template 언어들을 제가 원하는 대로 사용하기 힘들 것이라 생각하고, 하나하나 함수와 클래스를 만들어가곤했는데, 개발이 거의 완료되어갈쯤 Jinja2의 사용법을 다시 읽다보니 spacectl에 Jinja2를 적절히 사용할 수 있을 것 같았고, 정신이 번뜩 들어 몇 시간만에 수작업으로 짠 코드들을 덜어내고 Jinja2를 이용해 좀 더 깔끔하게 변수 치환 및 추가로 Jinja2의 built-in filter들을 이용할 수 있었습니다!\n개발하면서 느낀 점 역시 개발은 남이 만든 패키지를 잘 사용해야한다는 것을 느꼈고, 그냥 복사 붙여넣기만 잘하면 된다는 의미가 아니라, 그런 것들을 빠르게 가져와서 적용시키고 부분 부분 커스터마이징하기 위해서는 기본기가 탄탄해야한다고 느꼈습니다. 물론 수작업으로 만드는 것도 좋을 수 있겠지만, 다양한 엣지케이스가 존재할 수 있고, 그 모든 작업들을 문서로 상세히 설명하는 것이 아니라면, 제작자인 제가 아닌 누군가가 그 기능을 이용하기는 힘들 것입니다. Jinja2를 이용한 템플릿 기능 제공이 이와 관련된 경험이 될 수 있겠습니다.\n또한 남의 코드를 자세히 읽어보는 게 처음이었는데, 덕분에 파이썬 프로젝트를 수행할 때 어떤 식으로 디렉토리 스트럭쳐를 짜면 좋을 지 생각해볼 수 있었던 계기였던 것 같습니다. 제가 설계하고, 개발한 내용을 짧게나마 발표한 뒤 리뷰를 받고 수정을 하면 한층 더 코드의 구조와 사용이 간결하고 직관적으로 보인다는 느낌을 받을 수 있었습니다. 이렇게 다양한 가르침을 주신 저희 팀의 선배 개발자분들께 항상 감사드립니다.\nMegazone CloudOne 팀의 DevOps 인턴으로서 근무했던 내용에 대한 후기가 거의 끝났습니다. 끝으로 인턴 활동에 대한 종합적인 느낀점을 이어서 보시거나 다시 목차를 보고싶으신 분은 여기를 클릭해주세요.\n "},{"uri":"https://umi0410.github.io/blog/golang/go-vs-java-oop/","title":"Go vs Java - Go에서의 객체 지향","tags":[],"description":"주로 Java로 설명되는 객체지향적인 특징들을 Go에서는 어떻게 적용할 수 있는지 알아봤습니다.
","content":" Table of Contents  시작하며  예시 코드   객체지향적 관점에서의 Go에 대해 💊 Encapsulation(캡슐화) 👩‍👧‍👦 Inheriatance(상속) Abstraction(추상화) 🌒🌓🌕 Polymorphism(다형성) Go에서의 객체 지향의 한계점과 장점 마무리 참고     시작하며 요즘 Go와 Java 모두를 이용해 개발을 하다보니 각각의 장단점에 대해 느껴볼 수 있었다. Go는 리소스를 적게 먹으며 코드가 간결하고 라이브러리나 프레임워크 또한 심플해서 적용하기 편하다. Java는 이런 저런 기능이 많은 반면 그런 기능을 이용하기 위해 이해해야하는 내용들이 많고, 코드가 투명하지는 않다(다양한 Annotation을 이용하게 되면서 코드가 투명하게 그 동작을 나타내지 않음). Java의 장점 중에서는 특히나 객체지향의 대표적인 언어답게 상속과 다형성을 능력에 따라 자유자재로 이용할 수 있다는 점이 매력적이었다.\nGo 언어를 좋아하는 입장에서 개인적으로 이런 객체지향적인 특징이나 예외 처리를 제외하고는 딱히 Java가 Go에 비해 갖는 장점이 크게 느껴지지 않았다. 예외 처리는 Go가 바라보는 방향이 일반적인 예외 처리와 다르기에 어쩔 수 없지만, 객체지향적의 특징들은 어떻게 적용해볼 수 있을까하는 생각에 공부를 좀 해봤고 그 내용을 정리해본다. (기회가 된다면 Go에서 error를 다루는 철학에 대해 추가적으로 공부해보고싶다.)\n❗Go에 대한 기본적인 내용을 정리해보는 것이 아니라 객체 지향 관점에서 바라본 Go에 대한 내용을 정리해보는 것이므로 Go의 기초 내용에 대한 설명은 생략할 것이므로 Go에 대한 기초 이해가 없다면, 그 부분을 먼저 알아보는 것을 추천한다!\n예시 코드 Go에서 객체 지향을 적용한 간단한 계산기 프로그램을 예시로 작성해보았다. 전체 소스코드를 다 볼 필요는 없겠지만 필요에 따라 참고할 수 있도록 아래와 같이 첨부한다.\n  main 패키지 - /main.go   package main import ( \u0026#34;bufio\u0026#34; \u0026#34;calculator/calc\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;strings\u0026#34; ) func main(){ var ( calculator *calc.Calculator = calc.NewCalculator() // 계산기 struct  operationUnit calc.OperationUnit // OperationUnit이라는 interface type을 통해 Polymorphism 이용  operationResult float64 // 계산 결과를 담음  operationErr error // 계산 수행에 대한 error을 담음  ) // main package에서는 calc package에 정의된 unexported name인 id에 접근할 수 없다.  //calculator.id = \u0026#34;Jinsu Park\u0026#34;  scanner := bufio.NewScanner(os.Stdin) // 연산 option을 위해 값을 입력받기 (e.g. 1)  fmt.Printf(`A simple calculator program. ======================================================= Operation options 1. Mulitply a, b float 64 2. Sqaure val, square float64 Please input an int for your desired operation. \u0026gt;\u0026gt;\u0026gt; `) scanner.Scan() option, _ := strconv.Atoi(scanner.Text()) // args를 위해 입력받기 (e.g. 10 20)  fmt.Printf(\u0026#34;Please input floats as args.\\n\u0026gt;\u0026gt;\u0026gt; \u0026#34;) scanner.Scan() inputs := make([]float64, 0) for _, input := range strings.Split(scanner.Text(), \u0026#34; \u0026#34;){ f, _ := strconv.ParseFloat(input, 64) inputs = append(inputs, f) } // 연산에 대한 multiplexing. 즉 option에 따른 연산을 수행한다는 의미  // operationUnit이라는 OperationUnit interface type을 통해 Polymorphism 이용  switch option{ case 1: operationUnit = calculator.Multiplier case 2: operationUnit = calculator.SquareMultiplier } // Operate라는 기능을 다양한 동작으로 수행할 수 있다.  operationResult, operationErr = operationUnit.Operate(inputs...) // result  if operationErr != nil{ fmt.Println(\u0026#34;[Error]\u0026#34;, operationErr) } else{ fmt.Println(\u0026#34;Result:\u0026#34;, operationResult) } }      calc 패키지 - /calc/calc.go   package calc import ( \u0026#34;errors\u0026#34; ) var autoIncrementID int = 1 type Calculator struct{ id int // 제조된 Calculator을 식별하기 위한 ID. 변수명이 소문자로 시작하므로 export 되지 않는다.  Multiplier OperationUnit SquareMultiplier OperationUnit } type OperationUnit interface{ IsValidInput(args ...float64) bool Operate(args ...float64) (float64, error) } type MultiplyOperationUnit struct{} type SquareOperationUnit struct{ MultiplyOperationUnit // Embed의 예시 } func NewCalculator() *Calculator{ c := \u0026amp;Calculator{ id: autoIncrementID, Multiplier: \u0026amp;MultiplyOperationUnit{}, SquareMultiplier: \u0026amp;SquareOperationUnit{MultiplyOperationUnit: MultiplyOperationUnit{}}, } autoIncrementID += 1 return c } func (unit *MultiplyOperationUnit) IsValidInput(args ...float64) bool{ if len(args) != 2{ return false } return true } func (unit *MultiplyOperationUnit) Operate(args ...float64) (float64, error){ if !unit.IsValidInput(args...){ return 0, errors.New(\u0026#34;MultiplyOperationUnit의 args가 유효하지 않습니다.\u0026#34;) } return args[0] * args[1], nil } //func (unit *SquareOperationUnit) IsValidInput(args ...float64) bool{ // ... 필요에 따라 정의하면 Override처럼 이용 가능 //}  func (unit *SquareOperationUnit) Operate(args ...float64) (float64, error){ // *SquareOperationUnit에 대한 IsValidInput 메소드는 정의한 적 없지만  // Embedding을 통해 일반적인 OOP에서 부모의 메소드를 이용하듯이 이용 가능.  if !unit.IsValidInput(args...){ return 0, errors.New(\u0026#34;SquareOperationUnit의 args가 유효하지 않습니다.\u0026#34;) } var ( // val^square 즉 \u0026#34;val의 square 제곱\u0026#34;에 대한 계산  val = args[0] square = args[1] reduced float64 = val // 제곱 연산 중 값을 담아 놓는 변수  cnt = float64(1) // 제곱 연산 루프를 위한 counter  err error ) for ;cnt \u0026lt; square; cnt ++{ reduced, err = unit.MultiplyOperationUnit.Operate(reduced, val) if err != nil{ return 0, err } } return reduced, err }     Calculator라는 계산기 struct가 존재  ✨ id field는 소문자로 시작하기때문에 외부에서 함부로 접근할 수 없도록 data를 캡슐화   계산기에서 각각의 연산을 담당하는 OperationUnit interface존재 OperationUnit interface는 Input의 유효성을 검사하는 IsValidInput 메소드와 연산을 수행하는 Operate 메소드 존재 각각의 연산을 담당하는 Unit은 OperationUnit interface가 정의한 메소드들을 구현함으로써 duck-typing을 통해 OperationUnit interface로 사용된다. ⇒ ✨ interface를 통한 추상화와 다형성 이용 가능  OperationUnit interface로 사용이 가능한 struct의 예시  MultiplyOperationUnit struct - 곱셈 연산을 담당 SquareOperationUnit struct - 제곱셈 연산을 담당  ✨ Embed를 통해 MultiplyOperationUnit을 상속한 것처럼 field와 method를 사용 가능        객체지향적 관점에서의 Go에 대해 Go는 아무래도 객체 지향 언어라고 하지는 않는 듯하다. 이에 대해선 다양한 의견이 있는 것 같은데 Post-OOP 언어라는 사람도 있고 OOP 언어는 아니지만 Object-Oriented하게 할 수 있으므로 OOP 언어이면서 OOP언어가 아니라는 사람도 있다.\nOOP에는 크게 4가지 원칙이 있다.\n 연관된 변수와 함수를 클래스로 묶으며 외부에서 특정 데이터나 기능에 접근하지 못하도록 하는 정보를 은닉해주는 캡슐화 부모 객체의 field, method를 자식 객체가 이용할 수 있도록해주고, Override할 수 있게 해주는 상속 세부 사항은 제외하고, 어떤 기능이 존재하는지 등의 추상적인 정보만으로도 이용할 수 있게 해주는 추상화 즉 하나의 타입이 여러 타입으로 이용될 수 있으며, 각각이 다양하게 동작할 수 있는 다형성  그리고 대표적인 OOP 언어인 Java는 이러한 내용들을 아주 잘 이용할 수 있게끔 되어있다. OOP 언어가 아닌 Go에서 이러한 OOP의 특징이자 장점인 요소들을 어떻게 적용할 수 있을지 알아보도록하겠다.\n💊 Encapsulation(캡슐화) Go에서는 Export를 통해 캡슐화를 이용할 수 있다. Export에 대해 간단히 설명하자면 private, public 을 이용해 변수나 함수에 접근 제한을 두는 것이 아니라 이름이 대문자냐 소문자냐에 따라 패키지 외부에서 접근을 제어하는 것을 말한다.\nGo의 Export 이용 방법은 많이 찾아볼 수 있으니 캡슐화에 초점을 맞춘 그 쓰임에 대해 알아본다.\n// Calculator에 대한 정의와 구현을 담당하는 calc 패키지 package calc type Calculator struct{ ID int // 제조된 Calculator을 식별하기 위한 ID. 변수명이 소문자로 시작하므로 export 되지 않는다. ... } 위와 같이 Calculator를 정의하는 calc라는 패키지가 있다고 가정하다. Calculator를 식별하는 ID에 대한 작업은 calc 패키지에서 담당하고 함부로 외부에서 값을 바꾸지 못하도록하고싶은 경우 field가 소문자로 시작하도록 함으로써 외부 패키지에서 직접 접근하지 못하도록할 수 있다.\n필요에 따라 Java에서 그러하듯 getter와 setter를 정의해줄 수도 있다.\npackage main ... func main(){ var ( calculator *calc.Calculator = calc.NewCalculator() ) // main package에서는 calc package에 정의된 unexported name인 id에 접근할 수 없다. //calculator.id = \u0026#34;Jinsu Park\u0026#34; } 앞서 말했듯이 이렇게 calc 패키지가 아닌 외부 패키지(예를 들어 main 패키지)에서는 unexported name인 id에 접근할 수 없다.\n주로 캡슐화와 Go에 대해선 은닉을 어떻게 하는가가 요점이라고 생각해 이 부분에 대해 다뤄보았다.\n 자세한 내용은? - 연관된 변수와 함수를 묶어주는 내용은 Go의 struct, receiver와 method 등에 대해 검색해보면 더 깊이 알아볼 수 있다.\n 👩‍👧‍👦 Inheriatance(상속) Go는 Composition을 이용한 Embedding이라는 방식을 통해 Inheritance와 같은 기능을 이용할 수 있게 해준다는 식으로 많이들 설명을 하는 것 같았으나 주관적인 해석을 해보자면 Go의 Embedding은 Composition이면서 자동으로 embed된 field의 method와 본인의 method인 것처럼 사용할 수 있게 해주기에 Inheritance처럼 이용할 수 있다고 볼 수 있겠다.\nGo의 Embedding은 struct의 field에 별도의 name이 아닌 type만을 적어줌으로써 이용할 수 있다. B라는 type이 는 메소드 Say()를 가지고 있는 경우 struct A가 type B를 Embed한다면 A는 B의 Say 메소드를 두 가지 방법으로 이용 가능하다.\n A.Say() - 이경우 암묵적으로 2.로 변환되어 수행되는 셈 A.B.Say() - 이렇게 명시적으로 Selector(여기선 B)를 적어줄 수도 있다. 일반적인 Composition과 동일하다.  Go에서 Embedding을 사용하는 방법 자체 또한 많은 내용을 인터넷에서 찾아볼 수 있으니 상속과 Embedding에 초점을 맞춰 설명해보도록하겠다.\n일반적인 객체 지향적인 방식에서는 type MultiplyOperationUnit struct 라는 type이 존재하고\ntype SquareOperationUnit struct 가 MultiplyOperationUnit type을 상속받는다면 SquareOperationUnit은 IsValidCheck 메소드를 비롯한 MultiplyOperationUnit의 메소드와 멤버 변수를 이용할 수 있을 것이다.\ntype MultiplyOperationUnit struct{} type SquareOperationUnit struct{ MultiplyOperationUnit // type만을 전달함으로써 Embed } // 곱셈 연산에 대한 Input validation func (unit *MultiplyOperationUnit) IsValidInput(args ...float64) bool{ fmt.Println(args) if len(args) != 2{ return false } return true } func (unit *MultiplyOperationUnit) Operate(args ...float64) (float64, error){ if !unit.IsValidInput(args...){ return 0, errors.New(\u0026#34;MultiplyOperationUnit의 args가 유효하지 않습니다.\u0026#34;) } ... } func (unit *SquareOperationUnit) Operate(args ...float64) (float64, error){ // *SquareOperationUnit에 대한 IsValidInput 메소드는 정의한 적 없지만  // Embedding을 통해 일반적인 OOP에서 부모의 메소드를 이용하듯이 이용 가능.  if !unit.IsValidInput(args...){ return 0, errors.New(\u0026#34;SquareOperationUnit의 args가 유효하지 않습니다.\u0026#34;) } ... } 이 경우 Go에서는 SquareOperationUnit이 MultiplyOperationUnit을 Embed하도록 한다. 평범한 Compostion 방식으로 이용할 수 있겠지만 Go의 특이한 Embed 방식을 이용함으로써 *SquareOperationUnit 에 대한 IsValidInput 메소드를 정의한 적 없지만 일반 OOP에서 부모 클래스에 정의된 메소드를 이용하듯unit *SquareOperationUnit과 같이 이용 가능하다.\nfunc (unit *SquareOperationUnit) IsValidInput(args ...float64) (float64, error){ ... } OOP에서의 Method Override와 같은 작업을 Go에서 하고싶다면 위와 같이 추가적으로 자신의 타입에 대한 method를 정의하면된다. 메소드를 추가적으로 정의해준 뒤 Selector를 지정하지 않으면 당연히 우리가 바란대로 Embed된 type의 method가 아닌 자기 자신의 method를 호출하게 된다.\n 자세한 내용은? - 아무래도 Go의 Embedding과 Composition에 대한 이해가 없다면 무슨 말인지 이해하기 힘들 수 있다. 따라서 해당 내용들에 대해 알아볼 것을 추천!\n Abstraction(추상화) 추상화는 그 객체의 세부 내용이 아닌 공통된 기능을 바탕으로 추려내는 것을 의미한다.\n추상화에 있어서는 Java와 Go가 interface를 이용한다는 점에서 크게 다르진 않다.\ninterface에 추상적으로 해당 interface를 구현하는 type들이 구현하기를 바라는 method를 정의만한다.\ntype OperationUnit interface{ IsValidInput(args ...float64) bool Operate(args ...float64) (float64, error) } 이 경우 OperationUnit는 \u0026ldquo;IsValidInput과 Operate 기능을 수행할 수 있는 무언가\u0026rdquo; 이라고 추상화된 것이다.\n 자세한 내용은? - 추상화의 쓰임은 다형성의 쓰임과도 밀접한 연관이 있다. Go에서 interface를 사용하는 패턴과 사용법에 대해 알아보면 좋을 것 같다.\n 🌒🌓🌕 Polymorphism(다형성) 다형성이란 한 가지 타입이 경우에 따라 같은 기능에 대해 다양한 동작을 수행할 수 있는 것을 말한다. 추상화는 interface에 대한 정의에 해당하고 다형성은 interface 활용에 해당하는 듯하다.\n일반적인 OOP 언어에서는 interface가 아닌 상속 관계에서도 부모⇒자식으로 타입 변환을 통해 다형성 활용이 가능하다. 하지만 Go는 이를 지원하지 않는다. 이유는 런타임에 동적으로 method dispatch(해당 type의 객체 혹은 value가 어떤 함수를 메소드로 할 지 결정하는 것)을 수행함으로 인한 오버헤드를 줄이기 위해서 컴파일 타임에 정적으로 method dispatch할 수 있게 하기 위해서라고한다.\ntype OperationUnit interface{ IsValidInput(args ...float64) bool Operate(args ...float64) (float64, error) } type MultiplyOperationUnit struct{} type SquareOperationUnit struct{ MultiplyOperationUnit } ... 각종 메소드 정의 생략 func main(){ var ( calculator *calc.Calculator = calc.NewCalculator() operationUnit calc.OperationUnit operationResult float64 operationErr error ) // 연산에 대한 multiplexing. 즉 option에 따른 연산을 수행한다는 의미  switch option{ case 1: operationUnit = calculator.Multiplier case 2: operationUnit = calculator.SquareMultiplier } operationResult, operationErr = operationUnit.Operate(inputs...) // result  if operationErr != nil{ fmt.Println(\u0026#34;[Error]\u0026#34;, operationErr) } else{ fmt.Println(\u0026#34;Result:\u0026#34;, operationResult) } } operationUnit이라는 Interface에 다른 type의 struct인 calculator.Multiplier와 calculator.SquareMultiplier가 담길 수 있다.\n이를 통해 operationUnit.Operate() 는 경우에 따라 \u0026ldquo;연산\u0026ldquo;이라는 기능으로 Multipliy 작업을 수행할 수도 있고, SqaureMultipliy 작업을 수행할 수도 있는데, 이를 다형성이라고 한다.\nGo에서의 객체 지향의 한계점과 장점 public class Example { public static void main(String[] args){ // Parent class와 Parent를 extends한 Child class에 대한 구현은 생략한다.  Parent parent = new Parent(); Parent child = new Child(); parent.ShowMetaData(); // parent.GetName() 이용  /* Output ======================================= Name: Parent ======================================= */ child.ShowMetaData(); // child.GetName()이용.  // 이 때에는 .ShowMetaData()가 Parent class가 아닌 Child class가 Override한 GetName() 이용  /* Output ======================================= Name: Child of Parent ======================================= */ } } 객체 지향 프로그래밍에선 부모 클래스에 정의된 메소드가 내부에서 자식이 Override한 메소드를 이용할 수도 있는데, Go는 그런 기능은 이용할 수 없다는 점이 가장 큰 한계점인 것 같다.\n예를 들어 Parent class의 .ShowMetaData()라는 method가 .GetName()이라는 메소드를 호출하는 경우, java에서는 Child가 GetName을 Override하면 child.ShowMetaData() 호출 시에 Child가 Override한 child.GetName()을 이용하지만, Go는 그럴 수 없다. 필요한 경우 함수를 인자나 field로 전달함으로써 사용할 수 있겠지만, 사용성이 제한적이다. 이 내용에 대해 여기서 설명하면 글이 길어질 것 같아 자세한 묘사는 생략하겠다.\n반면 Go에서의 객체 지향은 장점은 이 글(https://www.toptal.com/go/golang-oop-tutorial)의 후반부에 잘 나와있는데, 굳이 Java를 이용하지 않아도 이렇게 OOP가 충분히 가능하다는 것이 핵심이다. Java의 VM/JIT으로 인한 리소스 부족, 자유도가 떨어지는 프레임워크, 많은 annotation, \u0026hellip; 등등의 단점 없이도 충분히 Go를 통해 가볍게 OOP 할 수 있다는 것이 장점이다.\n마무리 아무래도 객체 지향적 개발을 하는 데에 있어서는 Java가 좀 더 직관적으로 그대로 설계, 구현해서 이용이 가능한 것 같다. 처음 Java를 공부했을 때부터 Java는 객체지향적으로 개발하는 패턴에 대해 수없이 많은 예제가 존재했고, 그 패턴이 명확했던 반면 Go는 명확한 패턴이나 깔끔하게 정의가 없다(이런 식으로 OOP 원칙을 이용해볼 수 있지 않을까~ 정도). 아마 애초에 Go는 객체 지향 언어로 설계하지 않았기 때문이 아닐까싶다.\n이를 계기로 Go에서는 OOP 원칙들이 어떻게 다양하게 적용할 수 있는지 좀 더 자세히 알아볼 수 있었다.\n참고  [번역] Go와 OOP - https://mingrammer.com/translation-go-and-oop/ Golang OOP tutorial - https://www.toptal.com/go/golang-oop-tutorial go object-oriented - https://golangkorea.github.io/post/go-start/object-oriented/  "},{"uri":"https://umi0410.github.io/blog/golang/golang-concurrent-pattern-pipeline/","title":"Go의 Pipeline pattern. 언제 사용해야할까? - Golang concurrent patterns","tags":[],"description":"Golang의 Concurrent pattern 중 하나로서 Channel을 바탕으로 커뮤니케이션하며 작업을 진행하는 패턴인
Pipeline pattern에 대해 간략해 소개해봤고, 언제 사용해야할지 그 use case에 대해 알아보았습니다. 
","content":" Table of Contents  ✋ 시작하며 ❓ Pipeline pattern이란?  Pipeline pattern의 흔한 예시로 square 하는 예제를 설명하는 글들   🤔 언제 쓰는 게 좋을까? 예시 프로그램  프로그램 설명 📉 벤치마크 결과 비교   마무리 참고     ✋ 시작하며 Go를 공부하기 시작한 지도 벌써 몇 달이 지난 것 같다. 데브옵스 인턴을 마치면서 특히나 관심있었던 Go를 공부하기 시작했었고, 지난 몇 달간 AWS KRUG내의 소모임인 AUSG의 스터디 활동으로 Go를 주제로 공부해왔다. Golang의 꽃이라고 할 수 있는 요소들이 몇 개 있었는데 나는 그 중 goroutine과 channel에서 매력을 느꼈고 그를 바탕으로한 concurrency pattern들에 대해 이래 저래 많이 알아봐왔다.\n하지만 concurrency pattern이 뭐고, 어떻게 사용하는지에 대해서는 여러 글을 찾아볼 수 있었지만 이게 왜 좋고 언제 쓰면 좋을지에 대한 내용은 찾아보기 힘들었다. 항상 어떤 기술을 접할 때 \u0026ldquo;왜 좋은데?\u0026ldquo;와 \u0026ldquo;언제 쓰면 좋은데?\u0026ldquo;를 많이 따지는 편이라서 늘 궁금증에 남아있었다.\n그러던 중 얼마 전 나의 궁금증을 해소시켜주는 간단한 댓글을 보게 되었고, 그를 바탕으로 몇 가지 서치를 해본 결과 concurrency pattern 중 하나인 pipeline pattern을 언제 쓰면 좋을지 알아보았다.\n❓ Pipeline pattern이란? Pipeline pattern은 golang의 Concurrency pattern 중 하나이다. 쉽게 말하자면 함수의 인자에 값을 전달해 작업하는 것이 아니라 channel 을 통해 실시간으로 함수(혹은 goroutine)들끼리 커뮤니케이션을 하며 작업을 진행하는 것이다. 실시간으로 함수들끼리 커뮤니케이션하며 data를 전송하기 때문에 data streaming 같은 느낌이라고 볼 수 있겠다.\n일반적으로는 한 함수에서 작업이 모두 끝난 뒤 그 결과값을 리턴하고, 또 다시 그 결과 값을 인풋으로 어떤 함수가 작업을 하고 해당 작업이 모두 끝난 뒤 또 다른 결과값을 리턴하는 형태로 진행이 된다. 하지만 Pipeline pattern에서는 한 작업이 모두 완료되지 않았다하더라도 해당 작업의 부분 부분의 데이터를 담는 channel을 리턴하고, 다른 작업은 해당 channel을 인풋으로 하여 부분 부분의 데이터를 channel에서 꺼내 바로 바로 작업한다.\ngenerator을 이용한 lazy evaluation. 0, 1, 2를 모두 Insert한 뒤 square and print 하는 것이 아닌 하나씩 실시간으로 진행\n이는 마치 Python의 Generator을 통한 Lazy evaluation을 이용할 때와 유사하게 볼 수는 있겠다. 하지만 많은 차이점이 존재하긴 할텐데 우선 generator는 thread-safe하지 않은(stackoverflow 참고) 반면 channel은 여러 goroutine에서 한 channel에 접근해도 thread-safe하다. \u0026lsquo;thread-safe한가\u0026rsquo;가 중요한 이유는 후에 잠깐 소개할 fan-in fan-out pattern이 바로 pipeline pattern이 여러 goroutine에서 이루어지는 경우이기 때문이다.\n(사실 내가 Pipeline pattern을 어디에 사용할 지 몰랐던 이유는 Lazy evaluation과 같은 개념이 없었기 때문이 아닐까싶기도 하다.)\nPipeline pattern의 흔한 예시로 square 하는 예제를 설명하는 글들 종종 Pipeline pattern의 예시를 알아보면서 square 작업을 하는 예제들을 몇 개 봤던 것 같은데, 그 중 참고할 수 있는 예시들을 제시해본다. square 작업이 pipeline pattern에 효율적이라기보다는 그냥 임의의 예시라고 생각한다.\n이 글은 Pipeline pattern을 언제 사용하면 좋을지에 초점을 맞추었기에 Pipeline pattern이 뭔지, 그 사용법은 어떻게 되는지 등의 내용을 모르는 상태라면 아래 글들을 추천한다. (특히 Aidan Bae님의 블로그에는 pipeline pattern 외에도 다양한 Go 관련 내용을 잘 적혀있다.)\n Aidan Bae님의 아주 간단한 Pipeline pattern 예시 - https://aidanbae.github.io/code/golang-design/pipeline/ GoBlog의 간단한 내용부터 꽤나 어려운 내용까지 들어가는 것 같은데, 내용도 길고 짧은 코드에 억지로 많은 내용을 끼워넣은 느낌.. https://blog.golang.org/pipelines  🤔 언제 쓰는 게 좋을까? 앞선 lazy evaluation 관련된 내용과 이 글의 댓글(go로 작업을 stream하라는 글에 \u0026ldquo;너 정말 저렇게 stream하는 게 좋다고 생각해? 이 경우 아니면 그닥 잘 모르겠는데?\u0026ldquo;라는 댓글)이 나의 이해를 도와주었다. Pipeline architecture를 사용해야하는 7가지 이유라는 글을 읽어보기도 했지만 여기의 내용은 실질적으로 왜 그 장점을 갖는지에 대한 설명이나 논리가 부족했다. (다소 Go와 Pipeline pattern에 대한 무조건적 사랑으로 느껴짐..ㅎㅎ\u0026hellip;)\n내 생각에는 아래 두 가지 경우에 Pipeline pattern을 이용하는 것이 좋을 것 같다.\n Input 작업을 모두 수행하는 데에 오랜 시간이 걸리는 경우 작업하는 데이터의 양이 너무 커서 Memory를 많이 점유하므로 쪼개서 바로 바로 처리하고 싶은 경우  \u0026lsquo;디버깅하기 쉽다\u0026lsquo;거나 \u0026lsquo;코드를 알아보기 쉽다\u0026rsquo; 등등의 글을 보긴했던 것 같은데 개인적으로는 그냥 pipeline과 channel을 안 쓰고 그냥 반복문을 돌리면서 데이터를 통째로 작업하는 게 디버깅이나 가독성면으로 훨씬 유리하다고 생각한다. 즉 Concurrent pattern의 장점은 특정 use case에서의 성능적인 측면이라고 생각한다.\n그리고 이건 여담인데 go의 channel을 통한 data 전달이 일반적으로 빠른 건 절대 아니다. channel은 우선 thread safe하기때문에 일반적인 작업에서는 thread-unsafe한 방식보다 느린 것이 당연하다. 또한 channel은 단순한 lock 기능 외에도 다른 goroutine(혹은 thread 혹은 function) 간의 데이터 전송, channel에 대한 반복, 열었는지 닫았는지에 대한 체크 등등 다양한 기능도 갖고있기때문에 mutex에 비해서도 훨씬 느린 듯하다. 그렇기 때문에 위에서 나열했듯 Input 작업이 오래 걸리거나 Memory 점유량을 줄이기 위한 경우에는 channel을 이용한 Pipeline pattern으로 실시간으로 작업하면 유리할 것이라고 생각한 것이다.\n그리고 추가적으로 앞에서 Fan-in Fan-out pattern에서는 여러 Goroutine에 대한 Pipeline pattern이 적용된다고했는데, Fan-in Fan-out이란 여러 goroutine이 하나의 channel에 값을 넣거나 빼는 구조를 말한다. 이 구조의 장점은 일반적인 Pipeline Pattern에서 각 단계가 하나의 goroutine만을 이용하는 것이 아니라 여러 goroutine을 이용할 수 있다는 점이다. CPU bound한 작업이 아닌 IO Block이 주요 latency를 차지하는 경우는 Goroutine을 늘려주면 concurrent하게 작업할 수 있기때문에 성능이 좋아지더라. Fan-in Fan-out pattern에 대해서는 기회가 된다면 더 자세히 다뤄보겠다.\n참고\n Mutex보다 channel이 느리다는 벤치마크 글 - http://www.dogfootlife.com/archives/452 Mutex를 쓸 수 있는 상황이면 Mutex를 쓰는 것이 좋을 수도 있다 - https://github.com/golang/go/wiki/MutexOrChannel  예시 프로그램 프로그램 설명 Input 작업을 수행하는 데에 오랜 시간이 걸리는 경우를 예시로 들기 위해 MySQL에서 Gopher에 대한 데이터를 조회한 뒤 Gopher들의 연령을 +10 증가시키는 예시를 만들어봤다.\nInput 작업을 모두 수행하는 데에 오래 걸리고, 작업 내용을 조금씩 쪼갤 수 있다는 전제조건을 위해 DB Query를 할 때 전체를 한 번에 Select 하는 것이 아니라 1개씩 Select하는 다소 비효율적이고 비현실적인 상황을 이용하긴하지만 간단하게 Pipeline pattern의 효율을 극대화시켜보고자했던 이유이므로 양해를 부탁드린다.\n예를 들어 Pipeline pattern을 통해 1000개의 Row를 Query하고 난 뒤 1000개를 Update하는 것이 아니라 1000개의 Row를 Query하면서 1개 1개의 Row를 얻어올 때 마다 바로바로 Update 작업에 Row를 넘겨줘 실시간으로 작업할 수 있게해주는 것이다.\n Pipeline pattern: 한 item 씩 실시간으로 전달하며 진행 Sequential pattern: 딱히 Pattern이라기엔 좀 그렇지만 그냥 일반적으로 Sequential하게 진행하는 경우를 의미. 이 예시에선 전체 Query 완료 후 전체 Update하는 방식.  Docker를 이용해 간단하게 MySQL 서버를 띄웠고, Gorm을 이용해 DB Query와 update를 수행했다.\n  예시 프로그램 코드   package main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/brianvoe/gofakeit/v6\u0026#34; \u0026#34;gorm.io/driver/mysql\u0026#34; //\u0026#34;gorm.io/driver/sqlite\u0026#34; \t\u0026#34;gorm.io/gorm\u0026#34; \u0026#34;gorm.io/gorm/logger\u0026#34; \u0026#34;math/rand\u0026#34; \u0026#34;time\u0026#34; ) var ( NumTotalData int = 100 // 초기화할 데이터 \tNumManipulateData int = 100 // fetch 후 map을 적용할 데이터 수 ) type Gopher struct { gorm.Model Name string Age int } func initializeDB(db *gorm.DB) { // Migrate the schema \tdb.AutoMigrate(\u0026amp;Gopher{}) // Create initial data \tfor i := 0; i \u0026lt; NumTotalData; i++ { gopher := \u0026amp;Gopher{Name: gofakeit.Name(), Age: rand.Intn(60) + 1} gopher.ID = uint(i + 1) db.Save(gopher) } fmt.Println(\u0026#34;Initialized DB\u0026#34;) } type SequentialPattern struct { DB *gorm.DB } type PipelinePattern struct { DB *gorm.DB } func (sp *SequentialPattern) Execute() { start := time.Now() sp.Map(sp.FetchData()) fmt.Println(\u0026#34;sequential\u0026#34;, sp) fmt.Println(\u0026#34;Elapsed:\u0026#34;, time.Now().Sub(start)) fmt.Print(time.Now().Sub(start)) gophers := make([]Gopher, 0) sp.DB.Limit(NumTotalData).Find(\u0026amp;gophers) fmt.Printf(\u0026#34;Average age(Num: %d): %d\\n\u0026#34;, len(gophers), func() int{ totalAge := 0 for _, gopher := range gophers{ totalAge += gopher.Age } return totalAge/len(gophers) }()) } func (sp *SequentialPattern) FetchData() []*Gopher { gophers := make([]*Gopher, NumManipulateData) for id := range gophers { gophers[id] = \u0026amp;Gopher{} sp.DB.Where(\u0026#34;id \u0026gt; ?\u0026#34;, id).Limit(1).Find(\u0026amp;gophers[id]) } return gophers } func (sp *SequentialPattern) Map(gophers []*Gopher) { for _, gopher := range gophers { gopher.Age += 10 sp.DB.Save(gopher) } } func (pp *PipelinePattern) Execute() { start := time.Now() pp.Map(pp.FetchData()) fmt.Println(\u0026#34;pipeline\u0026#34;, pp) fmt.Println(\u0026#34;Elapsed:\u0026#34;, time.Now().Sub(start)) gophers := make([]Gopher, 0) pp.DB.Limit(NumTotalData).Find(\u0026amp;gophers) fmt.Printf(\u0026#34;Average age(Num: %d): %d\\n\u0026#34;, len(gophers), func() int{ totalAge := 0 for _, gopher := range gophers{ totalAge += gopher.Age } return totalAge/len(gophers) }()) } func (pp *PipelinePattern) FetchData() \u0026lt;-chan *Gopher { ch := make(chan *Gopher) go func() { for id := 0; id \u0026lt; NumManipulateData; id++ { var gopher Gopher pp.DB.Table(\u0026#34;gophers\u0026#34;).Where(\u0026#34;id \u0026gt; ?\u0026#34;, id).Limit(1).Find(\u0026amp;gopher) ch \u0026lt;- \u0026amp;gopher } close(ch) }() return ch } // 고퍼들을 모두 10살 더 먹게 함. func (pp *PipelinePattern) Map(gophers \u0026lt;-chan *Gopher) { for gopher := range gophers { gopher.Age += 10 pp.DB.Save(gopher) } } func main() { // $ docker run -it --name mysql --rm -e MYSQL_ROOT_PASSWORD=root -e MYSQL_DATABASE=concurrency -p 3306:3306 mysql \t// 로 mysql 서버를 띄울 수 있다. \tdb, err := gorm.Open(mysql.Open(\u0026#34;root:root@tcp(127.0.0.1:3306)/concurrency?charset=utf8mb4\u0026amp;parseTime=True\u0026amp;loc=Local\u0026#34;), \u0026amp;gorm.Config{Logger: logger.Default.LogMode(logger.Silent)}) if err != nil { fmt.Println(err) panic(\u0026#34;failed to connect database\u0026#34;) } initializeDB(db) // Execute with PipelinePattern \tfunc() { fmt.Println(\u0026#34;==============================\u0026#34;) for rep := 0; rep \u0026lt; 10; rep++ { pattern := \u0026amp;SequentialPattern{DB: db} pattern.Execute() } fmt.Println(\u0026#34;==============================\u0026#34;) }() fmt.Print(\u0026#34;DB 부하를 가라앉히기 위해 5초간 휴식\u0026#34;) time.Sleep(5 * time.Second) // Execute with PipelinePattern \tfunc() { fmt.Println(\u0026#34;==============================\u0026#34;) for rep := 0; rep \u0026lt; 10; rep++ { pattern := \u0026amp;PipelinePattern{DB: db} pattern.Execute() } fmt.Println(\u0026#34;==============================\u0026#34;) }() }    📉 벤치마크 결과 비교 Sequential하게 진행할 경우 모든 Item에 대한 Query가 완료될 때 까지 Update 작업은 지연되게 된다. 그리고 Query가 모두 완료되어야 비로소 Update 작업을 시작할 수 있게되므로 좀 더 작업 시간이 오래걸리는 편이다.\n반면 Pipeline으로 진행할 경우 한 Item을 Query 하자마자 바로 바로 Upadte 작업이 이루어질 수 있기때문에 작업 시간이 더 짧은 경향이 있다.\n하지만 이 정도 차이는 뚜렷한 성능 차이로 보기엔 다소 미미한 것 같았다. 그래서 좀 더 IO latency가 긴 상황을 이용해봤다. DB로 localhost에서 docker mysql server를 이용하는 것이 아니라 운영 중이던 k8s cluster의 mysql pod를 kubectl port forward하여 이용해보았다. (정확히는 모르지만 remote db + kubectl port forawrd 를 이용하는 경우 network latency가 아주 커져 극단적으로 좋은 예시 상황이 될 수 있을 것 같았다.)\n위에서 가정한 remote db + port forward의 극단적 상황은 작업 진행 pattern에 따라 latency를 2배 가량 차이나게 했다.\n즉 한 단계에서 오랜 시간이 소모되어 다음 단계가 지연되는 경우 Pipeline을 이용하면 좋은 것 같다. 이는 마치 컴퓨터 구조에서 MIPS Processor의 Pipeline을 공부할 때와 유사한 듯한 느낌을 줬다.\n마무리  이 글이 정확한 내용은 아닐 수 있지만 조사해본 선에서 벤치마크 해보며 작성해봤습니다. 혹시 글의 내용 중 잘못된 내용에 대한 피드백을 제시해주신다면 기회가 닿는 한 열심히 다시 알아보겠습니다~!\n 평소 궁금했던 \u0026ldquo;Go의 concurrent pattern은 어떨 때 쓰면 좋을까?\u0026rdquo; 라는 의문 중 pipeline pattern 에 대해 이렇게 알아봤다.\nPipeline pattern이 CPU Bound한 작업보다는 IO latency로 인해 오랜 시간이 소모되는 작업에 더 효율적이라고 생각한 이유는 CPU Bound한 작업에서는 CPU가 이미 혹사당하고 있기 때문에 굳이 여러 goroutine을 schedule, switch하거나 channel을 통한 동기처리를 하면서 작업을 진행하기보다는 그냥 일반적인 방식으로 순차적으로 작업을 진행하는 게 좋을 수 있기 때문이다. 워낙에 얼마나 CPU bound한 작업인지, 얼마나 많은 goroutine을 이용하는지 등등 다양한 경우에 따라 달라지기 때문에 딱 잘라말할 수도 없고, 나도 많이 부족하기 때문에 정확히는 모르겠다. 하지만 추측컨대 적어도 io latency로 인해 작업들이 지연되는 경우에는 pipeline pattern이 좋은 듯하다.\n추가적으로 Fan-in Fan-out pattern 또한 channel을 바탕으로 data를 전달하면서 작업하기 때문에 일종의 Pipeline pattern이 적용된 패턴인듯하여 기회가 된다면 어떤 경우에 단순히 Pipeline 패턴을 이용하는 것보다 여러 Goroutine을 이용해 작업하는 Fan-in fan out pattern이 좋을지 비교해보는 글을 적어보고싶다.\n참고  Aidan Bae님의 Pipeline pattern에 대한 소개 - https://aidanbae.github.io/code/golang-design/pipeline/ Go Blog의 Pipeline pattern에 대한 소개 - https://blog.golang.org/pipelines Pipeline은 ~~~ 이런 상황에 도움이 될 껄?하는 댓글 - https://www.reddit.com/r/golang/comments/7rjdw6/go_go_go_stream_processing_for_go/ Mutex와 Channel 비교 - https://github.com/golang/go/wiki/MutexOrChannel Mutex와 Channel의 속도 차이 - http://www.dogfootlife.com/archives/452 Pipeline architecture를 사용하는 7가지 이유(개인적으론 공감 안 감) - https://labs.bawi.io/7-reasons-to-use-pipeline-architecture-93346f604b87 python generator 소개, 사용법- https://wikidocs.net/16069 python generator는 thread-safe하지 않다. - https://stackoverflow.com/questions/1131430/are-generators-threadsafe  "},{"uri":"https://umi0410.github.io/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://umi0410.github.io/tags/","title":"Tags","tags":[],"description":"","content":""}]